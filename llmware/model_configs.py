# Copyright 2023-2024 llmware

# Licensed under the Apache License, Version 2.0 (the "License"); you
# may not use this file except in compliance with the License.  You
# may obtain a copy of the License at

# http://www.apache.org/licenses/LICENSE-2.0

# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
# implied.  See the License for the specific language governing
# permissions and limitations under the License.


"""Global Default Configs for Models, Finetune Wrappers and Prompt Instructions Catalog.

These configs generally do not need to be accessed directly, but can be viewed, accessed and modified through
ModelCatalog and PromptCatalog classes.

For customization, there is also the option in ModelCatalog to load a custom model catalog from json file, which
would over-write this list.
"""

global_model_repo_catalog_list = [

    # embedding models

    {"model_name": "all-MiniLM-L6-v2", "display_name": "mini-lm-sbert", "model_family": "HFEmbeddingModel",
     "model_category": "embedding", "model_location": "hf_repo", "embedding_dims": 384, "context_window": 512,
     "link": "https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2",
     "custom_model_files": [], "custom_model_repo": "",
     "hf_repo": "sentence-transformers/all-MiniLM-L6-v2"},

    {"model_name": 'all-mpnet-base-v2', "display_name": "mpnet-base", "model_family": "HFEmbeddingModel",
     "model_category": "embedding", "model_location": "hf_repo", "embedding_dims": 768, "context_window": 514,
     "link": "https://huggingface.co/sentence-transformers/all-mpnet-base-v2",
     "custom_model_files": [], "custom_model_repo": "",
     "hf_repo": "sentence-transformers/all-mpnet-base-v2"},

  {"model_name": 'industry-bert-insurance', "display_name": "industry-bert-insurance",
      "model_family": "HFEmbeddingModel",
      "model_category": "embedding", "model_location": "hf_repo", "embedding_dims": 768, "context_window":512,
      "link": "https://huggingface.co/llmware/industry-bert-insurance-v0.1", "custom_model_files":[],
      "custom_model_repo": "",
      "hf_repo": "llmware/industry-bert-insurance-v0.1"},

     {"model_name": 'industry-bert-contracts', "display_name": "industry-bert-contracts",
      "model_family": "HFEmbeddingModel",
      "model_category": "embedding", "model_location": "hf_repo", "embedding_dims": 768, "context_window":512,
      "link": "https://huggingface.co/llmware/industry-bert-contracts-v0.1", "custom_model_files":[],
      "custom_model_repo": "",
      "hf_repo": "llmware/industry-bert-contracts-v0.1"},

    {"model_name": 'industry-bert-asset-management', "display_name": "industry-bert-asset-management",
      "model_family": "HFEmbeddingModel", "model_category": "embedding", "model_location": "hf_repo",
      "embedding_dims": 768, "context_window":512,
      "link": "https://huggingface.co/llmware/industry-bert-asset-management-v0.1", "custom_model_files":[],
      "custom_model_repo": "",
      "hf_repo": "llmware/industry-bert-asset-management-v0.1"},

     {"model_name": 'industry-bert-sec', "display_name": "industry-bert-sec", "model_family": "HFEmbeddingModel",
      "model_category": "embedding", "model_location": "hf_repo", "embedding_dims": 768, "context_window":512,
      "link": "https://huggingface.co/llmware/industry-bert-sec-v0.1", "custom_model_files": [], "custom_model_repo": "",
      "hf_repo": "llmware/industry-bert-sec-v0.1"},

     {"model_name": 'industry-bert-loans', "display_name": "industry-bert-loans",
       "model_family": "HFEmbeddingModel", "model_category": "embedding", "model_location": "hf_repo",
       "embedding_dims": 768, "context_window": 512,
       "link": "https://huggingface.co/llmware/industry-bert-loans",
       "custom_model_files": [], "custom_model_repo": "", "hf_repo": "llmware/industry-bert-loans"},

  {"model_name": 'nomic-ai/nomic-embed-text-v1', "display_name": "nomic-text-v1",
   "model_family": "HFEmbeddingModel",
   "model_category": "embedding", "model_location": "hf_repo", "embedding_dims": 768, "context_window": 8192,
   "link": "https://huggingface.co/nomic-ai/nomic-embed-text-v1", "custom_model_files": [], "custom_model_repo": "",
   "hf_repo": "nomic-ai/nomic-embed-text-v1"},

  {"model_name": 'jinaai/jina-embeddings-v2-base-en', "display_name": "jina-base-en-v2",
   "model_family": "HFEmbeddingModel",
   "model_category": "embedding", "model_location": "hf_repo", "embedding_dims": 768, "context_window": 8192,
   "link": "https://huggingface.co/jinaai/jina-embeddings-v2-base-en", "custom_model_files": [], "custom_model_repo": "",
   "hf_repo": "jinaai/jina-embeddings-v2-base-en"},

  {"model_name": 'jinaai/jina-embeddings-v2-small-en', "display_name": "jina-small-en-v2",
   "model_family": "HFEmbeddingModel",
   "model_category": "embedding", "model_location": "hf_repo", "embedding_dims": 512, "context_window": 8192,
   "link": "https://huggingface.co/jinaai/jina-embeddings-v2-small-en", "custom_model_files": [], "custom_model_repo": "",
   "hf_repo": "jinaai/jina-embeddings-v2-small-en"},

  # new add - reranker models
  {"model_name": 'jinaai/jina-reranker-v1-turbo-en', "display_name": "jina-reranker-turbo",
  "model_family": "HFReRankerModel",
  "model_category": "reranker", "model_location": "hf_repo", "embedding_dims": 384, "context_window": 8192,
  "link": "https://huggingface.co/jinaai/jina-reranker-v1-turbo-en", "custom_model_files": [],
  "custom_model_repo": "",
  "hf_repo": "jinaai/jina-reranker-v1-turbo-en"},

 {"model_name": 'jinaai/jina-reranker-v1-tiny-en', "display_name": "jina-reranker-tiny",
  "model_family": "HFReRankerModel",
  "model_category": "reranker", "model_location": "hf_repo", "embedding_dims": 384, "context_window": 8192,
  "link": "https://huggingface.co/jinaai/jina-reranker-v1-tiny-en", "custom_model_files": [],
  "custom_model_repo": "",
  "hf_repo": "jinaai/jina-reranker-v1-tiny-en"},
  # end - reranker models

 {"model_name": 'BAAI/bge-small-en-v1.5', "display_name": "bge-small-en-v1.5", "model_family": "HFEmbeddingModel",
   "model_category": "embedding", "model_location": "hf_repo", "embedding_dims": 384, "context_window": 512,
   "link": "https://huggingface.co/BAAI/bge-small-en-v1.5", "custom_model_files": [], "custom_model_repo": "",
   "hf_repo": "BAAI/bge-small-en-v1.5"},

  {"model_name": 'BAAI/bge-large-en-v1.5', "display_name": "bge-large-en-v1.5", "model_family": "HFEmbeddingModel",
   "model_category": "embedding", "model_location": "hf_repo", "embedding_dims": 1024, "context_window": 512,
   "link": "https://huggingface.co/BAAI/bge-large-en-v1.5", "custom_model_files": [], "custom_model_repo": "",
   "hf_repo": "BAAI/bge-large-en-v1.5"},

  {"model_name": 'BAAI/bge-base-en-v1.5', "display_name": "bge-base-en-v1.5", "model_family": "HFEmbeddingModel",
   "model_category": "embedding", "model_location": "hf_repo", "embedding_dims": 768, "context_window": 512,
   "link": "https://huggingface.co/BAAI/bge-base-en-v1.5", "custom_model_files": [], "custom_model_repo": "",
   "hf_repo": "BAAI/bge-base-en-v1.5"},

 {"model_name": "thenlper/gte-small", "display_name": "gte-small",
   "model_family": "HFEmbeddingModel",
   "model_category": "embedding", "model_location": "hf_repo", "embedding_dims": 384, "context_window": 512,
   "link": "https://huggingface.co/thenlper/gte-small", "custom_model_files": [], "custom_model_repo": "",
   "hf_repo": "thenlper/gte-small"},

  {"model_name": "thenlper/gte-base", "display_name": "gte-base",
   "model_family": "HFEmbeddingModel",
   "model_category": "embedding", "model_location": "hf_repo", "embedding_dims": 768, "context_window": 512,
   "link": "https://huggingface.co/thenlper/gte-base", "custom_model_files": [], "custom_model_repo": "",
   "hf_repo": "thenlper/gte-base"},

  {"model_name": "thenlper/gte-large", "display_name": "gte-large",
   "model_family": "HFEmbeddingModel",
   "model_category": "embedding", "model_location": "hf_repo", "embedding_dims": 1024, "context_window": 512,
   "link": "https://huggingface.co/thenlper/gte-large", "custom_model_files": [], "custom_model_repo": "",
   "hf_repo": "thenlper/gte-large"},

  {"model_name": 'llmrails/ember-v1', "display_name": "ember-v1",
   "model_family": "HFEmbeddingModel",
   "model_category": "embedding", "model_location": "hf_repo", "embedding_dims": 1024, "context_window": 512,
   "link": "https://huggingface.co/llmrails/ember-v1", "custom_model_files": [], "custom_model_repo": "",
   "hf_repo": "llmrails/ember-v1"},

  {"model_name": "WhereIsAI/UAE-Large-V1", "display_name": "uae-large-v1",
   "model_family": "HFEmbeddingModel",
   "model_category": "embedding", "model_location": "hf_repo", "embedding_dims": 1024, "context_window": 512,
   "link": "https://huggingface.co/WhereIsAI/UAE-Large-V1", "custom_model_files": [], "custom_model_repo": "",
   "hf_repo": "WhereIsAI/UAE-Large-V1"},

    # add open ai embeddings
    {"model_name": 'text-embedding-ada-002', "display_name": "OpenAI-Embedding", "model_family": "OpenAIEmbeddingModel",
     "model_category": "embedding", "model_location": "api", "context_window": 8191, "embedding_dims": 1536},

    {"model_name": 'text-embedding-3-small', "display_name": "OpenAI-Embedding", "model_family": "OpenAIEmbeddingModel",
     "model_category": "embedding", "model_location": "api", "context_window": 8191, "embedding_dims": 1536},

    {"model_name": 'text-embedding-3-large', "display_name": "OpenAI-Embedding", "model_family": "OpenAIEmbeddingModel",
     "model_category": "embedding", "model_location": "api", "context_window": 8191, "embedding_dims": 3072},

    # add cohere embeddings
    {"model_name": 'medium', "display_name": "Cohere-Medium-Embedding", "model_family": "CohereEmbeddingModel",
     "model_category": "embedding", "model_location": "api", "context_window": 2048, "embedding_dims": 4096},

    {"model_name": 'xlarge', "display_name": "Cohere-XLarge-Embedding", "model_family": "CohereEmbeddingModel",
     "model_category": "embedding", "model_location": "api", "context_window": 2048, "embedding_dims": 4096},

    # insert new cohere embedding model - v3 - announced first week of November 2023
    {"model_name": 'embed-english-v3.0', "display_name": "Cohere-English-v3", "model_family": "CohereEmbeddingModel",
     "model_category": "embedding", "model_location": "api",  "context_window": 2048, "embedding_dims": 1024},

    {"model_name": 'embed-multilingual-v3.0', "display_name": "Cohere-Multi-Lingual-v3", "model_family": "CohereEmbeddingModel",
     "model_category": "embedding", "model_location": "api", "context_window": 2048, "embedding_dims": 1024},

    {"model_name": 'embed-english-light-v3.0', "display_name": "Cohere-English-v3", "model_family": "CohereEmbeddingModel",
     "model_category": "embedding", "model_location": "api", "context_window": 2048, "embedding_dims": 384},

    {"model_name": 'embed-multilingual-light-v3.0', "display_name": "Cohere-English-v3",
     "model_family": "CohereEmbeddingModel", "model_category": "embedding", "model_location": "api",
     "context_window": 2048, "embedding_dims": 384},

    {"model_name": 'embed-english-v2.0', "display_name": "Cohere-English-v3",
     "model_family": "CohereEmbeddingModel", "model_category": "embedding", "model_location": "api",
     "context_window": 2048, "embedding_dims": 4096},

    {"model_name": 'embed-english-light-v2.0', "display_name": "Cohere-English-v3",
     "model_family": "CohereEmbeddingModel", "model_category": "embedding", "model_location": "api",
     "context_window": 2048, "embedding_dims": 1024},

    {"model_name": 'embed-multilingual-v2.0', "display_name": "Cohere-English-v3",
     "model_family": "CohereEmbeddingModel", "model_category": "embedding", "model_location": "api",
     "context_window": 2048, "embedding_dims": 768},
    # end - new cohere embeddings

    # add google embeddings - textembedding-gecko@001
    {"model_name": 'textembedding-gecko@latest', "display_name": "Google-Embedding", "model_family": "GoogleEmbeddingModel",
     "model_category": "embedding","model_location": "api", "context_window": 4000, "embedding_dims": 768},

    # generative-api models
    {"model_name": 'claude-v1', "display_name": "Anthropic Claude-v1", "model_family": "ClaudeModel",
     "model_category": "generative-api", "model_location": "api",  "context_window": 8000},
    {"model_name": 'claude-instant-v1', "display_name": "claude-instant-1.2", "model_family": "ClaudeModel",
     "model_category": "generative-api","model_location": "api", "context_window": 8000},

    # new Anthropic v3 models

    # please note: we have kept Claude-3 window in model_configs at 8192 - but actual model window is 200K
    #  if you pass a single passage of up to 200K, the model should work OK
    # --the shorter context window of 8192 will be applied as default in Prompt when batching up evidence chunks
    # --this can be configured and over-ridden if you prefer to use the full 200K window

    {"model_name": 'claude-3-opus-20240229', "display_name": "Anthropic-Claude-3-Opus", "model_family": "ClaudeModel",
     "model_category": "generative-api", "model_location": "api", "context_window": 8192},

    {"model_name": 'claude-3-sonnet-20240229', "display_name": "Anthropic-Claude-3-Sonnet", "model_family": "ClaudeModel",
     "model_category": "generative-api", "model_location": "api", "context_window": 8192},

    {"model_name": 'claude-2.1', "display_name": "Anthropic Claude-2.1", "model_family": "ClaudeModel",
    "model_category": "generative-api", "model_location": "api", "context_window": 8192},

    {"model_name": 'claude-2.0', "display_name": "Anthropic Claude-Claude2-.0",
    "model_family": "ClaudeModel", "model_category": "generative-api", "model_location": "api", "context_window": 8192},

    {"model_name": 'command-medium-nightly', "display_name": "Cohere Command Medium", "model_family": "CohereGenModel",
     "model_category": "generative-api","model_location": "api", "context_window": 2048},

    {"model_name": 'command-xlarge-nightly', "display_name": "Cohere Command XLarge", "model_family": "CohereGenModel",
     "model_category": "generative-api","model_location": "api", "context_window": 2048},

    {"model_name": 'summarize-xlarge', "display_name": "Cohere Summarize Xlarge", "model_family": "CohereGenModel",
     "model_category":"generative-api","model_location": "api", "context_window": 2048},
    {"model_name": 'summarize-medium', "display_name": "Cohere Summarize Medium", "model_family": "CohereGenModel",
     "model_category":"generative-api","model_location": "api", "context_window": 2048},
    {"model_name": 'j2-jumbo-instruct', "display_name": "Jurassic-2-Jumbo-Instruct", "model_family": "JurassicModel",
     "model_category":"generative-api", "model_location": "api", "context_window": 2048},
    {"model_name": 'j2-grande-instruct', "display_name": "Jurassic-2-Grande-Instruct", "model_family": "JurassicModel",
     "model_category":"generative-api","model_location": "api", "context_window": 2048},
    {"model_name": 'text-bison@001', "display_name": "Google Palm", "model_family": "GoogleGenModel",
     "model_category": "generative-api","model_location": "api", "context_window": 8192},
    {"model_name": 'chat-bison@001', "display_name": "Google Chat", "model_family": "GoogleGenModel",
     "model_category": "generative-api","model_location": "api", "context_window": 8192},
    {"model_name": 'text-davinci-003', "display_name": "GPT3-Davinci", "model_family": "OpenAIGenModel",
     "model_category": "generative-api","model_location": "api", "context_window": 4096},
    {"model_name": 'text-curie-001', "display_name": "GPT3-Curie", "model_family": "OpenAIGenModel",
     "model_category": "generative-api","model_location": "api", "context_window": 2048},
    {"model_name": 'text-babbage-001', "display_name": "GPT3-Babbage", "model_family": "OpenAIGenModel",
     "model_category": "generative-api","model_location": "api", "context_window": 2048},
    {"model_name": 'text-ada-001', "display_name": "GPT3-Ada", "model_family": "OpenAIGenModel",
     "model_category": "generative-api","model_location": "api", "context_window": 2048},
    {"model_name": "gpt-3.5-turbo", "display_name": "ChatGPT", "model_family": "OpenAIGenModel",
     "model_category": "generative-api","model_location": "api", "context_window": 4000},

    # gpt-4
    {"model_name": "gpt-4", "display_name": "GPT-4", "model_family": "OpenAIGenModel",
     "model_category": "generative-api", "model_location": "api", "context_window": 8000},

    # gpt-3.5-turbo-instruct
    {"model_name": "gpt-3.5-turbo-instruct", "display_name": "GPT-3.5-Instruct", "model_family": "OpenAIGenModel",
     "model_category": "generative-api", "model_location": "api", "context_window": 4000},

    # gpt-4 model announced in November 2023
    {"model_name": "gpt-4-1106-preview", "display_name": "GPT-4-Turbo-1106", "model_family": "OpenAIGenModel",
     "model_category": "generative-api", "model_location": "api", "context_window": 128000},

    # gpt-3.5 model announced in November 2023
    {"model_name": "gpt-3.5-turbo-1106", "display_name": "GPT-3.5-Turbo-1106", "model_family": "OpenAIGenModel",
     "model_category": "generative-api", "model_location": "api", "context_window": 16385},

    # gpt-4 model announced in January 2024
    {"model_name": "gpt-4-0125-preview", "display_name": "GPT-4-Turbo-0125", "model_family": "OpenAIGenModel",
     "model_category": "generative-api", "model_location": "api", "context_window": 128000},
  
    # gpt-3.5 model announced in January 2024
    {"model_name": "gpt-3.5-turbo-0125", "display_name": "GPT-3.5-Turbo-0125", "model_family": "OpenAIGenModel",
     "model_category": "generative-api", "model_location": "api", "context_window": 16385},

    # gpt-4o model announced in May 2024
    {"model_name": "gpt-4o", "display_name": "GPT-4o", "model_family": "OpenAIGenModel",
     "model_category": "generative-api", "model_location": "api", "context_window": 128000},

    # gpt-4o-2024-0513 model announced in May 2024
    {"model_name": "gpt-4o-2024-05-13", "display_name": "gpt-4o-2024-05-13", "model_family": "OpenAIGenModel",
     "model_category": "generative-api", "model_location": "api", "context_window": 128000},

    # add api-based llmware custom model
    {"model_name": "llmware-inference-server", "display_name": "LLMWare-GPT", "model_family": "LLMWareModel",
     "model_category": "generative-api", "model_location": "api", "context_window": 2048},

    # core llmware bling open source models available in catalog directly
    {"model_name": "llmware/bling-1.4b-0.1", "display_name": "bling-1.4b", "model_family": "HFGenerativeModel",
     "model_category": "generative_local", "model_location": "hf_repo", "context_window": 2048,
     "instruction_following": False, "prompt_wrapper": "human_bot", "temperature": 0.3, "trailing_space":"",
     "link": "https://huggingface.co/llmware/bling-1.4b-0.1",
     "custom_model_files": [], "custom_model_repo": "",
     "hf_repo": "llmware/bling-1.4b-0.1"},

    {"model_name": "llmware/bling-1b-0.1", "display_name": "bling-1b", "model_family": "HFGenerativeModel",
     "model_category": "generative_local", "model_location": "hf_repo", "context_window": 2048,
     "instruction_following": False, "prompt_wrapper": "human_bot", "temperature": 0.3, "trailing_space": "",
     "link": "https://huggingface.co/llmware/bling-1b-0.1",
     "custom_model_files": [], "custom_model_repo": "",
     "hf_repo": "llmware/bling-1b-0.1"},

    {"model_name": "llmware/bling-falcon-1b-0.1", "display_name": "bling-falcon-1.3b", "model_family": "HFGenerativeModel",
     "model_category": "generative_local", "model_location": "hf_repo", "context_window": 2048,
     "instruction_following": False, "prompt_wrapper": "human_bot", "temperature": 0.3, "trailing_space": "",
     "link": "https://huggingface.co/llmware/bling-falcon-1b-0.1",
     "custom_model_files": [], "custom_model_repo": "",
     "hf_repo": "llmware/bling-falcon-1b-0.1"
     },

    {"model_name": "llmware/bling-sheared-llama-1.3b-0.1", "display_name": "bling-sheared-llama-1.3b",
     "model_family": "HFGenerativeModel", "model_category": "generative_local", "model_location": "hf_repo",
     "context_window": 2048, "instruction_following": False, "prompt_wrapper": "human_bot",
     "temperature": 0.3, "trailing_space": "", "link": "https://huggingface.co/llmware/bling-sheared-llama-1.3b-0.1",
     "custom_model_files": [], "custom_model_repo": "",
     "hf_repo": "llmware/bling-sheared-llama-1.3b-0.1"
     },

    {"model_name": "llmware/bling-red-pajamas-3b-0.1", "display_name": "bling-red-pajamas-3b",
     "model_family": "HFGenerativeModel", "model_category": "generative_local", "model_location": "hf_repo",
     "context_window": 2048, "instruction_following": False, "prompt_wrapper": "human_bot",
     "temperature": 0.3, "trailing_space": "", "link": "https://huggingface.co/llmware/bling-red-pajamas-3b-0.1",
     "custom_model_files": [], "custom_model_repo": "",
     "hf_repo": "llmware/bling-red-pajamas-3b-0.1"},

    {"model_name": "llmware/bling-sheared-llama-2.7b-0.1", "display_name": "bling-sheared-llama-2.7b",
     "model_family": "HFGenerativeModel", "model_category": "generative_local", "model_location": "hf_repo",
     "context_window": 2048, "instruction_following": False, "prompt_wrapper": "human_bot",
     "temperature": 0.3, "trailing_space": "", "link": "https://huggingface.co/llmware/bling-sheared-llama-2.7b-0.1",
     "custom_model_files": [], "custom_model_repo": "",
     "hf_repo": "llmware/bling-sheared-llama-2.7b-0.1"},

    {"model_name": "llmware/bling-stable-lm-3b-4e1t-v0", "display_name": "bling-stablelm-3b",
     "model_family": "HFGenerativeModel", "model_category": "generative_local", "model_location": "hf_repo",
     "context_window": 2048, "instruction_following": False, "prompt_wrapper": "human_bot",
     "temperature": 0.3, "trailing_space": "", "link": "https://huggingface.co/llmware/bling-stable-lm-3b-4e1t-v0",
     "custom_model_files": [], "custom_model_repo": "",
     "hf_repo": "llmware/bling-stable-lm-3b-4e1t-v0"},

    {"model_name": "llmware/bling-cerebras-1.3b-0.1", "display_name": "bling-cerebras-1.3b",
     "model_family": "HFGenerativeModel", "model_category": "generative_local", "model_location": "hf_repo",
     "context_window": 2048, "instruction_following": False, "prompt_wrapper": "human_bot",
     "temperature": 0.3, "trailing_space": "", "link": "https://huggingface.co/llmware/bling-cerebras-1.3b-0.1",
     "custom_model_files": [], "custom_model_repo": "",
     "hf_repo": "llmware/bling-cerebras-1.3b-0.1"},

    {"model_name": "llmware/bling-tiny-llama-v0", "display_name": "bling-tiny-llama-1b",
     "model_family": "HFGenerativeModel", "model_category": "generative_local", "model_location": "hf_repo",
     "context_window": 2048, "instruction_following": False, "prompt_wrapper": "human_bot",
     "temperature": 0.3, "trailing_space": "", "link": "https://huggingface.co/llmware/bling-tiny-llama-v0",
     "custom_model_files": [], "custom_model_repo": "",
     "hf_repo": "llmware/bling-tiny-llama-v0"},

    # create dragon models
    {"model_name": "llmware/dragon-yi-6b-v0", "display_name": "dragon-yi-6b",
     "model_family": "HFGenerativeModel", "model_category": "generative_local", "model_location": "hf_repo",
     "context_window": 4096, "instruction_following": False, "prompt_wrapper": "human_bot",
     "temperature": 0.3, "trailing_space": "\n", "link": "https://huggingface.co/llmware/dragon-yi-6b-v0",
     "custom_model_files": [], "custom_model_repo": "",
     "hf_repo": "llmware/dragon-yi-6b-v0"},

    {"model_name": "llmware/dragon-stablelm-7b-v0", "display_name": "dragon-stablelm-7b",
     "model_family": "HFGenerativeModel", "model_category": "generative_local", "model_location": "hf_repo",
     "context_window": 2048, "instruction_following": False, "prompt_wrapper": "human_bot",
     "temperature": 0.3, "trailing_space": "", "link": "https://huggingface.co/llmware/dragon-stablelm-7b-v0",
     "custom_model_files": [], "custom_model_repo": "",
     "hf_repo": "llmware/dragon-stablelm-7b-v0"},

    {"model_name": "llmware/dragon-mistral-7b-v0", "display_name": "dragon-mistral-7b",
     "model_family": "HFGenerativeModel", "model_category": "generative_local", "model_location": "hf_repo",
     "context_window": 4096, "instruction_following": False, "prompt_wrapper": "human_bot",
     "temperature": 0.3, "trailing_space": "", "link": "https://huggingface.co/llmware/dragon-mistral-7b-v0",
     "custom_model_files": [], "custom_model_repo": "",
     "hf_repo": "llmware/dragon-mistral-7b-v0"},

 {"model_name": "llmware/dragon-mistral-0.3", "display_name": "dragon-mistral-0.3",
  "model_family": "HFGenerativeModel", "model_category": "generative_local", "model_location": "hf_repo",
  "context_window": 4096, "instruction_following": False, "prompt_wrapper": "human_bot",
  "temperature": 0.3, "trailing_space": "", "link": "https://huggingface.co/llmware/dragon-mistral-0.3",
  "custom_model_files": [], "custom_model_repo": "",
  "hf_repo": "llmware/dragon-mistral-0.3"},

 {"model_name": "llmware/dragon-qwen-7b", "display_name": "dragon-qwen-7b",
  "model_family": "HFGenerativeModel", "model_category": "generative_local", "model_location": "hf_repo",
  "context_window": 4096, "instruction_following": False, "prompt_wrapper": "human_bot",
  "temperature": 0.3, "trailing_space": "", "link": "https://huggingface.co/llmware/dragon-qwen-7b",
  "custom_model_files": [], "custom_model_repo": "",
  "hf_repo": "llmware/dragon-qwen-7b"},

 {"model_name": "llmware/dragon-red-pajama-7b-v0", "display_name": "dragon-red-pajama-7b",
     "model_family": "HFGenerativeModel", "model_category": "generative_local", "model_location": "hf_repo",
     "context_window": 2048, "instruction_following": False, "prompt_wrapper": "human_bot",
     "temperature": 0.3, "trailing_space": "", "link": "https://huggingface.co/llmware/dragon-red-pajama-7b-v0",
     "custom_model_files": [], "custom_model_repo": "",
     "hf_repo": "llmware/dragon-red-pajama-7b-v0"},

    {"model_name": "llmware/dragon-deci-6b-v0", "display_name": "dragon-deci-6b",
     "model_family": "HFGenerativeModel", "model_category": "generative_local", "model_location": "hf_repo",
     "context_window": 2048, "instruction_following": False, "prompt_wrapper": "human_bot",
     "temperature": 0.3, "trailing_space": "", "link": "https://huggingface.co/llmware/dragon-deci-6b-v0",
     "custom_model_files": [], "custom_model_repo": "",
     "hf_repo": "llmware/dragon-deci-6b-v0"},

    {"model_name": "llmware/dragon-falcon-7b-v0", "display_name": "dragon-falcon-7b",
     "model_family": "HFGenerativeModel", "model_category": "generative_local", "model_location": "hf_repo",
     "context_window": 2048, "instruction_following": False, "prompt_wrapper": "human_bot",
     "temperature": 0.3, "trailing_space": "", "link": "https://huggingface.co/llmware/dragon-falcon-7b-v0",
     "custom_model_files": [], "custom_model_repo": "",
     "hf_repo": "llmware/dragon-falcon-7b-v0"},

    {"model_name": "llmware/dragon-llama-7b-v0", "display_name": "dragon-llama-7b",
     "model_family": "HFGenerativeModel", "model_category": "generative_local", "model_location": "hf_repo",
     "context_window": 2048, "instruction_following": False, "prompt_wrapper": "human_bot",
     "temperature": 0.3, "trailing_space": "", "link": "https://huggingface.co/llmware/dragon-llama-7b-v0",
     "custom_model_files": [], "custom_model_repo": "",
     "hf_repo": "llmware/dragon-llama-7b-v0"},

    {"model_name": "llmware/dragon-deci-7b-v0", "display_name": "dragon-deci-7b",
     "model_family": "HFGenerativeModel", "model_category": "generative_local", "model_location": "hf_repo",
     "context_window": 2048, "instruction_following": False, "prompt_wrapper": "human_bot",
     "temperature": 0.3, "trailing_space": "", "link": "https://huggingface.co/llmware/dragon-deci-7b-v0",
     "custom_model_files": [], "custom_model_repo": "",
     "hf_repo": "llmware/dragon-deci-7b-v0"},

 {"model_name": "llmware/dragon-llama-3.1", "display_name": "dragon-llama-3.1",
  "model_family": "HFGenerativeModel", "model_category": "generative_local", "model_location": "hf_repo",
  "context_window": 4096, "instruction_following": False, "prompt_wrapper": "human_bot",
  "temperature": 0.3, "trailing_space": "", "link": "https://huggingface.co/llmware/dragon-llama-3.1",
  "custom_model_files": [], "custom_model_repo": "",
  "hf_repo": "llmware/dragon-llama-3.1"},

 # adding bling-phi-3
   {"model_name": "llmware/bling-phi-3", "display_name": "bling-phi-3",
    "model_family": "HFGenerativeModel", "model_category": "generative_local", "model_location": "hf_repo",
    "context_window": 4096, "instruction_following": False, "prompt_wrapper": "human_bot",
    "temperature": 0.0, "trailing_space": "", "link": "https://huggingface.co/llmware/bling-phi-3",
    "custom_model_files": [], "custom_model_repo": "",
    "hf_repo": "llmware/bling-phi-3"},

 # adding bling-phi-3.5
 {"model_name": "llmware/bling-phi-3.5", "display_name": "bling-phi-3.5",
  "model_family": "HFGenerativeModel", "model_category": "generative_local", "model_location": "hf_repo",
  "context_window": 4096, "instruction_following": False, "prompt_wrapper": "human_bot",
  "temperature": 0.0, "trailing_space": "", "link": "https://huggingface.co/llmware/bling-phi-3.5",
  "custom_model_files": [], "custom_model_repo": "",
  "hf_repo": "llmware/bling-phi-3.5"},

 # gguf models
   {"model_name": "bling-phi-3-gguf", "display_name": "llmware/bling-phi-3-gguf",
    "model_family": "GGUFGenerativeModel", "model_category": "generative_local", "model_location": "llmware_repo",
    "context_window": 4096, "instruction_following": False, "prompt_wrapper": "human_bot",
    "temperature": 0.0, "trailing_space": "",
    "gguf_file": "bling-phi-3.gguf",
    "gguf_repo": "llmware/bling-phi-3-gguf",
    "fetch": {"module": "llmware.models", "method": "pull_snapshot_from_hf"},
    "validation_files": ["bling-phi-3.gguf"],
    "tokenizer_local": "tokenizer_phi3.json",
    "link": "https://huggingface.co/llmware/bling-phi-3-gguf",
    "custom_model_files": [], "custom_model_repo": ""},

 # NEW
 {"model_name": "bling-phi-3.5-gguf", "display_name": "llmware/bling-phi-3.5-gguf",
  "model_family": "GGUFGenerativeModel", "model_category": "generative_local", "model_location": "llmware_repo",
  "context_window": 4096, "instruction_following": False, "prompt_wrapper": "human_bot",
  "temperature": 0.0, "trailing_space": "",
  "gguf_file": "bling-phi3-5.gguf",
  "gguf_repo": "llmware/bling-phi-3.5-gguf",
  "fetch": {"module": "llmware.models", "method": "pull_snapshot_from_hf"},
  "validation_files": ["bling-phi3-5.gguf"],
  "tokenizer_local": "tokenizer_phi3.json",
  "link": "https://huggingface.co/llmware/bling-phi-3.5-gguf",
  "custom_model_files": [], "custom_model_repo": ""},

 # NEW
 {"model_name": "dragon-llama-3.1-gguf", "display_name": "llmware/dragon-llama-3.1-gguf",
  "model_family": "GGUFGenerativeModel", "model_category": "generative_local", "model_location": "llmware_repo",
  "context_window": 4096, "instruction_following": False, "prompt_wrapper": "human_bot",
  "temperature": 0.0, "trailing_space": "",
  "gguf_file": "dragon-llama31.gguf",
  "gguf_repo": "llmware/dragon-llama-3.1-gguf",
  "fetch": {"module": "llmware.models", "method": "pull_snapshot_from_hf"},
  "validation_files": ["dragon-llama31.gguf"],
  "tokenizer_local": "tokenizer_phi3.json",
  "link": "https://huggingface.co/llmware/dragon-llama-3.1-gguf",
  "custom_model_files": [], "custom_model_repo": ""},

# NEW
 {"model_name": "dragon-mistral-0.3-gguf", "display_name": "llmware/dragon-mistral-0.3-gguf",
  "model_family": "GGUFGenerativeModel", "model_category": "generative_local", "model_location": "llmware_repo",
  "context_window": 4096, "instruction_following": False, "prompt_wrapper": "human_bot",
  "temperature": 0.0, "trailing_space": "",
  "gguf_file": "dragon-mistral-03.gguf",
  "gguf_repo": "llmware/dragon-mistral-0.3-gguf",
  "fetch": {"module": "llmware.models", "method": "pull_snapshot_from_hf"},
  "validation_files": ["dragon-mistral-03.gguf"],
  "tokenizer_local": "tokenizer_phi3.json",
  "link": "https://huggingface.co/llmware/dragon-mistral-0.3-gguf",
  "custom_model_files": [], "custom_model_repo": ""},

 # NEW
 {"model_name": "bling-phi-2-gguf", "display_name": "llmware/bling-phi-2-gguf",
  "model_family": "GGUFGenerativeModel", "model_category": "generative_local", "model_location": "llmware_repo",
  "context_window": 4096, "instruction_following": False, "prompt_wrapper": "human_bot",
  "temperature": 0.0, "trailing_space": "",
  "gguf_file": "bling-phi2-tool.gguf",
  "gguf_repo": "llmware/bling-phi-2-gguf",
  "fetch": {"module": "llmware.models", "method": "pull_snapshot_from_hf"},
  "validation_files": ["bling-phi2-tool.gguf"],
  "tokenizer_local": "tokenizer_phi2.json",
  "link": "https://huggingface.co/llmware/bling-phi-2-gguf",
  "custom_model_files": [], "custom_model_repo": ""},

 # NEW
 {"model_name": "dragon-yi-9b-gguf", "display_name": "llmware/dragon-yi-9b-gguf",
  "model_family": "GGUFGenerativeModel", "model_category": "generative_local", "model_location": "llmware_repo",
  "context_window": 4096, "instruction_following": False, "prompt_wrapper": "human_bot",
  "temperature": 0.0, "trailing_space": "",
  "gguf_file": "dragon-yi-1-5-9.gguf",
  "gguf_repo": "llmware/dragon-yi-9b-gguf",
  "fetch": {"module": "llmware.models", "method": "pull_snapshot_from_hf"},
  "validation_files": ["dragon-yi-1-5-9.gguf"],
  "tokenizer_local": "tokenizer_yi.json",
  "link": "https://huggingface.co/llmware/dragon-yi-9b-gguf",
  "custom_model_files": [], "custom_model_repo": ""},

 # NEW
 {"model_name": "dragon-qwen-7b-gguf", "display_name": "llmware/dragon-qwen-7b-gguf",
  "model_family": "GGUFGenerativeModel", "model_category": "generative_local", "model_location": "llmware_repo",
  "context_window": 4096, "instruction_following": False, "prompt_wrapper": "human_bot",
  "temperature": 0.0, "trailing_space": "",
  "gguf_file": "dragon-qwen.gguf",
  "gguf_repo": "llmware/dragon-qwen-7b-gguf",
  "fetch": {"module": "llmware.models", "method": "pull_snapshot_from_hf"},
  "validation_files": ["dragon-qwen.gguf"],
  "tokenizer_local": "tokenizer_qw.json",
  "link": "https://huggingface.co/llmware/dragon-qwen-7b-gguf",
  "custom_model_files": [], "custom_model_repo": ""},

 # NEW
 {"model_name": "bling-qwen-1.5b-gguf", "display_name": "bling-qwen-mini-tool",
  "model_family": "GGUFGenerativeModel", "model_category": "generative_local", "model_location": "llmware_repo",
  "context_window": 4096, "instruction_following": False, "prompt_wrapper": "human_bot",
  "temperature": 0.0, "trailing_space": "",
  "gguf_file": "bling-qwen-1-5b.gguf",
  "gguf_repo": "llmware/bling-qwen-mini-tool",
  "fetch": {"module": "llmware.models", "method": "pull_snapshot_from_hf"},
  "validation_files": ["bling-qwen-1-5b.gguf"],
  "tokenizer_local": "tokenizer_qw.json",
  "link": "https://huggingface.co/llmware/bling-qwen-1.5b-gguf",
  "custom_model_files": [], "custom_model_repo": ""},

 # NEW
 {"model_name": "bling-qwen-0.5b-gguf", "display_name": "llmware/bling-qwen-nano-tool",
  "model_family": "GGUFGenerativeModel", "model_category": "generative_local", "model_location": "llmware_repo",
  "context_window": 4096, "instruction_following": False, "prompt_wrapper": "human_bot",
  "temperature": 0.0, "trailing_space": "",
  "gguf_file": "bling-qwen-0-5.gguf",
  "gguf_repo": "llmware/bling-qwen-nano-tool",
  "fetch": {"module": "llmware.models", "method": "pull_snapshot_from_hf"},
  "validation_files": ["bling-qwen-0-5.gguf"],
  "tokenizer_local": "tokenizer_qw.json",
  "link": "https://huggingface.co/llmware/bling-qwen-nano-tool",
  "custom_model_files": [], "custom_model_repo": ""},

 # deprecated access to dragon-mistral-7b-gguf -> replaced by dragon-mistral-answer-tool
    {"model_name": "llmware/dragon-mistral-7b-gguf", "display_name": "dragon-mistral-7b-gguf",
     "model_family": "GGUFGenerativeModel", "model_category": "generative_local", "model_location": "llmware_repo",
     "context_window": 4096, "instruction_following": False, "prompt_wrapper": "human_bot",
     "fetch": {"module": "llmware.models", "method": "pull_model_from_hf"},
     "validation_files": ["dragon-mistral-7b-q4_k_m.gguf"],
     "temperature": 0.3, "trailing_space": "",
     "gguf_file": "dragon-mistral-7b-q4_k_m.gguf",
     "gguf_repo": "llmware/dragon-mistral-7b-v0",
     "link": "https://huggingface.co/llmware/dragon-mistral-7b-v0",
     "custom_model_files": [], "custom_model_repo": ""},

    # deprecated access to dragon-llama-7b-gguf -> replaced by dragon-llama-answer-tool
    {"model_name": "llmware/dragon-llama-7b-gguf", "display_name": "dragon-llama-7b-gguf",
     "model_family": "GGUFGenerativeModel", "model_category": "generative_local", "model_location": "llmware_repo",
     "context_window": 2048, "instruction_following": False, "prompt_wrapper": "human_bot",
     "temperature": 0.3, "trailing_space": "",
     "gguf_file": "dragon-llama-7b-q4_k_m.gguf",
     "gguf_repo": "llmware/dragon-llama-7b-v0",
     "fetch": {"module": "llmware.models", "method": "pull_model_from_hf"},
     "validation_files": ["dragon-llama-7b-q4_k_m.gguf"],
     "link": "https://huggingface.co/llmware/dragon-llama-7b-v0",
     "custom_model_files": [], "custom_model_repo": ""},

    # deprecated access to dragon-yi-6b-gguf -> replaced by dragon-yi-answer-tool
    {"model_name": "llmware/dragon-yi-6b-gguf", "display_name": "dragon-yi-6b-gguf",
     "model_family": "GGUFGenerativeModel", "model_category": "generative_local", "model_location": "llmware_repo",
     "context_window": 4096, "instruction_following": False, "prompt_wrapper": "human_bot",
     "temperature": 0.3, "trailing_space": "\n",
     "gguf_file": "dragon-yi-6b-q4_k_m.gguf",
     "gguf_repo": "llmware/dragon-yi-6b-v0",
     "fetch": {"module": "llmware.models", "method": "pull_model_from_hf"},
     "validation_files": ["dragon-yi-6b-q4_k_m.gguf"],
     "link": "https://huggingface.co/llmware/dragon-yi-6b-v0",
     "custom_model_files": [], "custom_model_repo": ""},

   {"model_name": "dragon-yi-answer-tool", "display_name": "dragon-yi-6b-answer-tool",
    "model_family": "GGUFGenerativeModel", "model_category": "generative_local", "model_location": "llmware_repo",
    "context_window": 4096, "instruction_following": False, "prompt_wrapper": "human_bot",
    "temperature": 0.3, "trailing_space": "\n",
    "gguf_file": "dragon-yi.gguf",
    "gguf_repo": "llmware/dragon-yi-answer-tool",
    "fetch": {"module": "llmware.models", "method": "pull_snapshot_from_hf"},
    "validation_files": ["dragon-yi.gguf"],
    "link": "https://huggingface.co/llmware/dragon-yi-answer-tool",
    "custom_model_files": [], "custom_model_repo": ""},

   {"model_name": "dragon-llama-answer-tool", "display_name": "dragon-llama-answer-tool",
    "model_family": "GGUFGenerativeModel", "model_category": "generative_local", "model_location": "llmware_repo",
    "context_window": 2048, "instruction_following": False, "prompt_wrapper": "human_bot",
    "temperature": 0.3, "trailing_space": "",
    "gguf_file": "dragon-llama.gguf",
    "gguf_repo": "llmware/dragon-llama-answer-tool",
    "fetch": {"module": "llmware.models", "method": "pull_snapshot_from_hf"},
    "validation_files": ["dragon-llama.gguf"],
    "link": "https://huggingface.co/llmware/dragon-llama-answer-tool",
    "custom_model_files": [], "custom_model_repo": ""},

   {"model_name": "dragon-mistral-answer-tool", "display_name": "dragon-mistral-answer-tool",
    "model_family": "GGUFGenerativeModel", "model_category": "generative_local", "model_location": "llmware_repo",
    "context_window": 4096, "instruction_following": False, "prompt_wrapper": "human_bot",
    "temperature": 0.3, "trailing_space": "",
    "gguf_file": "dragon-mistral.gguf",
    "gguf_repo": "llmware/dragon-mistral-answer-tool",
    "fetch": {"module": "llmware.models", "method": "pull_snapshot_from_hf"},
    "validation_files": ["dragon-mistral.gguf"],
    "link": "https://huggingface.co/llmware/dragon-mistral-answer-tool",
    "custom_model_files": [], "custom_model_repo": ""},


 # selected top HF open source chat models - gguf

 # NEW
{"model_name": "phi-3.5-gguf", "display_name": "phi-3.5-gguf",
  "model_family": "GGUFGenerativeModel", "model_category": "generative_local", "model_location": "llmware_repo",
  "context_window": 2048, "instruction_following": True, "prompt_wrapper": "phi_3",
  "temperature": 0.3, "trailing_space": "",
  "gguf_file": "phi35.gguf",
  "gguf_repo": "llmware/bonchon",
  "fetch": {"module": "llmware.models", "method": "pull_model_from_hf"},
  "validation_files": ["phi35.gguf"],
  "link": "https://huggingface.co/llmware/bonchon",
  "custom_model_files": [], "custom_model_repo": ""},

 # NEW
{"model_name": "qwen2-7B-instruct-gguf", "display_name": "qwen2-7B-instruct-gguf",
  "model_family": "GGUFGenerativeModel", "model_category": "generative_local", "model_location": "llmware_repo",
  "context_window": 2048, "instruction_following": True, "prompt_wrapper": "hf_chat",
  "temperature": 0.3, "trailing_space": "",
  "gguf_file": "qwen2-7b-instruct.gguf",
  "gguf_repo": "llmware/bonchon",
  "fetch": {"module": "llmware.models", "method": "pull_model_from_hf"},
  "validation_files": ["qwen2-7b-instruct.gguf"],
  "link": "https://huggingface.co/llmware/bonchon",
  "custom_model_files": [], "custom_model_repo": ""},

# NEW
{"model_name": "qwen2-1.5b-instruct-gguf", "display_name": "qwen2-1.5b-instruct-gguf",
  "model_family": "GGUFGenerativeModel", "model_category": "generative_local", "model_location": "llmware_repo",
  "context_window": 2048, "instruction_following": True, "prompt_wrapper": "hf_chat",
  "temperature": 0.3, "trailing_space": "",
  "gguf_file": "qwen-instruct-1-5b.gguf",
  "gguf_repo": "llmware/bonchon",
  "fetch": {"module": "llmware.models", "method": "pull_model_from_hf"},
  "validation_files": ["qwen-instruct-1-5b.gguf"],
  "link": "https://huggingface.co/llmware/bonchon",
  "custom_model_files": [], "custom_model_repo": ""},

# NEW
{"model_name": "qwen2-0.5b-instruct-gguf", "display_name": "qwen2-0.5b-instruct-gguf",
  "model_family": "GGUFGenerativeModel", "model_category": "generative_local", "model_location": "llmware_repo",
  "context_window": 2048, "instruction_following": True, "prompt_wrapper": "hf_chat",
  "temperature": 0.3, "trailing_space": "",
  "gguf_file": "qwen2-0_5b-instruct-q4_k_m.gguf",
  "gguf_repo": "llmware/bonchon",
  "fetch": {"module": "llmware.models", "method": "pull_model_from_hf"},
  "validation_files": ["qwen2-0_5b-instruct-q4_k_m.gguf"],
  "link": "https://huggingface.co/llmware/bonchon",
  "custom_model_files": [], "custom_model_repo": ""},

#TODO:  NEW = meta-llama/Meta-Llama-3.1-8B-Instruct
{"model_name": "llama-3.1-instruct-gguf", "display_name": "llama-3.1-instruct-gguf",
  "model_family": "GGUFGenerativeModel", "model_category": "generative_local", "model_location": "llmware_repo",
  "context_window": 2048, "instruction_following": True, "prompt_wrapper": "hf_chat",
  "temperature": 0.3, "trailing_space": "",
  "gguf_file": "llama-31-instruct.gguf",
  "gguf_repo": "llmware/bonchon",
  "fetch": {"module": "llmware.models", "method": "pull_model_from_hf"},
  "validation_files": ["llama-31-instruct.gguf"],
  "link": "https://huggingface.co/llmware/bonchon",
  "custom_model_files": [], "custom_model_repo": ""},


 {"model_name": "TheBloke/Llama-2-7B-Chat-GGUF", "display_name": "llama-2-7b-chat-gguf",
     "model_family": "GGUFGenerativeModel", "model_category": "generative_local", "model_location": "llmware_repo",
     "context_window": 2048, "instruction_following": True, "prompt_wrapper": "<INST>",
     "temperature": 0.3, "trailing_space": "",
     "gguf_file": "llama-2-7b-chat.Q4_K_M.gguf",
     "gguf_repo": "llmware/bonchon",
     "fetch": {"module": "llmware.models", "method": "pull_model_from_hf"},
     "validation_files": ["llama-2-7b-chat.Q4_K_M.gguf"],
     "link": "https://huggingface.co/llmware/bonchon",
     "custom_model_files": [], "custom_model_repo": ""},

    {"model_name": "TheBloke/OpenHermes-2.5-Mistral-7B-GGUF", "display_name": "openhermes-mistral-7b-gguf",
     "model_family": "GGUFGenerativeModel", "model_category": "generative_local", "model_location": "llmware_repo",
     "context_window": 4096, "instruction_following": True, "prompt_wrapper": "chat_ml",
     "temperature": 0.3, "trailing_space": "",
     "gguf_file": "openhermes-2.5-mistral-7b.Q4_K_M.gguf",
     "gguf_repo": "llmware/bonchon",
     "fetch": {"module": "llmware.models", "method": "pull_model_from_hf"},
     "validation_files": ["openhermes-2.5-mistral-7b.Q4_K_M.gguf"],
     "link": "https://huggingface.co/llmware/bonchon",
     "custom_model_files": [], "custom_model_repo": ""},

    {"model_name": "TheBloke/zephyr-7B-beta-GGUF", "display_name": "zephyr-7b-gguf",
     "model_family": "GGUFGenerativeModel", "model_category": "generative_local", "model_location": "llmware_repo",
     "context_window": 4096, "instruction_following": True, "prompt_wrapper": "hf_chat",
     "temperature": 0.3, "trailing_space": "",
     "gguf_file": "zephyr-7b-beta.Q4_K_M.gguf",
     "gguf_repo": "llmware/bonchon",
     "fetch": {"module": "llmware.models", "method": "pull_model_from_hf"},
     "validation_files": ["zephyr-7b-beta.Q4_K_M.gguf"],
     "link": "https://huggingface.co/llmware/bonchon",
     "custom_model_files": [], "custom_model_repo": ""},

    {"model_name": "TheBloke/Starling-LM-7B-alpha-GGUF", "display_name": "starling-7b-gguf",
     "model_family": "GGUFGenerativeModel", "model_category": "generative_local", "model_location": "llmware_repo",
     "context_window": 8192, "instruction_following": True, "prompt_wrapper": "open_chat",
     "temperature": 0.3, "trailing_space": "",
     "gguf_file": "starling-lm-7b-alpha.Q4_K_M.gguf",
     "gguf_repo": "llmware/bonchon",
     "fetch": {"module": "llmware.models", "method": "pull_model_from_hf"},
     "validation_files": ["starling-lm-7b-alpha.Q4_K_M.gguf"],
     "link": "https://huggingface.co/llmware/bonchon",
     "custom_model_files": [], "custom_model_repo": ""},

    {"model_name": "microsoft/Phi-3-mini-4k-instruct-gguf", "display_name": "phi-3-gguf", "model_family": "GGUFGenerativeModel",
     "model_category": "generative_local", "model_location": "llmware_repo", "context_window": 4096,
     "instruction_following": False, "prompt_wrapper": "phi_3", "temperature": 0.3, "trailing_space": "",
     "gguf_file": "Phi-3-mini-4k-instruct-q4.gguf",
     "gguf_repo": "microsoft/Phi-3-mini-4k-instruct-gguf",
     "link": "https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf",
     "tokenizer_local": "tokenizer_phi3.json",
     "fetch": {"module": "llmware.models", "method": "pull_model_from_hf"},
     "validation_files": ["Phi-3-mini-4k-instruct-q4.gguf"],
     "custom_model_files": [], "custom_model_repo": ""},

    {"model_name": "microsoft/Phi-3-mini-4k-instruct", "display_name": "phi-3",
     "model_family": "HFGenerativeModel", "model_category": "generative_local", "model_location": "hf_repo",
     "context_window": 4096, "instruction_following": False, "prompt_wrapper": "phi_3",
     "temperature": 0.3, "trailing_space": "", "link": "https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf",
     "custom_model_files": [], "custom_model_repo": "",
     "hf_repo": "microsoft/Phi-3-mini-4k-instruct"},

    {"model_name": "microsoft/Phi-3-mini-128k-instruct", "display_name": "phi-3-128k",
    "model_family": "HFGenerativeModel", "model_category": "generative_local", "model_location": "hf_repo",
    "context_window": 4096, "instruction_following": False, "prompt_wrapper": "phi_3",
    "temperature": 0.3, "trailing_space": "", "link": "https://huggingface.co/microsoft/Phi-3-mini-128k-instruct-gguf",
    "custom_model_files": [], "custom_model_repo": "",
    "hf_repo": "microsoft/Phi-3-mini-128k-instruct"},

    {"model_name": "Meta-Llama-3-8B-Instruct", "display_name": "llama-3-instruct",
     "model_family": "HFGenerativeModel", "model_category": "generative_local", "model_location": "hf_repo",
     "context_window": 8192, "instruction_following": False, "prompt_wrapper": "llama_3_chat",
     "temperature": 0.3, "trailing_space": "", "link": "https://huggingface.co/meta-llama/Meta-LLama-3-8B-instruct",
     "custom_model_files": [], "custom_model_repo": "",
     "hf_repo": "meta-llama/Meta-Llama-3-8B-Instruct"},

    {"model_name": "Meta-Llama-3-8B", "display_name": "llama-3-base",
     "model_family": "HFGenerativeModel", "model_category": "generative_local", "model_location": "hf_repo",
     "context_window": 8192, "instruction_following": False, "prompt_wrapper": "llama_3_chat",
     "temperature": 0.3, "trailing_space": "", "link": "https://huggingface.co/meta-llama/Meta-LLama-3-8B",
     "custom_model_files": [], "custom_model_repo": "",
     "hf_repo": "meta-llama/Meta-Llama-3-8B"},

    {"model_name": "QuantFactory/Meta-Llama-3-8B-Instruct-GGUF", "display_name": "llama-3-instruct-qf-gguf",
     "model_family": "GGUFGenerativeModel", "model_category": "generative_local", "model_location": "llmware_repo",
     "context_window": 8192, "instruction_following": False, "prompt_wrapper": "llama_3_chat",
     "temperature": 0.0, "sample_default": False, "trailing_space": "",
     "gguf_file": "Meta-Llama-3-8B-Instruct.Q4_K_M.gguf",
     "gguf_repo": "QuantFactory/Meta-Llama-3-8B-Instruct-GGUF",
     "fetch": {"module": "llmware.models", "method": "pull_model_from_hf"},
     "validation_files": ["Meta-Llama-3-8B-Instruct.Q4_K_M.gguf"],
     "link": "https://huggingface.co/QuantFactory/Meta-Llama-3-8B-Instruct-GGUF",
     "custom_model_files": [], "custom_model_repo": ""},

    {"model_name": "QuantFactory/Meta-Llama-3-8B-GGUF", "display_name": "llama-3-base-qf-gguf",
     "model_family": "GGUFGenerativeModel", "model_category": "generative_local", "model_location": "llmware_repo",
     "context_window": 8192, "instruction_following": False, "prompt_wrapper": "llama_3_chat",
     "temperature": 0.0, "sample_default": False, "trailing_space": "",
     "gguf_file": "Meta-Llama-3-8B.Q4_K_M.gguf",
     "gguf_repo": "QuantFactory/Meta-Llama-3-8B-GGUF",
     "fetch": {"module": "llmware.models", "method": "pull_model_from_hf"},
     "validation_files": ["Meta-Llama-3-8B.Q4_K_M.gguf"],
     "link": "https://huggingface.co/QuantFactory/Meta-Llama-3-GGUF",
     "custom_model_files": [], "custom_model_repo": ""},

 {"model_name": "bartowski/Meta-Llama-3-8B-Instruct-GGUF", "display_name": "llama-3-instruct-bartowski-gguf",
    "model_family": "GGUFGenerativeModel", "model_category": "generative_local", "model_location": "llmware_repo",
    "context_window": 8192, "instruction_following": False, "prompt_wrapper": "llama_3_chat",
    "temperature": 0.0, "sample_default": False, "trailing_space": "",
    "gguf_file": "Meta-Llama-3-8B-Instruct-Q4_K_M.gguf",
    "gguf_repo": "bartowski/Meta-Llama-3-8B-Instruct-GGUF",
    "fetch": {"module": "llmware.models", "method": "pull_model_from_hf"},
    "validation_files": ["Meta-Llama-3-8B-Instruct-Q4_K_M.gguf"],
    "link": "https://huggingface.co/bartowski/Meta-Llama-3-8B-Instruct-GGUF",
    "custom_model_files": [], "custom_model_repo": ""},

 {"model_name": "tiny-llama-chat-gguf", "display_name": "tiny-llama-chat-gguf",
  "model_family": "GGUFGenerativeModel", "model_category": "generative_local", "model_location": "llmware_repo",
  "context_window": 2048, "instruction_following": False, "prompt_wrapper": "hf_chat",
  "temperature": 0.3, "sample_default": True, "trailing_space": "",
  "gguf_file": "tiny-llama-chat.gguf",
  "gguf_repo": "llmware/bonchon",
  "fetch": {"module": "llmware.models", "method": "pull_model_from_hf"},
  "validation_files": ["tiny-llama-chat.gguf"],
  "link": "https://huggingface.co/llmware/bonchon",
  "tokenizer_local": "tokenizer_tl.json",
  "custom_model_files": [], "custom_model_repo": ""},

  # whisper-cpp models
 {"model_name": "whisper-cpp-base-english", "display_name": "whisper-en-base",
  "model_family": "WhisperCPPModel", "model_category": "generative_local", "model_location": "llmware_repo",
  "context_window": 2048, "instruction_following": False, "prompt_wrapper": "",
  "temperature": 0.0, "trailing_space": "",
  "gguf_file": "ggml-base.en.bin",
  "gguf_repo": "llmware/bonchon",
  "fetch": {"module": "llmware.models", "method": "pull_model_from_hf"},
  "validation_files": ["ggml-base.en.bin"],
  "link": "https://huggingface.co/llmware/bonchon",
  "custom_model_files": [], "custom_model_repo": ""},

 {"model_name": "whisper-cpp-base", "display_name": "whisper-base",
  "model_family": "WhisperCPPModel", "model_category": "generative_local", "model_location": "llmware_repo",
  "context_window": 2048, "instruction_following": False, "prompt_wrapper": "",
  "temperature": 0.0, "trailing_space": "",
  "gguf_file": "ggml-base.bin",
  "gguf_repo": "llmware/bonchon",
  "fetch": {"module": "llmware.models", "method": "pull_model_from_hf"},
  "validation_files": ["ggml-base.bin"],
  "link": "https://huggingface.co/llmware/bonchon",
  "custom_model_files": [], "custom_model_repo": ""},

 {"model_name": "whisper-cpp-tiny-diarize", "display_name": "whisper-en-tiny-diarize",
  "model_family": "WhisperCPPModel", "model_category": "generative_local", "model_location": "llmware_repo",
  "context_window": 2048, "instruction_following": False, "prompt_wrapper": "",
  "temperature": 0.0, "trailing_space": "",
  "gguf_file": "ggml-small.en-tdrz.bin",
  "gguf_repo": "llmware/bonchon",
  "fetch": {"module": "llmware.models", "method": "pull_model_from_hf"},
  "validation_files": ["ggml-small.en-trdz.bin"],
  "link": "https://huggingface.co/llmware/bonchon",
  "custom_model_files": [], "custom_model_repo": ""},

    {"model_name": "slim-ner-tool", "display_name": "slim-ner-tool",
     "model_family": "GGUFGenerativeModel", "model_category": "generative_local", "model_location": "llmware_repo",
     "context_window": 2048, "instruction_following": False, "prompt_wrapper": "human_bot",
     "temperature": 0.0, "sample_default": False, "trailing_space": "",
     "gguf_file": "slim-ner.gguf",
     "gguf_repo": "llmware/slim-ner-tool",
     "link": "https://huggingface.co/llmware/slim-ner-tool",
     "fetch": {"module": "llmware.models", "method": "pull_snapshot_from_hf"},
     "validation_files": ["slim-ner.gguf"],
     "custom_model_files": [], "custom_model_repo": "",
     "output_type": "dict",
     "function_call": True,
     "primary_keys": ["people", "location", "organization", "misc"],
     "fc_output_values": [],
     "tokenizer": "llmware/slim-sentiment",
     "tokenizer_local": "tokenizer_tl.json",
     "marker_tokens": [], "marker_token_lookup": {},
     "function": ["classify"]},

    {"model_name": "slim-sentiment-tool", "display_name": "slim-sentiment-tool",
     "model_family": "GGUFGenerativeModel", "model_category": "generative_local", "model_location": "llmware_repo",
     "context_window": 2048, "instruction_following": False, "prompt_wrapper": "human_bot",
     "temperature": 0.0, "sample_default": False, "trailing_space": "",
     "gguf_file": "slim-sentiment.gguf",
     "gguf_repo": "llmware/slim-sentiment-tool",
     "link": "https://huggingface.co/llmware/slim-sentiment-tool",
     "custom_model_files": [], "custom_model_repo": "",
     "output_type": "dict",
     "function_call": True,
     "primary_keys": ["sentiment"],
     "fc_output_values": ["positive", "neutral", "negative"],
     "tokenizer": "llmware/slim-sentiment",
     "tokenizer_local": "tokenizer_tl.json",
     "marker_tokens": [1066, 22198, 17821],
     "marker_token_lookup": {1066: "positive", 22198: "negative", 17821: "neutral"},
     "function": ["classify"],
     "fetch": {"module": "llmware.models", "method": "pull_snapshot_from_hf"},
     "validation_files": ["slim-sentiment.gguf"]},

    {"model_name": "slim-emotions-tool", "display_name": "slim-emotions-tool",
     "model_family": "GGUFGenerativeModel", "model_category": "generative_local", "model_location": "llmware_repo",
     "context_window": 2048, "instruction_following": False, "prompt_wrapper": "human_bot",
     "temperature": 0.0, "sample_default": False, "trailing_space": "",
     "gguf_file": "slim-emotions.gguf",
     "gguf_repo": "llmware/slim-emotions-tool",
     "link": "https://huggingface.co/llmware/slim-emotions-tool",
     "custom_model_files": [], "custom_model_repo": "",
     "output_type": "dict",
     "function_call": True,
     "primary_keys": ["emotions"],
     "fc_output_values": ["afraid", "anger", "angry", "annoyed", "anticipating", "anxious", "apprehensive",
                          "ashamed", "caring", "confident", "content", "devastated", "disappointed", "disgusted",
                          "embarrassed", "excited", "faithful", "fear", "furious", "grateful", "guilty",
                          "hopeful", "impressed", "jealous", "joy", "joyful", "lonely", "love", "nostalgic",
                          "prepared", "proud", "sad", "sadness", "sentimental", "surprise", "surprised",
                          "terrified", "trusting"],
     "tokenizer": "llmware/slim-sentiment",
     "tokenizer_local": "tokenizer_tl.json",
     "marker_tokens": [],
     "marker_token_lookup": {},
     "function": ["classify"],
     "fetch": {"module": "llmware.models", "method": "pull_snapshot_from_hf"},
     "validation_files": ["slim-emotions.gguf"]},

    {"model_name": "slim-ratings-tool", "display_name": "slim-ratings-tool",
     "model_family": "GGUFGenerativeModel", "model_category": "generative_local", "model_location": "llmware_repo",
     "context_window": 2048, "instruction_following": False, "prompt_wrapper": "human_bot",
     "temperature": 0.0, "sample_default": False, "trailing_space": "",
     "gguf_file": "slim-ratings.gguf",
     "gguf_repo": "llmware/slim-ratings-tool",
     "link": "https://huggingface.co/llmware/slim-ratings-tool",
     "custom_model_files": [], "custom_model_repo": "",
     "output_type": "dict",
     "function_call": True,
     "primary_keys": ["rating"],
     "fc_output_values": ["1", "2", "3", "4", "5"],
     "tokenizer": "llmware/slim-sentiment",
     "tokenizer_local": "tokenizer_tl.json",
     "marker_tokens": [],
     "marker_token_lookup": {},
     "function": ["classify"],
     "fetch": {"module": "llmware.models", "method": "pull_snapshot_from_hf"},
     "validation_files": ["slim-ratings.gguf"]},

   {"model_name": "slim-intent-tool", "display_name": "slim-intent-tool",
    "model_family": "GGUFGenerativeModel", "model_category": "generative_local", "model_location": "llmware_repo",
    "context_window": 2048, "instruction_following": False, "prompt_wrapper": "human_bot",
    "temperature": 0.0, "sample_default": False, "trailing_space": "",
    "gguf_file": "slim-intent.gguf",
    "gguf_repo": "llmware/slim-intent-tool",
    "link": "https://huggingface.co/llmware/slim-intent-tool",
    "custom_model_files": [], "custom_model_repo": "",
    "output_type": "dict",
    "function_call": True,
    "primary_keys": ["intent"],
    "fc_output_values": ["account", "cancel", "complaint", "customer service", "delivery", "feedback",
                         "invoice", "new account", "order", "payments", "refund", "shipping",
                         "subscription", "terminate"],
    "tokenizer": "llmware/slim-sentiment",
    "tokenizer_local": "tokenizer_tl.json",
    "marker_tokens": [],
    "marker_token_lookup": {},
    "function": ["classify"],
    "fetch": {"module": "llmware.models", "method": "pull_snapshot_from_hf"},
    "validation_files": ["slim-intent.gguf"]},

    {"model_name": "slim-nli-tool", "display_name": "slim-nli-tool",
     "model_family": "GGUFGenerativeModel", "model_category": "generative_local", "model_location": "llmware_repo",
     "context_window": 2048, "instruction_following": False, "prompt_wrapper": "human_bot",
     "temperature": 0.0, "sample_default": False, "trailing_space": "",
     "gguf_file": "slim-nli.gguf",
     "gguf_repo": "llmware/slim-nli-tool",
     "link": "https://huggingface.co/llmware/slim-nli-tool",
     "custom_model_files": [], "custom_model_repo": "",
     "output_type": "dict",
     "function_call": True,
     "primary_keys": ["evidence"],
     "fc_output_values": ["supports", "neutral", "contradicts"],
     "tokenizer": "llmware/slim-sentiment",
     "tokenizer_local": "tokenizer_tl.json",
     "marker_tokens": [9996,5924,17821],
     "marker_token_lookup": {9996: "contradicts", 5924: "supports", 17821: "neutral"},
     "function": ["classify"],
     "fetch": {"module": "llmware.models", "method": "pull_snapshot_from_hf"},
     "validation_files": ["slim-nli.gguf"]},

    {"model_name": "slim-topics-tool", "display_name": "slim-topics-tool",
     "model_family": "GGUFGenerativeModel", "model_category": "generative_local", "model_location": "llmware_repo",
     "context_window": 2048, "instruction_following": False, "prompt_wrapper": "human_bot",
     "temperature": 0.0, "sample_default": False, "trailing_space": "",
     "gguf_file": "slim-topics.gguf",
     "gguf_repo": "llmware/slim-topics-tool",
     "link": "https://huggingface.co/llmware/slim-topics-tool",
     "custom_model_files": [], "custom_model_repo": "",
     "output_type": "dict",
     "function_call": True,
     "primary_keys": ["topics"],
     "fc_output_values": [],
     "tokenizer": "llmware/slim-sentiment",
     "tokenizer_local": "tokenizer_tl.json",
     "marker_tokens": [],
     "marker_token_lookup": {},
     "function": ["classify"],
     "fetch": {"module": "llmware.models", "method": "pull_snapshot_from_hf"},
     "validation_files": ["slim-topics.gguf"]},

    {"model_name": "slim-tags-tool", "display_name": "slim-tags-tool",
     "model_family": "GGUFGenerativeModel", "model_category": "generative_local", "model_location": "llmware_repo",
     "context_window": 2048, "instruction_following": False, "prompt_wrapper": "human_bot",
     "temperature": 0.0, "sample_default": False, "trailing_space": "",
     "gguf_file": "slim-tags.gguf", "gguf_repo": "llmware/slim-tags-tool",
     "link": "https://huggingface.co/llmware/slim-tags-tool",
     "custom_model_files": [], "custom_model_repo": "",
     "output_type": "dict",
     "function_call": True,
     "primary_keys": ["tags"],
     "fc_output_values": [],
     "tokenizer": "llmware/slim-sentiment",
     "tokenizer_local": "tokenizer_tl.json",
     "marker_tokens": [],
     "marker_token_lookup": {},
     "function": ["classify"],
     "fetch": {"module": "llmware.models", "method": "pull_snapshot_from_hf"},
     "validation_files": ["slim-tags.gguf"]},

    {"model_name": "slim-sql-tool", "display_name": "slim-sql-tool",
     "model_family": "GGUFGenerativeModel", "model_category": "generative_local", "model_location": "llmware_repo",
     "context_window": 2048, "instruction_following": False, "prompt_wrapper": "human_bot",
     "temperature": 0.0, "sample_default": False, "trailing_space": "",
     "gguf_file": "slim-sql.gguf",
     "gguf_repo": "llmware/slim-sql-tool",
     "fc_output_values": [],
     "link": "https://huggingface.co/llmware/slim-sql-tool",
     "custom_model_files": [], "custom_model_repo": "",
     "tokenizer": "llmware/slim-sql-1b-v0",
     "tokenizer_local": "tokenizer_tl.json",
     "fetch": {"module": "llmware.models", "method": "pull_snapshot_from_hf"},
     "validation_files": ["slim-sql.gguf"]},

    {"model_name": "bling-answer-tool", "display_name": "bling-answer-tool",
     "model_family": "GGUFGenerativeModel", "model_category": "generative_local", "model_location": "llmware_repo",
     "context_window": 2048, "instruction_following": False, "prompt_wrapper": "human_bot",
     "temperature": 0.0, "sample_default": False, "trailing_space": "",
     "gguf_file": "bling-answer.gguf",
     "gguf_repo": "llmware/bling-answer-tool",
     "link": "https://huggingface.co/llmware/bling-answer-tool",
     "custom_model_files": [], "custom_model_repo": "",
     "tokenizer": "llmware/bling-tiny-llama-1b-v0",
     "tokenizer_local": "tokenizer_tl.json",
     "fetch": {"module": "llmware.models", "method": "pull_snapshot_from_hf"},
     "validation_files": ["bling-answer.gguf"]},

   {"model_name": "slim-category-tool", "display_name": "slim-category-tool",
    "model_family": "GGUFGenerativeModel", "model_category": "generative_local", "model_location": "llmware_repo",
    "context_window": 2048, "instruction_following": False, "prompt_wrapper": "human_bot",
    "temperature": 0.3, "sample_default": False, "trailing_space": "",
    "gguf_file": "slim-category.gguf",
    "gguf_repo": "llmware/slim-category-tool",
    "link": "https://huggingface.co/llmware/slim-category-tool",
    "custom_model_files": [], "custom_model_repo": "",
    "output_type": "dict",
    "function_call": True,
    "primary_keys": ["category"],
    "fc_output_values": ["analyst", "announcements", "bonds", "business", "central bank", "commentary",
                         "commodities", "currencies", "dividend", "earnings", "energy", "entertainment",
                         "financials", "health", "human resources", "legal and regulation", "macroeconomics",
                         "markets", "mergers and acquisitions", "opinion", "politics", "public markets",
                         "science", "sports", "stocks", "tech", "world"],
    "tokenizer": "llmware/slim-sentiment",
    "tokenizer_local": "tokenizer_tl.json",
    "marker_tokens": [],
    "marker_token_lookup": {},
    "function": ["classify"],
    "fetch": {"module": "llmware.models", "method": "pull_snapshot_from_hf"},
    "validation_files": ["slim-category.gguf"]},

    # pytorch slim models start here

    {"model_name": "llmware/slim-intent", "display_name": "slim-intent-1b",
     "model_family": "HFGenerativeModel", "model_category": "generative_local", "model_location": "hf_repo",
     "context_window": 2048, "instruction_following": False, "prompt_wrapper": "human_bot",
     "temperature": 0.0, "sample_default": False, "trailing_space": "", "gguf_file": "", "gguf_repo": "",
     "link": "https://huggingface.co/llmware/slim-intent",
     "hf_repo": "llmware/slim-intent",
     "custom_model_files": [""], "custom_model_repo": "",
     "output_type": "dict",
     "function_call": True,
     "primary_keys": ["intent"],
     "fc_output_values": ["account", "cancel", "complaint", "customer service", "delivery", "feedback",
                          "invoice", "new account", "order", "payments", "refund", "shipping",
                          "subscription", "terminate"],
     "function": ["classify"],
     "marker_tokens": [1066, 22198, 17821],
     "marker_token_lookup": {1066: "positive", 22198: "negative", 17821: "neutral"},
     },

    {"model_name": "llmware/slim-sentiment", "display_name": "slim-sentiment-1b",
     "model_family": "HFGenerativeModel", "model_category": "generative_local", "model_location": "hf_repo",
     "context_window": 2048, "instruction_following": False, "prompt_wrapper": "human_bot",
     "temperature": 0.0, "sample_default": False, "trailing_space": "", "gguf_file": "", "gguf_repo": "",
     "link": "https://huggingface.co/llmware/slim-sentiment",
     "hf_repo": "llmware/slim-sentiment",
     "custom_model_files": [""], "custom_model_repo": "",
     "output_type": "dict",
     "function_call": True,
     "primary_keys": ["sentiment"],
     "fc_output_values": ["positive", "neutral", "negative"],
     "marker_tokens": [1066, 22198, 17821],
     "marker_token_lookup": {1066: "positive", 22198: "negative", 17821: "neutral"},
     "function": ["classify"]},

    {"model_name": "llmware/slim-emotions", "display_name": "slim-emotions-1b",
     "model_family": "HFGenerativeModel", "model_category": "generative_local", "model_location": "hf_repo",
     "context_window": 2048, "instruction_following": False, "prompt_wrapper": "human_bot",
     "temperature": 0.0, "sample_default": False, "trailing_space": "", "gguf_file": "", "gguf_repo": "",
     "link": "https://huggingface.co/llmware/slim-emotions",
     "hf_repo": "llmware/slim-emotions",
     "custom_model_files": [], "custom_model_repo": "",
     "output_type": "dict",
     "function_call": True,
     "primary_keys": ["emotions"],
     "fc_output_values": ["afraid", "anger", "angry", "annoyed", "anticipating", "anxious", "apprehensive",
                          "ashamed", "caring", "confident", "content", "devastated", "disappointed", "disgusted",
                          "embarrassed", "excited", "faithful", "fear", "furious", "grateful", "guilty",
                          "hopeful", "impressed", "jealous", "joy", "joyful", "lonely", "love", "nostalgic",
                          "prepared", "proud", "sad", "sadness", "sentimental", "surprise", "surprised",
                          "terrified", "trusting"],
     "marker_tokens": [1066, 22198, 17821],
     "marker_token_lookup": {1066: "positive", 22198: "negative", 17821: "neutral"},
     "function": ["classify"]},

    {"model_name": "llmware/slim-ner", "display_name": "slim-ner-1b",
     "model_family": "HFGenerativeModel", "model_category": "generative_local", "model_location": "hf_repo",
     "context_window": 2048, "instruction_following": False, "prompt_wrapper": "human_bot",
     "temperature": 0.0, "sample_default": False, "trailing_space": "", "gguf_file": "", "gguf_repo": "",
     "link": "https://huggingface.co/llmware/slim-ner",
     "custom_model_files": [], "custom_model_repo": "",
     "output_type": "dict",
     "hf_repo": "llmware/slim-ner",
     "function_call": True,
     "primary_keys": ["person", "organization", "place", "misc"],
     "fc_output_values": [],
     "marker_tokens": [],
     "marker_token_lookup": {},
     "function": ["classify"]},

    {"model_name": "llmware/slim-nli", "display_name": "slim-nli-1b",
     "model_family": "HFGenerativeModel", "model_category": "generative_local", "model_location": "hf_repo",
     "context_window": 2048, "instruction_following": False, "prompt_wrapper": "human_bot",
     "temperature": 0.0, "sample_default": False, "trailing_space": "", "gguf_file": "", "gguf_repo": "",
     "link": "https://huggingface.co/llmware/slim-nli",
     "custom_model_files": [], "custom_model_repo": "",
     "hf_repo": "llmware/slim-nli",
     "output_type": "dict",
     "function_call": True,
     "primary_keys": ["evidence"],
     "fc_output_values": ["supports", "neutral", "contradicts"],
     "marker_tokens": [],
     "marker_token_lookup": {},
     "function": ["classify"]},

    {"model_name": "llmware/slim-ratings", "display_name": "slim-ratings-1b",
     "model_family": "HFGenerativeModel", "model_category": "generative_local", "model_location": "hf_repo",
     "context_window": 2048, "instruction_following": False, "prompt_wrapper": "human_bot",
     "temperature": 0.0, "sample_default": False, "trailing_space": "", "gguf_file": "", "gguf_repo": "",
     "link": "https://huggingface.co/llmware/slim-ratings",
     "hf_repo": "llmware/slim-ratings",
     "custom_model_files": [], "custom_model_repo": "",
     "output_type": "dict",
     "function_call": True,
     "primary_keys": ["rating"],
     "fc_output_values": ["1", "2", "3", "4", "5"],
     "marker_tokens": [],
     "marker_token_lookup": {},
     "function": ["classify"]},

    {"model_name": "llmware/slim-category", "display_name": "slim-category-1b",
     "model_family": "HFGenerativeModel", "model_category": "generative_local", "model_location": "hf_repo",
     "context_window": 2048, "instruction_following": False, "prompt_wrapper": "human_bot",
     "temperature": 0.0, "sample_default": False, "trailing_space": "", "gguf_file": "", "gguf_repo": "",
     "link": "https://huggingface.co/llmware/slim-category",
     "custom_model_files": [], "custom_model_repo": "",
     "output_type": "dict",
     "hf_repo": "llmware/slim-category",
     "function_call": True,
     "primary_keys": ["category"],
     "fc_output_values": ["analyst", "announcements", "bonds", "business", "central bank", "commentary",
                          "commodities", "currencies", "dividend", "earnings", "energy", "entertainment",
                          "financials", "health", "human resources", "legal and regulation", "macroeconomics",
                          "markets", "mergers and acquisitions", "opinion", "politics", "public markets",
                          "science", "sports", "stocks", "tech", "world"],
     "marker_tokens": [],
     "marker_token_lookup": {},
     "function": ["classify"]},

    {"model_name": "llmware/slim-tags", "display_name": "slim-tags-1b",
     "model_family": "HFGenerativeModel", "model_category": "generative_local", "model_location": "hf_repo",
     "context_window": 2048, "instruction_following": False, "prompt_wrapper": "human_bot",
     "temperature": 0.0, "sample_default": False, "trailing_space": "", "gguf_file": "", "gguf_repo": "",
     "link": "https://huggingface.co/llmware/slim-tags",
     "custom_model_files": [], "custom_model_repo": "",
     "hf_repo": "llmware/slim-tags",
     "outout_type": "dict",
     "function_call": True,
     "marker_tokens": [],
     "marker_token_lookup": {},
     "primary_keys": ["tags"],
     "fc_output_values": [],
     "function": ["classify"]},

   {"model_name": "llmware/slim-topics", "display_name": "slim-topics-1b",
    "model_family": "HFGenerativeModel", "model_category": "generative_local", "model_location": "hf_repo",
    "context_window": 2048, "instruction_following": False, "prompt_wrapper": "human_bot",
    "temperature": 0.0,"sample_default": False,  "trailing_space": "", "gguf_file": "", "gguf_repo": "",
    "link": "https://huggingface.co/llmware/slim-topics",
    "hf_repo": "llmware/slim-topics",
    "custom_model_files": [], "custom_model_repo": "",
    "output_type": "dict",
    "function_call": True,
    "marker_tokens": [],
    "marker_token_lookup": {},
    "primary_keys": ["topics"],
    "fc_output_values": [],
    "function": ["classify"]},

    # sql pytorch model
    {"model_name": "llmware/slim-sql-1b-v0", "display_name": "slim-sql-1b",
     "model_family": "HFGenerativeModel", "model_category": "generative_local", "model_location": "hf_repo",
     "context_window": 2048, "instruction_following": False, "prompt_wrapper": "human_bot",
     "temperature": 0.0, "sample_default": False,
     "trailing_space": "", "link": "https://huggingface.co/llmware/slim-sql-1b-v0",
     "custom_model_files": [], "custom_model_repo": "",
     "hf_repo": "llmware/slim-sql-1b-v0",
     #TODO: assess how to handle SQL models with function call parameters
     "function_call": False,
     "fc_output_values": [],
     "primary_keys": ["sql"], "function": ["sql"]},

    {"model_name": "bling-stablelm-3b-tool", "display_name": "llmware/bling-stablelm-3b-gguf",
     "model_family": "GGUFGenerativeModel", "model_category": "generative_local", "model_location": "llmware_repo",
     "context_window": 2048, "instruction_following": False, "prompt_wrapper": "human_bot",
     "temperature": 0.0, "sample_default": False, "trailing_space": "", "gguf_file": "bling-stablelm.gguf",
     "gguf_repo": "llmware/bling-stablelm-3b-gguf",
     "fetch": {"module": "llmware.models", "method": "pull_snapshot_from_hf"},
     "validation_files": ["bling-stablelm.gguf"],
     "link": "https://huggingface.co/llmware/bling-stablelm-3b-gguf",
     "tokenizer_local": "tokenizer_stablelm.json",
     "custom_model_files": [], "custom_model_repo": ""},

    {"model_name": "slim-xsum", "display_name": "llmware/slim-xsum",
     "model_family": "HFGenerativeModel", "model_category": "generative_local", "model_location": "hf_repo",
     "context_window": 2048, "instruction_following": False, "prompt_wrapper": "human_bot",
     "temperature": 0.0, "sample_default": False, "trailing_space": "", "gguf_file": "", "gguf_repo": "",
     "link": "https://huggingface.co/llmware/slim-xsum", "hf_repo": "llmware/slim-xsum",
     "custom_model_files": [], "custom_model_repo": "", "output_type": "dict", "function_call": True,
     "marker_tokens": [], "marker_token_lookup": {}, "primary_keys": ["xsum"], "fc_output_values": [],
     "function": ["classify"]},

    {"model_name": "slim-xsum-tool", "display_name": "slim-xsum-tool",
     "model_family": "GGUFGenerativeModel", "model_category": "generative_local",
     "model_location": "llmware_repo", "context_window": 2048, "instruction_following": False,
     "prompt_wrapper": "human_bot", "temperature": 0.0, "sample_default": False, "trailing_space": "",
     "gguf_file": "slim-xsum.gguf", "gguf_repo": "llmware/slim-xsum-tool",
     "link": "https://huggingface.co/llmware/slim-xsum-tool",
     "custom_model_files": [], "custom_model_repo": "",
     "output_type": "dict", "function_call": True, "primary_keys": ["xsum"], "fc_output_values": [],
     "tokenizer": "llmware/slim-extract",
     "tokenizer_local": "tokenizer_stablelm.json",
     "marker_tokens": [], "marker_token_lookup": {}, "function": ["classify"],
     "fetch": {"module": "llmware.models", "method": "pull_snapshot_from_hf"},
     "validation_files": ["slim-xsum.gguf"],
     },

    {"model_name": "slim-extract", "display_name": "llmware/slim-extract",
     "model_family": "HFGenerativeModel", "model_category": "generative_local", "model_location": "hf_repo",
     "context_window": 2048, "instruction_following": False, "prompt_wrapper": "human_bot",
     "temperature": 0.0, "sample_default": False, "trailing_space": "", "gguf_file": "", "gguf_repo": "",
     "link": "https://huggingface.co/llmware/slim-extract", "hf_repo": "llmware/slim-extract",
     "custom_model_files": [], "custom_model_repo": "", "output_type": "dict", "function_call": True,
     "marker_tokens": [], "marker_token_lookup": {}, "primary_keys": ["key data points"], "fc_output_values": [],
     "function": ["extract"]},

    {"model_name": "slim-extract-tiny", "display_name": "llmware/slim-extract-tiny",
     "model_family": "HFGenerativeModel", "model_category": "generative_local", "model_location": "hf_repo",
     "context_window": 2048, "instruction_following": False, "prompt_wrapper": "human_bot",
     "temperature": 0.0, "sample_default": False, "trailing_space": "", "gguf_file": "", "gguf_repo": "",
     "link": "https://huggingface.co/llmware/slim-extract-tiny", "hf_repo": "llmware/slim-extract-tiny",
     "custom_model_files": [], "custom_model_repo": "", "output_type": "dict", "function_call": True,
     "marker_tokens": [], "marker_token_lookup": {}, "primary_keys": ["key data points"], "fc_output_values": [],
     "function": ["extract"]},

 {"model_name": "slim-extract-tool", "display_name": "slim-extract-tool",
  "model_family": "GGUFGenerativeModel", "model_category": "generative_local",
  "model_location": "llmware_repo", "context_window": 2048, "instruction_following": False,
  "prompt_wrapper": "human_bot", "temperature": 0.0, "sample_default": False, "trailing_space": "",
  "gguf_file": "slim-extract.gguf", "gguf_repo": "llmware/slim-extract-tool",
  "link": "https://huggingface.co/llmware/slim-extract-tool",
  "custom_model_files": [], "custom_model_repo": "", "output_type": "dict",
  "function_call": True, "primary_keys": ["key data points"], "fc_output_values": [],
  "tokenizer": "llmware/slim-extract",
  "tokenizer_local": "tokenizer_stablelm.json",
  "marker_tokens": [],
  "marker_token_lookup": {}, "function": ["extract"],
  "fetch": {"module": "llmware.models", "method": "pull_snapshot_from_hf"},
  "validation_files": ["slim-extract.gguf"],
  },

  # NEW
  {"model_name": "slim-extract-phi-3-gguf", "display_name": "slim-extract-phi-3-tool",
     "model_family": "GGUFGenerativeModel", "model_category": "generative_local",
     "model_location": "llmware_repo", "context_window": 4096, "instruction_following": False,
     "prompt_wrapper": "human_bot", "temperature": 0.0, "sample_default": False, "trailing_space": "",
     "gguf_file": "phi3-extract.gguf", "gguf_repo": "llmware/slim-extract-phi-3-gguf",
     "link": "https://huggingface.co/llmware/slim-extract-phi-3-gguf",
     "custom_model_files": [], "custom_model_repo": "", "output_type": "dict",
     "function_call": True, "primary_keys": ["key data points"], "fc_output_values": [],
     "tokenizer": "llmware/slim-extract-phi-3",
     "tokenizer_local": "tokenizer_phi3.json",
     "marker_tokens": [],
     "marker_token_lookup": {}, "function": ["extract"],
     "fetch": {"module": "llmware.models", "method": "pull_snapshot_from_hf"},
     "validation_files": ["phi3-extract.gguf"],
     },

 # NEW
 {"model_name": "slim-extract-qwen-1.5b-gguf", "display_name": "slim-extract-qwen-1.5b-gguf",
  "model_family": "GGUFGenerativeModel", "model_category": "generative_local",
  "model_location": "llmware_repo", "context_window": 4096, "instruction_following": False,
  "prompt_wrapper": "human_bot", "temperature": 0.0, "sample_default": False, "trailing_space": "",
  "gguf_file": "qwen-extract-1-5.gguf", "gguf_repo": "llmware/slim-extract-qwen-1.5b-gguf",
  "link": "https://huggingface.co/llmware/slim-extract-qwen-1.5b-gguf",
  "custom_model_files": [], "custom_model_repo": "", "output_type": "dict",
  "function_call": True, "primary_keys": ["key data points"], "fc_output_values": [],
  "tokenizer": "llmware/slim-extract-qwen-1.5b",
  "tokenizer_local": "tokenizer_qw.json",
  "marker_tokens": [],
  "marker_token_lookup": {}, "function": ["extract"],
  "fetch": {"module": "llmware.models", "method": "pull_snapshot_from_hf"},
  "validation_files": ["qwen-extract-1-5.gguf"],
  },

 {"model_name": "slim-extract-qwen-nano-gguf", "display_name": "slim-extract-qwen-0.5b-gguf",
     "model_family": "GGUFGenerativeModel", "model_category": "generative_local",
     "model_location": "llmware_repo", "context_window": 2048, "instruction_following": False,
     "prompt_wrapper": "human_bot", "temperature": 0.0, "sample_default": False, "trailing_space": "",
     "gguf_file": "qwen-extract-0-5.gguf", "gguf_repo": "llmware/slim-extract-qwen-0.5b-gguf",
     "link": "https://huggingface.co/llmware/slim-extract-qwen-0.5b-gguf",
     "custom_model_files": [], "custom_model_repo": "", "output_type": "dict",
     "function_call": True, "primary_keys": ["key data points"], "fc_output_values": [],
     "tokenizer": "llmware/slim-extract-qwen-0.5b-gguf",
     "tokenizer_local": "tokenizer_qw.json",
     "marker_tokens": [],
     "marker_token_lookup": {}, "function": ["extract"],
     "fetch": {"module": "llmware.models", "method": "pull_snapshot_from_hf"},
     "validation_files": ["qwen-extract-0-5.gguf"],
     },

    {"model_name": "llmware/slim-extract-tiny-tool", "display_name": "slim-extract-tiny-tool",
     "model_family": "GGUFGenerativeModel", "model_category": "generative_local",
     "model_location": "llmware_repo", "context_window": 2048, "instruction_following": False,
     "prompt_wrapper": "human_bot","temperature": 0.0, "sample_default": False, "trailing_space": "",
     "gguf_file": "tiny-extract.gguf", "gguf_repo": "llmware/slim-extract-tiny-tool",
     "link": "https://huggingface.co/llmware/slim-extract-tiny-tool",
     "custom_model_files": [], "custom_model_repo": "", "output_type": "dict",
     "function_call": True, "primary_keys": ["key points"], "fc_output_values": [],
     "tokenizer": "llmware/slim-sentiment",
     "tokenizer_local": "tokenizer_tl.json",
     "marker_tokens": [], "marker_token_lookup": {},
     "function": ["classify"],
     "fetch": {"module": "llmware.models", "method": "pull_snapshot_from_hf"},
     "validation_files": ["tiny-extract.gguf"]},

     {"model_name": "llmware/slim-summary-tiny-tool", "display_name": "slim-summary-tiny-tool",
      "model_family": "GGUFGenerativeModel", "model_category": "generative_local",
      "model_location": "llmware_repo", "context_window": 2048, "instruction_following": False,
      "prompt_wrapper": "human_bot", "temperature": 0.0, "sample_default": False, "trailing_space": "",
      "gguf_file": "tiny-summary.gguf", "gguf_repo": "llmware/slim-summary-tiny-tool",
      "link": "https://huggingface.co/llmware/slim-summary-tiny-tool",
      "custom_model_files": [], "custom_model_repo": "", "output_type": "dict",
      "function_call": True,"primary_keys": ["key points"], "fc_output_values": [],
      "tokenizer": "llmware/slim-sentiment",
      "tokenizer_local": "tokenizer_tl.json",
      "marker_tokens": [], "marker_token_lookup": {},
      "function": ["classify"],
      "fetch": {"module": "llmware.models", "method": "pull_snapshot_from_hf"},
      "validation_files": ["tiny-summary.gguf"]},

  # NEW
  {"model_name": "slim-summary-phi-3-gguf", "display_name": "slim-summary-phi-3-tool",
  "model_family": "GGUFGenerativeModel", "model_category": "generative_local",
  "model_location": "llmware_repo", "context_window": 2048, "instruction_following": False,
  "prompt_wrapper": "human_bot", "temperature": 0.0, "sample_default": False, "trailing_space": "",
  "gguf_file": "phi3-summary.gguf", "gguf_repo": "llmware/slim-summary-phi-3-gguf",
  "link": "https://huggingface.co/llmware/slim-summary-phi-3-gguf",
  "custom_model_files": [], "custom_model_repo": "", "output_type": "dict",
  "function_call": True, "primary_keys": ["key points"], "fc_output_values": [],
  "tokenizer": "llmware/slim-summary-phi3",
  "tokenizer_local": "tokenizer_phi3.json",
  "marker_tokens": [], "marker_token_lookup": {},
  "function": ["classify"],
  "fetch": {"module": "llmware.models", "method": "pull_snapshot_from_hf"},
  "validation_files": ["phi3-summary.gguf"]},

 # NEW
 {"model_name": "slim-xsum-phi-3-gguf", "display_name": "slim-xsum-phi-3-gguf",
  "model_family": "GGUFGenerativeModel", "model_category": "generative_local",
  "model_location": "llmware_repo", "context_window": 2048, "instruction_following": False,
  "prompt_wrapper": "human_bot", "temperature": 0.0, "sample_default": False, "trailing_space": "",
  "gguf_file": "slim-xsum.gguf", "gguf_repo": "llmware/slim-xsum-phi-3-gguf",
  "link": "https://huggingface.co/llmware/slim-xsum-phi-3-gguf",
  "custom_model_files": [], "custom_model_repo": "", "output_type": "dict",
  "function_call": True, "primary_keys": ["key points"], "fc_output_values": [],
  "tokenizer": "llmware/slim-xsum-phi-3",
  "tokenizer_local": "tokenizer_phi3.json",
  "marker_tokens": [], "marker_token_lookup": {},
  "function": ["classify"],
  "fetch": {"module": "llmware.models", "method": "pull_snapshot_from_hf"},
  "validation_files": ["slim-xsum.gguf"]},

 {"model_name": "slim-boolean", "display_name": "llmware/slim-boolean",
     "model_family": "HFGenerativeModel", "model_category": "generative_local", "model_location": "hf_repo",
     "context_window": 2048, "instruction_following": False, "prompt_wrapper": "human_bot",
     "temperature": 0.0, "sample_default": False, "trailing_space": "", "gguf_file": "", "gguf_repo": "",
     "link": "https://huggingface.co/llmware/slim-boolean", "hf_repo": "llmware/slim-boolean",
     "custom_model_files": [], "custom_model_repo": "", "output_type": "dict", "function_call": True,
     "marker_tokens": [2369,9820], "marker_token_lookup": {2369: "no", 9820: "yes"},
     "primary_keys": [], "fc_output_values": [],
     "function": ["boolean"]},

    {"model_name": "slim-boolean-tool", "display_name": "slim-boolean-tool",
     "model_family": "GGUFGenerativeModel", "model_category": "generative_local",
     "model_location": "llmware_repo", "context_window": 2048, "instruction_following": False,
     "prompt_wrapper": "human_bot", "temperature": 0.0, "sample_default": False, "trailing_space": "",
     "gguf_file": "slim-boolean.gguf", "gguf_repo": "llmware/slim-boolean-tool",
     "link": "https://huggingface.co/llmware/slim-boolean-tool",
     "custom_model_files": [], "custom_model_repo": "", "output_type": "dict",
     "function_call": True, "primary_keys": [], "fc_output_values": [],
     "tokenizer": "llmware/slim-extract",
     "tokenizer_local": "tokenizer_stablelm.json",
     "marker_tokens": [2369,9820], "marker_token_lookup": {2369: "no", 9820: "yes"},
     "function": ["boolean"],
     "fetch": {"module": "llmware.models", "method": "pull_snapshot_from_hf"},
     "validation_files": ["slim-boolean.gguf"],
     },

 # NEW
 {"model_name": "slim-boolean-phi-3-gguf", "display_name": "slim-boolean-phi-3-gguf",
  "model_family": "GGUFGenerativeModel", "model_category": "generative_local",
  "model_location": "llmware_repo", "context_window": 2048, "instruction_following": False,
  "prompt_wrapper": "human_bot", "temperature": 0.0, "sample_default": False, "trailing_space": "",
  "gguf_file": "slim-boolean.gguf", "gguf_repo": "llmware/slim-boolean-phi-3-gguf",
  "link": "https://huggingface.co/llmware/slim-boolean-phi-3-gguf",
  "custom_model_files": [], "custom_model_repo": "", "output_type": "dict",
  "function_call": True, "primary_keys": [], "fc_output_values": [],
  "tokenizer": "llmware/slim-boolean-phi-3",
  "tokenizer_local": "tokenizer_phi3.json",
  "marker_tokens": [2369, 9820], "marker_token_lookup": {2369: "no", 9820: "yes"},
  "function": ["boolean"],
  "fetch": {"module": "llmware.models", "method": "pull_snapshot_from_hf"},
  "validation_files": ["slim-boolean.gguf"],
  },

 {"model_name": "slim-sa-ner", "display_name": "llmware/slim-sa-ner",
     "model_family": "HFGenerativeModel", "model_category": "generative_local", "model_location": "hf_repo",
     "context_window": 2048, "instruction_following": False, "prompt_wrapper": "human_bot",
     "temperature": 0.0, "sample_default": False, "trailing_space": "", "gguf_file": "", "gguf_repo": "",
     "link": "https://huggingface.co/llmware/slim-sa-ner", "hf_repo": "llmware/slim-sa-ner",
     "custom_model_files": [], "custom_model_repo": "", "output_type": "dict", "function_call": True,
     "marker_tokens": [], "marker_token_lookup": {},
     "primary_keys": ["sentiment, person, organization, place"], "fc_output_values": [],
     "function": ["classify"]},

 # NEW
  {"model_name": "slim-sa-ner-phi-3-gguf", "display_name": "slim-sa-ner-phi-3-gguf",
  "model_family": "GGUFGenerativeModel", "model_category": "generative_local",
  "model_location": "llmware_repo", "context_window": 2048, "instruction_following": False,
  "prompt_wrapper": "human_bot", "temperature": 0.0, "sample_default": False, "trailing_space": "",
  "gguf_file": "slim-sa-ner.gguf", "gguf_repo": "llmware/slim-sa-ner-phi-3-gguf",
  "link": "https://huggingface.co/llmware/slim-sa-ner-phi-3-gguf",
  "custom_model_files": [], "custom_model_repo": "", "output_type": "dict",
  "function_call": True, "primary_keys": ["sentiment, person, organization, place"], "fc_output_values": [],
  "tokenizer": "llmware/slim-extract-phi-3",
  "tokenizer_local": "tokenizer_phi3.json",
  "marker_tokens": [],
  "marker_token_lookup": {}, "function": ["classify"],
  "fetch": {"module": "llmware.models", "method": "pull_snapshot_from_hf"},
  "validation_files": ["slim-sa-ner.gguf"],
   },

    {"model_name": "slim-sa-ner-tool", "display_name": "slim-sa-ner-tool",
     "model_family": "GGUFGenerativeModel", "model_category": "generative_local",
     "model_location": "llmware_repo", "context_window": 2048, "instruction_following": False,
     "prompt_wrapper": "human_bot", "temperature": 0.0, "sample_default": False, "trailing_space": "",
     "gguf_file": "sa-ner.gguf", "gguf_repo": "llmware/slim-sa-ner-tool",
     "link": "https://huggingface.co/llmware/slim-sa-ner-tool",
     "custom_model_files": [], "custom_model_repo": "", "output_type": "dict",
     "function_call": True, "primary_keys": ["sentiment, person, organization, place"], "fc_output_values": [],
     "tokenizer": "llmware/slim-extract",
     "tokenizer_local": "tokenizer_stablelm.json",
     "marker_tokens": [],
     "marker_token_lookup": {}, "function": ["classify"],
     "fetch": {"module": "llmware.models", "method": "pull_snapshot_from_hf"},
     "validation_files": ["sa-ner.gguf"],
     },

    {"model_name": "slim-tags-3b", "display_name": "llmware/slim-tags-3b",
     "model_family": "HFGenerativeModel", "model_category": "generative_local", "model_location": "hf_repo",
     "context_window": 2048, "instruction_following": False, "prompt_wrapper": "human_bot",
     "temperature": 0.0, "sample_default": False, "trailing_space": "", "gguf_file": "", "gguf_repo": "",
     "link": "https://huggingface.co/llmware/slim-tags-3b", "hf_repo": "llmware/slim-tags-3b",
     "custom_model_files": [], "custom_model_repo": "", "output_type": "dict", "function_call": True,
     "marker_tokens": [], "marker_token_lookup": {},
     "primary_keys": ["tags"], "fc_output_values": [],
     "function": ["classify"]},

    {"model_name": "slim-tags-3b-tool", "display_name": "slim-tags-3b-tool",
     "model_family": "GGUFGenerativeModel", "model_category": "generative_local",
     "model_location": "llmware_repo", "context_window": 2048, "instruction_following": False,
     "prompt_wrapper": "human_bot", "temperature": 0.0, "sample_default": False, "trailing_space": "",
     "gguf_file": "slim-tags-3b.gguf", "gguf_repo": "llmware/slim-tags-3b-tool",
     "link": "https://huggingface.co/llmware/slim-tags-3b-tool",
     "custom_model_files": [], "custom_model_repo": "", "output_type": "dict",
     "function_call": True, "primary_keys": ["tags"], "fc_output_values": [],
     "tokenizer": "llmware/slim-extract",
     "tokenizer_local": "tokenizer_stablelm.json",
     "marker_tokens": [],
     "marker_token_lookup": {}, "function": ["classify"],
     "fetch": {"module": "llmware.models", "method": "pull_snapshot_from_hf"},
     "validation_files": ["slim-tags-3b.gguf"],
     },

    {"model_name": "slim-summary", "display_name": "llmware/slim-summary",
     "model_family": "HFGenerativeModel", "model_category": "generative_local", "model_location": "hf_repo",
     "context_window": 2048, "instruction_following": False, "prompt_wrapper": "human_bot",
     "temperature": 0.0, "sample_default": False, "trailing_space": "", "gguf_file": "", "gguf_repo": "",
     "link": "https://huggingface.co/llmware/slim-summary", "hf_repo": "llmware/slim-summary",
     "custom_model_files": [], "custom_model_repo": "", "output_type": "list", "function_call": True,
     "marker_tokens": [], "marker_token_lookup": {}, "primary_keys": ["key points (3)"], "fc_output_values": [],
     "function": ["summarize"]},

    {"model_name": "slim-summary-tiny", "display_name": "llmware/slim-summary-tiny",
     "model_family": "HFGenerativeModel", "model_category": "generative_local", "model_location": "hf_repo",
     "context_window": 2048, "instruction_following": False, "prompt_wrapper": "human_bot",
     "temperature": 0.0, "sample_default": False, "trailing_space": "", "gguf_file": "", "gguf_repo": "",
     "link": "https://huggingface.co/llmware/slim-summary-tiny", "hf_repo": "llmware/slim-summary-tiny",
     "custom_model_files": [], "custom_model_repo": "", "output_type": "list", "function_call": True,
     "marker_tokens": [], "marker_token_lookup": {}, "primary_keys": ["key points (3)"], "fc_output_values": [],
     "function": ["summarize"]},

    {"model_name": "slim-summary-tool", "display_name": "slim-summary-tool",
     "model_family": "GGUFGenerativeModel", "model_category": "generative_local",
     "model_location": "llmware_repo", "context_window": 2048, "instruction_following": False,
     "prompt_wrapper": "human_bot", "temperature": 0.0, "sample_default": False, "trailing_space": "",
     "gguf_file": "slim-summarize.gguf", "gguf_repo": "llmware/slim-summary-tool",
     "link": "https://huggingface.co/llmware/slim-summary-tool",
     "custom_model_files": [], "custom_model_repo": "", "output_type": "list",
     "function_call": True, "primary_keys": ["key points (3)"], "fc_output_values": [],
     "tokenizer": "llmware/slim-extract",
     "tokenizer_local": "tokenizer_stablelm.json",
     "marker_tokens": [], "marker_token_lookup": {}, "function": ["summarize"],
     "fetch": {"module": "llmware.models", "method": "pull_snapshot_from_hf"},
     "validation_files": ["slim-summarize.gguf"],
     },

    # adding new slim q-gen models
    {"model_name": "slim-q-gen-phi-3-tool", "display_name": "slim-q-gen-tool",
     "model_family": "GGUFGenerativeModel", "model_category": "generative_local", "model_location": "llmware_repo",
     "context_window": 4096, "instruction_following": False, "prompt_wrapper": "human_bot",
     "temperature": 0.3, "sample_default": True, "trailing_space": "",
     "gguf_file": "q_gen.gguf",
     "gguf_repo": "llmware/slim-q-gen-phi-3-tool",
     "link": "https://huggingface.co/llmware/slim-q-gen-phi-3-tool",
     "custom_model_files": [], "custom_model_repo": "",
     "output_type": "dict",
     "function_call": True,
     "primary_keys": ["question"],
     "fc_output_values": [],
     "tokenizer": "microsoft/Phi-3-mini-4k-instruct",
     "tokenizer_local": "tokenizer_phi3.json",
     "marker_tokens": [], "marker_token_lookup": {},
     "function": ["generate"],
     "fetch": {"module": "llmware.models", "method": "pull_snapshot_from_hf"},
     "validation_files": ["q_gen.gguf"]},

    {"model_name": "slim-q-gen-tiny-tool", "display_name": "llmware/slim-q-gen-tiny-tool",
     "model_family": "GGUFGenerativeModel", "model_category": "generative_local", "model_location": "llmware_repo",
     "context_window": 4096, "instruction_following": False, "prompt_wrapper": "human_bot",
     "temperature": 0.5, "sample_default": True, "trailing_space": "",
     "gguf_file": "q_gen.gguf",
     "gguf_repo": "llmware/slim-q-gen-tiny-tool",
     "link": "https://huggingface.co/slim-q-gen-tiny-tool",
     "custom_model_files": [], "custom_model_repo": "",
     "output_type": "dict",
     "function_call": True,
     "primary_keys": ["question"],
     "fc_output_values": [],
     "tokenizer": "llmware/slim-sentiment",
     "tokenizer_local": "tokenizer_tl.json",
     "marker_tokens": [], "marker_token_lookup": {},
     "function": ["generate"],
     "fetch": {"module": "llmware.models", "method": "pull_snapshot_from_hf"},
     "validation_files": ["q_gen.gguf"],
     },

    {"model_name": "llmware/slim-q-gen-tiny", "display_name": "slim-q-gen-tiny",
     "model_family": "HFGenerativeModel", "model_category": "generative_local", "model_location": "hf_repo",
     "context_window": 2048, "instruction_following": False, "prompt_wrapper": "human_bot",
     "temperature": 0.5, "sample_default": True, "trailing_space": "", "gguf_file": "", "gguf_repo": "",
     "link": "https://huggingface.co/llmware/slim-q-gen-tiny",
     "hf_repo": "llmware/slim-q-gen-tiny",
     "custom_model_files": [""], "custom_model_repo": "",
     "output_type": "dict", "function_call": True,
     "primary_keys": ["question"],
     "fc_output_values": ["question"],
     "marker_tokens": [],
     "marker_token_lookup": {},
     "function": ["generate"]},

    {"model_name": "llmware/slim-q-gen-phi-3", "display_name": "slim-q-gen-phi-3",
     "model_family": "HFGenerativeModel", "model_category": "generative_local", "model_location": "hf_repo",
     "context_window": 2048, "instruction_following": False, "prompt_wrapper": "human_bot",
     "temperature": 0.5, "sample_default": True, "trailing_space": "", "gguf_file": "", "gguf_repo": "",
     "link": "https://huggingface.co/llmware/slim-q-gen-phi-3",
     "hf_repo": "llmware/slim-q-gen-phi-3",
     "custom_model_files": [""], "custom_model_repo": "",
     "output_type": "dict", "function_call": True,
     "primary_keys": ["question"],
     "fc_output_values": ["question"],
     "marker_tokens": [],
     "marker_token_lookup": {},
     "function": ["generate"]},

     {"model_name": "slim-qa-gen-tiny-tool", "display_name": "llmware/slim-qa-gen-tiny-tool",
      "model_family": "GGUFGenerativeModel", "model_category": "generative_local", "model_location": "llmware_repo",
      "context_window": 4096, "instruction_following": False, "prompt_wrapper": "human_bot",
      "temperature": 0.5, "sample_default": True, "trailing_space": "",
      "gguf_file": "qa_gen_v3.gguf",
      "gguf_repo": "llmware/slim-qa-gen-tiny-tool",
      "link": "https://huggingface.co/slim-qa-gen-tiny-tool",
      "custom_model_files": [], "custom_model_repo": "",
      "output_type": "dict",
      "function_call": True,
      "primary_keys": ["question, answer"],    # also accepts boolean and multiple choice
      "fc_output_values": [],
      "tokenizer": "llmware/slim-sentiment",
      "tokenizer_local": "tokenizer_tl.json",
      "marker_tokens": [], "marker_token_lookup": {},
      "function": ["generate"],
      "fetch": {"module": "llmware.models", "method": "pull_snapshot_from_hf"},
      "validation_files": ["qa_gen_v3.gguf"],
      },

    {"model_name": "slim-qa-gen-phi-3-tool", "display_name": "slim-qa-gen-phi-3-tool",
     "model_family": "GGUFGenerativeModel", "model_category": "generative_local", "model_location": "llmware_repo",
     "context_window": 4096, "instruction_following": False, "prompt_wrapper": "human_bot",
     "temperature": 0.3, "sample_default": True, "trailing_space": "",
     "gguf_file": "qa_gen_v3.gguf",
     "gguf_repo": "llmware/slim-qa-gen-phi-3-tool",
     "link": "https://huggingface.co/llmware/slim-qa-gen-phi-3-tool",
     "custom_model_files": [], "custom_model_repo": "",
     "output_type": "dict",
     "function_call": True,
     "primary_keys": ["question, answer"],    # also accepts boolean and multiple choice
     "fc_output_values": [],
     "tokenizer": "microsoft/Phi-3-mini-4k-instruct",
     "tokenizer_local": "tokenizer_phi3.json",
     "marker_tokens": [], "marker_token_lookup": {},
     "function": ["generate"],
     "fetch": {"module": "llmware.models", "method": "pull_snapshot_from_hf"},
     "validation_files": ["qa_gen_v3.gguf"]},

     {"model_name": "llmware/slim-qa-gen-tiny", "display_name": "slim-qa-gen-tiny",
      "model_family": "HFGenerativeModel", "model_category": "generative_local", "model_location": "hf_repo",
      "context_window": 2048, "instruction_following": False, "prompt_wrapper": "human_bot",
      "temperature": 0.5, "sample_default": True, "trailing_space": "", "gguf_file": "", "gguf_repo": "",
      "link": "https://huggingface.co/llmware/slim-qa-gen-tiny",
      "hf_repo": "llmware/slim-qa-gen-tiny",
      "custom_model_files": [""], "custom_model_repo": "",
      "output_type": "dict", "function_call": True,
      "primary_keys": ["question, answer"],
      "fc_output_values": ["question, answer"],
      "marker_tokens": [],
      "marker_token_lookup": {},
      "function": ["generate"]},

     {"model_name": "llmware/slim-qa-gen-phi-3", "display_name": "slim-qa-gen-phi-3",
      "model_family": "HFGenerativeModel", "model_category": "generative_local", "model_location": "hf_repo",
      "context_window": 2048, "instruction_following": False, "prompt_wrapper": "human_bot",
      "temperature": 0.5, "sample_default": True, "trailing_space": "", "gguf_file": "", "gguf_repo": "",
      "link": "https://huggingface.co/llmware/slim-qa-gen-phi-3",
      "hf_repo": "llmware/slim-qa-gen-phi-3",
      "custom_model_files": [""], "custom_model_repo": "",
      "output_type": "dict", "function_call": True,
      "primary_keys": ["question, answer"],
      "fc_output_values": ["question, answer"],
      "marker_tokens": [],
      "marker_token_lookup": {},
      "function": ["generate"]}

]

""" Fine-tuning Prompt Wrappers - virtually all instruct fine-tuned models will have a special 'prompt wrapper' 
that is an artifact from fine-tuning and needs to be applied consistently to lead to the expected model behavior.   
There are a number of common formats captured in the default catalog, but can be extended through ModelCatalog.   
When constructing the prompt, this wrapper will be applied automatically. """

global_model_finetuning_prompt_wrappers_lookup = {

        #   each wrapper can consist of up to 5 elements to represent common segments of the prompt
        #   1.  optional - "system_start" and "system_stop"
        #   2.  required - "main_start" and "main_stop"
        #   3.  required - "start_llm_response"

        "human_bot": {"main_start": "<human>: ", "main_stop": "\n", "start_llm_response": "<bot>:"},

        "<INST>": {"main_start": "<INST>", "main_stop": "</INST>", "start_llm_response": ""},

        "hf_chat": {"system_start": "<|im_start|>system\n", "system_stop": "<|im_end|>\n",
                    "main_start": "<|im_start|>user", "main_stop": "<|im_end|>\n",
                    "start_llm_response": "<|im_start|>assistant"},

        "open_chat": {"main_start": "GPT4 User: ", "main_stop": "<|endofturn|>",
                      "start_llm_response": "GPT4 Assistant:"},

        "alpaca": {"main_start": "### Instruction: ", "main_stop": "\n",
                   "start_llm_response": "### Response: "},

        "chat_ml": {"system_start": "<|im_start|>system", "system_stop":"<|im_end|>\n",
                    "main_start":"<|im_start|>user", "main_stop":"<|im_end|>\n",
                    "start_llm_response":"<|im_start|>assistant"},

        "phi_3": {"system_start": "<|system|>\n", "system_stop": "<|end|>\n",
                  "main_start": "<|user|>\n", "main_stop": "<|end|>\n", "start_llm_response": "<|assistant|>"},

        "llama_3_chat": {"system_start": "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
                         "system_stop": "<|eot_id|>",
                         "main_start": "<|start_header_id|>user>|end_header_id|>\n",
                         "main_stop": "<|eot_id|>",
                         "start_llm_response": "<|start_header_id|>assistant<|end_header_id|>\n"},

        "tiny_llama_chat": {"system_start": "<|system|>", "system_stop": "</s>",
                            "main_start": "<|user|>", "main_stop": "</s>",
                            "start_llm_response": "<|assistant|>"},

        "stablelm_zephyr_chat": {"system_start": "", "system_stop": "",
                                 "main_start": "<|user|>", "main_stop": "<|endoftext|>\n",
                                 "start_llm_response": "<|assistant|>"},

        "google_gemma_chat": {"system_start": "", "system_stop": "",
                              "main_start": "<bos><start_of_turn>user\n",
                              "main_stop": "<end_of_turn>\n",
                              "start_llm_response": "<start_of_turn>model"},

        "vicuna_chat": {"system_start": "", "system_stop": "",
                        "main_start": "USER: ", "main_stop": "",
                        "start_llm_response": " ASSISTANT:"}

}

""" Global default prompt catalog consists of a set of prebuilt useful prompt instructions across a wide range
of models.   Unlike prompt_wrappers, which tend to be an attribute of the model, the prompt catalog can be invoked
on a 'prompt-by-prompt' basis to drive different behavior from a model.   Note:  not all models will support
 very complex open-ended instructions or respond in a consistent manner. """

global_default_prompt_catalog = [

    {"prompt_name": "just_the_facts",
     "prompt_description": "Closed Context - read passage, answer question, stick to the facts.",
     "run_order": ["blurb1", "$context", "blurb2", "$query", "instruction"],
     "blurb1": "Please read the following text: ",
     "blurb2": " Please answer the question: ",
     "instruction": "In providing the answer, please only use facts contained in the text.",
     "system_message": "You are a helpful assistant who speaks with facts and no wasted words.",
     "user_vars": {}},

    {"prompt_name": "answer_or_not_found",
     "prompt_description": "Closed Context - read passage, answer question, provide 'Not Found' if no answer in text.",
     "run_order": ["blurb1", "$context", "blurb2", "$query", "instruction"],
     "blurb1": "Please read the following text: ",
     "blurb2": " Please answer the question: ",
     "instruction": "Please only use facts in the text.  If the text does not provide the answer, then please "
                    "respond with: {{not_found_response}}",
     "system_message": "You are a helpful assistant who speaks with facts and no wasted words.",
     "user_vars": {"not_found_response": "'Not Found.'"}},

    {"prompt_name": "number_or_none",
     "prompt_description": "Closed Context - read passage, answer question, provide 'Not Found' if no answer in text.",
     "run_order": ["blurb1", "$context", "blurb2", "$query","instruction"],
     "blurb1" : "Please read the following text: ",
     "blurb2" : " Please answer the question: ",
     "instruction": "Please provide a specific number as an answer from the text.  "
                    "If the text does not provide a specific numerical answer, then please respond "
                    "with: {{not_found_response}}",
     "system_message": "You are a helpful assistant who speaks with facts and no wasted words.",
     "user_vars": {"not_found_response": "'Not Found.'"}},

    {"prompt_name": "summarize_with_bullets",
     "prompt_description": "Basic summarization with open ended number of bullet points.",
     "run_order": ["blurb1", "$context", "instruction"],
     "blurb1": "Please read the following text: ",
     "instruction": "Please summarize with bulletpoints.",
     "system_message": "You are a helpful assistant who speaks with facts and no wasted words.",
     "user_vars": {}},

    {"prompt_name": "summarize_with_numbered_bullets",
     "prompt_description": "Summarization with specified number of bullet points.",
     "run_order": ["blurb1", "$context", "instruction"],
     "blurb1": "Please read the following text: ",
     "instruction": "Please summarize the text with approximately {{number_of_bulletpoints}} numbered bulletpoints.",
     "system_message": "You are a helpful assistant who speaks with facts and no wasted words.",
     "user_vars": {"number_of_bulletpoints": 5}},

    {"prompt_name": "xsummary",
     "prompt_description": "Xtreme summarization with specified number of words.",
     "run_order": ["blurb1", "$context", "instruction"],
     "blurb1": "Please read the following text: ",
     "instruction": "Please summarize the text in no more than {{number_of_words}} words.",
     "system_message": "You are a helpful assistant who speaks with facts and no wasted words.",
     "user_vars": {"number_of_words": 25}},

    {"prompt_name": "completion",
     "prompt_description": "Open context text generation to complete starting point provided in prompt.",
     "run_order": ["blurb1", "$query", "instruction"],
     "blurb1": "Here is the starting point of a longer text: ",
     "instruction": "Please complete this text in the style provided in the text.",
     "system_message": "You are a helpful assistant who is a good creative writer.",
     "user_vars": {}},

    {"prompt_name": "dialog_summary",
     "prompt_description": "General summarization of a conversation text with specified number of bullet points.",
     "run_order": ["blurb1", "$context", "instruction"],
     "blurb1": "Please read the following discussion between two parties: ",
     "instruction": "Please summarize the key points from the conversation using less "
                    "than {{number_of_bulletpoints}} bulletpoints.",
     "system_message": "You are a helpful assistant.",
     "user_vars": {"number_of_bulletpoints": 10}},

    {"prompt_name": "not_found_classifier",
     "prompt_description": "Not Found Response classifier - used to ask a model to classify a particular response "
                           "as 'not found' - very useful in RAG applications.",
     "run_order": ["blurb1", "blurb2", "$context", "instruction"],
     "blurb1": "Here are several examples of a 'not found' response: "
               "Not Found \n"
               "The text does not provide an answer. \n"
               "The answer is not clear. \n"
               "Sorry, I could not find a definitive answer. \n"
               "The answer is not provided in the information given. \n"
               "The text does not specify the answer to this question. \n",
     "blurb2": "Here is a new example: ",
     "instruction": "Please respond 'Yes' or 'No' if this new example is a 'Not Found' response.",
     "system_message": "You are a helpful assistant.",
     "user_vars": {}},

    {"prompt_name": "top_level_select",
     "prompt_description": "Select the best answer among choices provided.",
     "run_order": ["blurb1", "$query", "blurb2","$context", "instruction"],
     "blurb1": "We are trying to answer the following question: ",
     "blurb2": "Which of the following selections best answers the question?",
     "instruction": "Please respond with the best answer among these selections.  "
                    "If more than one answer is useful, please summarize with bulletpoints.",
     "system_message": "You are a helpful assistant who speaks with facts and no wasted words.",
     "user_vars": {}},

    {"prompt_name": "answer_question_in_role",
     "prompt_description": "Answer a question with a specific role or point of view.",
     "run_order": ["blurb1", "$context", "blurb2", "$query", "instruction"],
     "blurb1": "Please read the following text: ",
     "blurb2": "Please answer the following question: ",
     "instruction": "In providing an answer to the question, please assume the perspective of a {{role}} and "
                    "write in that style.",
     "system_message": "You are a helpful assistant.",
     "user_vars": {"role": "business analyst"}},

    {"prompt_name": "editor_in_role",
     "prompt_description": "Edit a passage with a specific role or point of view.",
     "run_order": ["blurb1", "$context", "instruction"],
     "blurb1": "Please read the following text: ",
     "instruction": "Our task is to edit and improve the language of the text from the perspective of a business analyst.",
     "system_message": "You are a helpful editor and writer who reads text and improves the writing.",
     "user_vars": {"role": "business analyst"}},

    {"prompt_name": "yes_no",
     "prompt_description": "Answer a question with 'Yes' or 'No'.",
     "run_order": ["blurb1", "$context", "blurb2", "$query", "instruction"],
     "blurb1": "Please read the following text: ",
     "blurb2": "Based on these materials, please answer the question: ",
     "instruction": "Please answer this question with 'Yes' or 'No'.  If the text does not provide an answer,"
                    "then please respond with 'Not Found.'",
     "system_message": "You are a helpful assistant who speaks with facts and no wasted words.",
     "user_vars": {}},

    {"prompt_name": "multiple_choice",
     "prompt_description": "Answer a question using a set of pre-defined choices provided.",
     "run_order": ["blurb1", "$context", "blurb2", "$query", "instruction"],
     "blurb1": "Please read the following text: ",
     "blurb2": "Based on these materials, please answer the question: ",
     "instruction": "Please select from the choices provided.  If the text does not provide an answer,"
                    "then please respond with 'Not Found.'",
     "system_message": "You are a helpful assistant who speaks with facts and no wasted words."},

    {"prompt_name": "default_with_context",
     "prompt_description": "Default simple prompt when a question and context are passed.",
     "run_order": ["blurb1", "$context", "blurb2", "$query"],
     "blurb1": "Please read the following text: ",
     "blurb2": "Based on this text, please answer the question: ",
     "instruction": "",
     "system_message": "You are a helpful assistant who speaks with facts and no wasted words."},

    {"prompt_name": "default_no_context",
     "prompt_description": "Default simple prompt when only a question is passed.",
     "run_order": ["blurb1","$query"],
     "blurb1": "Please discuss the following: ",
     # "blurb2": "Based on this text, please answer the question: ",
     "instruction": "",
     "system_message": "You are a helpful assistant who likes to answer questions."},

    {"prompt_name": "summarize_with_bullets_w_query",
     "prompt_description": "Summarization of a text with a specific question being posed.",
     "run_order": ["blurb1", "$context", "blurb2","$query","instruction"],
     "blurb1": "Please read the following text: ",
     "blurb2": "Please read the following question: ",
     "instruction": "Please summarize with bulletpoints an analysis of the question.",
     "system_message": "You are a helpful assistant who speaks with facts and no wasted words."},

    {"prompt_name": "summarize_with_references_w_query",
     "prompt_description": "Summarization with text with guidance to provide reference to specific "
                           "information in the text passage.",
     "run_order": ["blurb1", "$context", "blurb2", "$query", "instruction"],
     "blurb1": "Please read the following text: ",
     "blurb2": "Please read the following question: ",
     "instruction": "Please provide an analysis of the question using information and specific clauses "
                    "in the text.",
     "system_message": "You are a helpful assistant who speaks with facts and no wasted words."},

    {"prompt_name": "write_poem",
     "prompt_description": "Write a poem prompt - note: results may vary greatly by model.",
     "run_order": ["instruction", "$query"],
     "instruction": "Please write a poem using the following prompt: ",
     "system_message": "You are a helpful assistant who is a creative writer and can rhyme words easily."},

    {"prompt_name": "ten_words",
     "prompt_description": "Xtreme summarization to answer question from a text in 10 words of less.",
     "run_order": ["instruction", "$query", "$context"],
     "blurb1": "Please read the following text: ",
     "blurb2": "Please read the following question: ",
     "instruction": "In no more than ten words, please give concise answer to the following question, using the "
                    "text as evidence to support",
     "system_message": "You are a helpful assistant who speaks with facts and no wasted words."},

    {"prompt_name": "explain_child",
     "prompt_description": "Standard simplified answer prompt - note: results may vary greatly by model.",
     "run_order": ["instruction", "$query", "$context"],
     "instruction": "Please explain to a child the following question using the provided text: ",
     "system_message": "You are a helpful assistant."},

    {"prompt_name": "make_joke",
     "prompt_description": "Standard joke prompt - note:  results may vary greatly by model.",
     "run_order": ["instruction", "$query"],
     "instruction": "Please be funny and tell a joke on the subject of: ",
     "system_message": "You are a helpful assistant with a good sense of humor."},

    {"prompt_name": "tell_story",
     "prompt_description": "Standard tell a story prompt - note: results may vary greatly by model.",
     "run_order": ["instruction", "$query"],
     "instruction": "Please write the start of a story on the topic of: ",
     "system_message": "You are a helpful assistant."},

    {"prompt_name": "write_headline",
     "prompt_description": "Generate a headline from a question and context.",
     "run_order": ["instruction", "$query", "$context"],
     "instruction": "Please write the headline only in a few words in capitalization to answer the question below, "
                    "using the materials provided. ",
     "system_message": "You are a helpful assistant."},

    {"prompt_name": "facts_only",
     "prompt_description": "Basic 'facts only' Q&A prompt.",
     "run_order": ["blurb1", "$context", "blurb2", "$query", "instruction"],
     "blurb1": "Please use the following materials- ",
     "blurb2": "Please answer the following question - ",
     "instruction": "In answering the question, please only use information contained in the provided materials.",
     "system_message": "You are a helpful assistant."},

    {"prompt_name": "top_bulletpoints",
     "prompt_description": "Summarization with question and answer in 5 bullet points.",
     "run_order": ["blurb1", "$context", "blurb2", "$query", "instruction"],
     "blurb1": "Please read the text below -  ",
     "blurb2": "Please read the following question - ",
     "instruction": "Please answer the question using the text, and write no more than 5 bulletpoints.",
     "system_message": "You are a helpful assistant."},

    {"prompt_name": "report_title",
     "prompt_description": "Generate title of report given context passage.",
     "run_order": ["instruction", "$context"],
     "instruction": "Please write the title to a report with the following information:  ",
     "system_message": "You are a helpful assistant."},

    {"prompt_name": "marketing_slogan",
     "prompt_description": "Generate marketing style slogan given context passage.",
     "run_order": ["blurb1", "$context", "blurb2", "$query", "instruction"],
     "blurb1": "Please read the following materials- ",
     "blurb2": "Please answer the following question - ",
     "instruction": "Please write a marketing slogan for the following offering using the following information as "
                    "background source materials.",
     "system_message": "You are a helpful assistant."},

    {"prompt_name": "top_level_summary",
     "prompt_description": "Summarization prompt intended for 'second-level' summaries of materials.",
     "run_order": ["blurb1", "$context", "blurb2", "$query", "instruction"],
     "blurb1": "Please read the following materials- ",
     "blurb2": "Please answer the following question - ",
     "instruction": "In answering the question, please write no more than five bulletpoints, and reference the most "
                    "important facts in the source materials.",
     "system_message": "You are a helpful assistant."},

]


model_benchmark_data = [

 {"model_name": "bling-phi-3-gguf",
  "base_model": "microsoft/Phi-3-mini-4k-instruct",
  "parameters": 3.8,
  "accuracy_score": 100,
  "not_found": 0.95,
  "yes_no": 0.975,
  "math_logic": 0.80,
  "complex_qa": 4,
  "summarization": 4},

 {"model_name": "bling-phi-3.5-gguf",
  "base_model": "microsoft/Phi-3.5-mini-instruct",
  "parameters": 3.8,
  "accuracy_score": 100,
  "not_found": 0.85,
  "yes_no": 0.95,
  "math_logic": 0.90,
  "complex_qa": 4,
  "summarization": 4},

 {"model_name": "dragon-yi-6b-v0",
  "base_model": "01-ai/yi-6b-v1",
  "parameters": 6.0,
  "accuracy_score": 99.5,
  "not_found": 0.90,
  "yes_no": 0.875,
  "math_logic": 0.775,
  "complex_qa": 4,
  "summarization": 4},

 {"model_name": "dragon-mistral-0.3-gguf",
  "base_model": "mistralai/Mistral-7B-v0.3",
  "parameters": 7.0,
  "accuracy_score": 99.5,
  "not_found": 0.90,
  "yes_no": 0.825,
  "math_logic": 0.675,
  "complex_qa": 4,
  "summarization": 4},

 {"model_name": "dragon-qwen2-7b-gguf",
  "base_model": "qwen/Qwen2-7b",
  "parameters": 7.0,
  "accuracy_score": 99,
  "not_found": 0.85,
  "yes_no": 1.0,
  "math_logic": 0.925,
  "complex_qa": 5,
  "summarization": 4},

 {"model_name": "dragon-yi-9b-gguf",
  "base_model": "01-ai/yi-v1.5-9b",
  "parameters": 8.8,
  "accuracy_score": 98,
  "not_found": 0.90,
  "yes_no": 0.925,
  "math_logic": 0.95,
  "complex_qa": 5,
  "summarization": 4},

 {"model_name": "dragon-deci-7b",
  "base_model": "Deci/Deci-7B",
  "parameters": 7.0,
  "accuracy_score": 97.5,
  "not_found": 0.95,
  "yes_no": 0.925,
  "math_logic": 0.9125,
  "complex_qa": 4,
  "summarization": 4},

 {"model_name": "dragon-llama-7b-v0",
  "base_model": "meta-llama/llama-2-base",
  "parameters": 7.0,
  "accuracy_score": 97.25,
  "not_found": 0.925,
  "yes_no": 0.95,
  "math_logic": 0.6375,
  "complex_qa": 3,
  "summarization": 3},

 {"model_name": "dragon-mistral-7b-v0",
  "base_model": "mistralai/mistral-7b-base-0.1",
  "parameters": 7.0,
  "accuracy_score": 96.5,
  "not_found": 0.925,
  "yes_no": 0.9750,
  "math_logic": 0.8125,
  "complex_qa": 4,
  "summarization": 4},

 {"model_name": "dragon-red-pajama-7b-v0",
  "base_model": "togethercomputer/RedPajama-INCITE-7B-Base",
  "parameters": 7.0,
  "accuracy_score": 96,
  "not_found": 0.55,
  "yes_no": 0.8125,
  "math_logic": 0.5250,
  "complex_qa": 3,
  "summarization": 3},

 {"model_name": "dragon-deci-6b",
  "base_model": "Deci/Deci-6B",
  "parameters": 6.0,
  "accuracy_score": 94.25,
  "not_found": 0.775,
  "yes_no": 0.9625,
  "math_logic": 0.6875,
  "complex_qa": 3,
  "summarization": 3},

 {"model_name": "dragon-llama-8b-3.1-gguf",
  "base_model": "meta-llama/meta-llama-8b-3.1-base",
  "parameters": 8.0,
  "accuracy_score": 94,
  "not_found": 0.70,
  "yes_no": 0.90,
  "math_logic": 0.7250,
  "complex_qa": 4,
  "summarization": 4},

 {"model_name": "dragon-stablelm-7b-v0",
  "base_model": "StableLM-7b-v2",
  "parameters": 7.0,
  "accuracy_score": 94,
  "not_found": 0.85,
  "yes_no": 0.8875,
  "math_logic": 0.6250,
  "complex_qa": 3,
  "summarization": 3},

 {"model_name": "dragon-falcon-7b-v0",
  "base_model": "tiiuae/falcon-7b",
  "parameters": 7.0,
  "accuracy_score": 94,
  "not_found": 0.75,
  "yes_no": 0.8125,
  "math_logic": 0.6675,
  "complex_qa": 3,
  "summarization": 3},

 {"model_name": "bling-stablelm-3b",
  "base_model": "stabilityai/stablelm-3b-4e1t",
  "parameters": 2.8,
  "accuracy_score": 94,
  "not_found": 0.675,
  "yes_no": 0.78,
  "math_logic": 0.29,
  "complex_qa": 3,
  "summarization": 3},

 {"model_name": "bling-qwen-mini-tool",
  "base_model": "Qwen/Qwen2-1.5b",
  "parameters": 1.5,
  "accuracy_score": 93.5,
  "not_found": 0.75,
  "yes_no": 0.875,
  "math_logic": 0.70,
  "complex_qa": 3,
  "summarization": 3},

 {"model_name": "bling-phi-2",
  "base_model": "microsoft/phi-2",
  "parameters": 2.8,
  "accuracy_score": 93,
  "not_found": 0.95,
  "yes_no": 0.850,
  "math_logic": 0.8250,
  "complex_qa": 3,
  "summarization": 3},

 {"model_name": "bling-red-pajamas-3b",
  "base_model": "togethercomputer/RedPajama-INCITE-Instruct-3B-v1",
  "parameters": 2.8,
  "accuracy_score": 92,
  "not_found": 0.45,
  "yes_no": 0.75,
  "math_logic": 0.20,
  "complex_qa": 2,
  "summarization": 3},

 {"model_name": "bling-sheared-llama-2.7b",
  "base_model": "princeton-nlp/Sheared-LLaMA-2.7B",
  "parameters": 2.7,
  "accuracy_score": 90.25,
  "not_found": 0.60,
  "yes_no": 0.80,
  "math_logic": 0.50,
  "complex_qa": 2,
  "summarization": 3},

 {"model_name": "bling-falcon-1b",
  "base_model": "tiiuae/falcon-1b",
  "parameters": 1.3,
  "accuracy_score": 89,
  "not_found": 0.575,
  "yes_no": 0.58,
  "math_logic": 0.25,
  "complex_qa": 1,
  "summarization": 3},

 {"model_name": "bling-phi-1.5",
  "base_model": "microsoft/phi-1.5",
  "parameters": 1.5,
  "accuracy_score": 87.75,
  "not_found": 0.475,
  "yes_no": 0.80,
  "math_logic": 0.5375,
  "complex_qa": 3,
  "summarization": 3},

 {"model_name": "bling-tiny-llama-v0",
  "base_model": "tinyllama/tinyllama-3T-1.1-v0[confirm]",
  "parameters": 1.1,
  "accuracy_score": 86.5,
  "not_found": 0.85,
  "yes_no": 0.825,
  "math_logic": 0.3750,
  "complex_qa": 3,
  "summarization": 3},

 {"model_name": "bling-sheared-llama-1.3b",
  "base_model": "princeton-nlp/Sheared-LLaMA-1.3B",
  "parameters": 1.3,
  "accuracy_score": 84.5,
  "not_found": 0.20,
  "yes_no": 0.6625,
  "math_logic": 0.0940,
  "complex_qa": 1,
  "summarization": 3},

 {"model_name": "bling-qwen-nano-tool",
  "base_model": "Qwen/Qwen2-0.5b",
  "parameters": 0.5,
  "accuracy_score": 81,
  "not_found": 0.65,
  "yes_no": 0.6250,
  "math_logic": 0.4250,
  "complex_qa": 3,
  "summarization": 3},

 {"model_name": "bling-1b-0.1",
  "base_model": "EleutherAI/pythia-1b",
  "parameters": 1.0,
  "accuracy_score": 73.25,
  "not_found": 0.1750,
  "yes_no": 0.29,
  "math_logic": 0.0,
  "complex_qa": 1,
  "summarization": 1},

 {"model_name": "bling-1.4b-0.1",
  "base_model": "EleutherAI/pythia-1.4b",
  "parameters": 1.4,
  "accuracy_score": 82.25,
  "not_found": 0.40,
  "yes_no": 0.6125,
  "math_logic": 0.0875,
  "complex_qa": 1,
  "summarization": 2}
]

