# llmware
![Static Badge](https://img.shields.io/badge/python-3.9_%7C_3.10%7C_3.11%7C_3.12-blue?color=blue)
![PyPI - Version](https://img.shields.io/pypi/v/llmware?color=blue)
[![discord](https://img.shields.io/badge/Chat%20on-Discord-blue?logo=discord&logoColor=white)](https://discord.gg/MhZn5Nc39h)   
[![Documentation](https://github.com/llmware-ai/llmware/actions/workflows/pages.yml/badge.svg)](https://github.com/llmware-ai/llmware/actions/workflows/pages.yml)

![DevFest GIF](https://i.giphy.com/media/v1.Y2lkPTc5MGI3NjExc3dodTV4czFsd2lrYWV5N3BhaXV5MXpucDhrcWZ2ODF4amM2aXo3diZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/Bkax2GRzAt0PDHcmSq/giphy.gif)

<div style="display: flex; justify-content: center; margin: 20px 0;">
    <a href="README.md" style="padding: 5px 10px; background-color: #e7e7e7; color: #333; text-decoration: none; border: 1px solid #ccc; border-radius: 3px; margin-right: 5px; font-size: 12px;">English</a>
    <a href="README_fr.md" style="padding: 5px 10px; background-color: #e7e7e7; color: #333; text-decoration: none; border: 1px solid #ccc; border-radius: 3px; font-size: 12px;">Fran√ßais</a>
</div>


**Les gagnants s√©lectionn√©s remporteront un prix de 25 $ sous forme de parrainage GitHub !**

## üß∞üõ†Ô∏èüî©Construire des pipelines RAG d'entreprise avec de petits mod√®les sp√©cialis√©s  

`llmware` fournit un cadre unifi√© pour construire des applications bas√©es sur les LLM (par exemple, RAG, Agents), en utilisant de petits mod√®les sp√©cialis√©s qui peuvent √™tre d√©ploy√©s en priv√©, int√©gr√©s en toute s√©curit√© aux sources de connaissances d'entreprise et adapt√©s de mani√®re rentable √† tout processus m√©tier.  

`llmware` comprend deux composants principaux :  

1. **Pipeline RAG** - des composants int√©gr√©s pour tout le cycle de vie de la connexion des sources de connaissances aux mod√®les d'IA g√©n√©rative ; et 

2. **50+ petits mod√®les sp√©cialis√©s** adapt√©s √† des t√¢ches cl√©s dans l'automatisation des processus d'entreprise, y compris les questions-r√©ponses factuelles, la classification, le r√©sum√© et l'extraction.  

En rassemblant ces deux composants et en int√©grant des mod√®les open source de premier plan et des technologies sous-jacentes, `llmware` offre un ensemble complet d'outils pour construire rapidement des applications LLM bas√©es sur la connaissance pour les entreprises.  

La plupart de nos exemples peuvent √™tre ex√©cut√©s sans serveur GPU - commencez tout de suite sur votre ordinateur portable.   

[Rejoignez-nous sur Discord](https://discord.gg/MhZn5Nc39h)   |  [Regardez nos tutoriels sur YouTube](https://www.youtube.com/@llmware)  | [Explorez nos familles de mod√®les sur Huggingface](https://www.huggingface.co/llmware)   

Nouveau dans les Agents ?  [Consultez la s√©rie de d√©marrage rapide des Agents](https://github.com/llmware-ai/llmware/tree/main/fast_start/agents)  

Nouveau dans RAG ?  [Regardez la s√©rie de vid√©os de d√©marrage rapide](https://www.youtube.com/playlist?list=PL1-dn33KwsmD7SB9iSO6vx4ZLRAWea1DB)  

üî•üî•üî• [**Agents Multi-Mod√®les avec des Mod√®les SLIM**](examples/SLIM-Agents/) - [**Vid√©o d'Introduction**](https://www.youtube.com/watch?v=cQfdaTcmBpY) üî•üî•üî•   

[Introduction aux Mod√®les SLIM avec Appels de Fonction](https://github.com/llmware-ai/llmware/blob/main/examples/Models/using_function_calls.py)  
Pas envie d'attendre ? Obtenez les SLIM tout de suite :

```python 
from llmware.models import ModelCatalog

ModelCatalog().get_llm_toolkit()  # obtenez tous les mod√®les SLIM, livr√©s en tant qu'outils quantifi√©s petits et rapides 
ModelCatalog().tool_test_run("slim-sentiment-tool") # voyez le mod√®le en action avec un script de test inclus  
```

## üéØ  Fonctionnalit√©s cl√©s 
√âcrire du code avec `llmware` repose sur quelques concepts principaux :

<details>
<summary><b>Catalogue de Mod√®les</b>: Acc√©dez √† tous les mod√®les de la m√™me mani√®re avec une recherche facile, quel que soit leur impl√©mentation sous-jacente. 
</summary>  

```python
#  150+ mod√®les dans le catalogue avec plus de 50 mod√®les RAG optimis√©s BLING, DRAGON et Industry BERT
#  Support complet pour GGUF, HuggingFace, Sentence Transformers et principaux mod√®les bas√©s sur API
#  Facile √† √©tendre pour ajouter des mod√®les personnalis√©s - voir les exemples

from llmware.models import ModelCatalog
from llmware.prompts import Prompt

#   tous les mod√®les sont accessibles via le ModelCatalog
models = ModelCatalog().list_all_models()

#   pour utiliser un mod√®le du catalogue - m√©thode "load_model" et passer le param√®tre model_name
my_model = ModelCatalog().load_model("llmware/bling-phi-3-gguf")
output = my_model.inference("quel est l'avenir de l'IA ?", add_context="Voici l'article √† lire")

#   pour int√©grer le mod√®le dans une invite
prompter = Prompt().load_model("llmware/bling-tiny-llama-v0")
response = prompter.prompt_main("quel est l'avenir de l'IA ?", context="Ins√©rer des sources d'information")
```

</details>  

<details>  
<summary><b>Biblioth√®que</b>: ing√©rer, organiser et indexer une collection de connaissances √† grande √©chelle - Analyse, Fragmentation du Texte et Insertion. </summary>  

```python

from llmware.library import Library

#   pour analyser et fragmenter un ensemble de documents (pdf, pptx, docx, xlsx, txt, csv, md, json/jsonl, wav, png, jpg, html)  

#   √©tape 1 - cr√©er une biblioth√®que, qui est la structure de "conteneur de base de connaissances"
#          - les biblioth√®ques ont √† la fois des ressources de collection de texte (DB) et des ressources de fichiers (par exemple, llmware_data/accounts/{library_name})
#          - les embeddings et les requ√™tes sont ex√©cut√©s contre une biblioth√®que

lib = Library().create_new_library("ma_biblioth√®que")

#    √©tape 2 - add_files est la fonction d'ingestion universelle - pointez-la vers un dossier local avec des types de fichiers mixtes
#           - les fichiers seront trait√©s par extension de fichier pour √™tre analys√©s, fragment√©s en texte et index√©s dans la base de donn√©es de collection de texte

lib.add_files("/chemin/dossier/vers/mes/fichiers")

#   pour installer un embedding sur une biblioth√®que - choisissez un mod√®le d'embedding et une base de donn√©es vectorielle
lib.install_new_embedding(embedding_model_name="mini-lm-sbert", vector_db="milvus", batch_size=500)

#   pour ajouter un deuxi√®me embedding √† la m√™me biblioth√®que (mod√®les + base de donn√©es vectorielle mix-and-match)  
lib.install_new_embedding(embedding_model_name="industry-bert-sec", vector_db="chromadb", batch_size=100)

#   facile de cr√©er plusieurs biblioth√®ques pour diff√©rents projets et groupes

finance_lib = Library().create_new_library("finance_q4_2023")
finance_lib.add_files("/dossier_finance/")

hr_lib = Library().create_new_library("politiques_rh")
hr_lib.add_files("/dossier_rh/")

#    tirer la carte de la biblioth√®que avec les m√©tadonn√©es cl√©s - documents, fragments de texte, images, tableaux, enregistrement d'embedding
lib_card = Library().get_library_card("ma_biblioth√®que")

#   voir toutes les biblioth√®ques
all_my_libs = Library().get_all_library_cards()

```
</details>  

<details> 
<summary><b>Requ√™te</b>: interroger des biblioth√®ques avec un m√©lange de texte, s√©mantique, hybride, m√©tadonn√©es et filtres personnalis√©s. </summary>

```python

from llmware.retrieval import Query
from llmware.library import Library

#   √©tape 1 - charger la biblioth√®que pr√©c√©demment cr√©√©e 
lib = Library().load_library("ma_biblioth√®que")

#   √©tape 2 - cr√©er un objet de requ√™te et passer la biblioth√®que
q = Query(lib)

#    √©tape 3 - ex√©cuter plusieurs requ√™tes diff√©rentes (de nombreuses autres options dans les exemples)

#    requ√™te de texte de base
results1 = q.text_query("requ√™te de texte", result_count=20, exact_mode=False)

#    requ√™te s√©mantique
results2 = q.semantic_query("requ√™te s√©mantique", result_count=10)

#    combinaison d'une requ√™te textuelle limit√©e √† certains documents de la biblioth√®que et "correspondance exacte" avec la requ√™te
results3 = q.text_query_with_document_filter("nouvelle requ√™te", {"file_name": "nom de fichier s√©lectionn√©"}, exact_mode=True)

#   pour appliquer un embedding sp√©cifique (si plusieurs sur la biblioth√®que), passer les noms lors de la cr√©ation de l'objet de requ√™te
q2 = Query(lib, embedding_model_name="mini_lm_sbert", vector_db="milvus

")

```
</details>

Here‚Äôs the converted text in French:

<details> 
<summary><b>Mod√®les optimis√©s RAG</b> - Mod√®les de 1 √† 7 milliards de param√®tres con√ßus pour l'int√©gration dans les flux de travail RAG et fonctionnant localement.</summary>

```python
""" Cet exemple 'Hello World' d√©montre comment commencer √† utiliser des mod√®les BLING locaux avec le contexte fourni, 
en utilisant √† la fois les versions Pytorch et GGUF. """

import time
from llmware.prompts import Prompt


def hello_world_questions():

    test_list = [

    {"query": "Quel est le montant total de la facture?",
     "answer": "$22,500.00",
     "context": "Services Vendor Inc. \n100 Elm Street Pleasantville, NY \n√Ä Alpha Inc. 5900 1st Street "
                "Los Angeles, CA \nDescription Service d'ing√©nierie frontale $5000.00 \n Service d'ing√©nierie"
                " arri√®re $7500.00 \n Gestion de la qualit√© $10,000.00 \n Montant total $22,500.00 \n"
                "Faites tous les ch√®ques √† l'ordre de Services Vendor Inc. Le paiement est d√ª dans les 30 jours."
                "Si vous avez des questions concernant cette facture, contactez Bia Hermes. "
                "MERCI POUR VOTRE COMMANDE !  FACTURE FACTURE # 0001 DATE 01/01/2022 POUR Projet Alpha P.O. # 1000"},

    {"query": "Quel √©tait le montant de l'exc√©dent commercial?",
     "answer": "62,4 milliards de yens (416,6 millions de dollars)",
     "context": "Le solde commercial du Japon pour septembre passe en exc√©dent, surprenant les attentes."
                "Le Japon a enregistr√© un exc√©dent commercial de 62,4 milliards de yens (416,6 millions de dollars) pour septembre, "
                "d√©passant les attentes des √©conomistes interrog√©s par Reuters, qui tablaient sur un d√©ficit commercial de 42,5 "
                "milliards de yens. Les donn√©es de l'agence des douanes japonaises ont r√©v√©l√© que les exportations en septembre "
                "ont augment√© de 4,3 % par rapport √† l'ann√©e pr√©c√©dente, tandis que les importations ont chut√© de 16,3 % par rapport √† "
                "la m√™me p√©riode l'ann√©e derni√®re. Selon FactSet, les exportations vers l'Asie ont chut√© pour le neuvi√®me mois cons√©cutif, "
                "r√©v√©lant une faiblesse persistante en Chine. Les exportations ont √©t√© soutenues par les envois vers "
                "les march√©s occidentaux, a ajout√© FactSet. ‚Äî Lim Hui Jie"},

    {"query": "Quand le march√© des machines LISP s'est-il effondr√©?",
     "answer": "1987.",
     "context": "Les participants sont devenus les leaders de la recherche en IA dans les ann√©es 1960."
                " Ils et leurs √©tudiants ont produit des programmes que la presse a d√©crits comme '√©tonnants': "
                "des ordinateurs apprenaient des strat√©gies de dames, r√©solvaient des probl√®mes de mots en alg√®bre, "
                "d√©montraient des th√©or√®mes logiques et parlaient anglais.  Au milieu des ann√©es 1960, la recherche aux "
                "√âtats-Unis √©tait fortement financ√©e par le D√©partement de la D√©fense et des laboratoires avaient √©t√© "
                "√©tablis √† travers le monde. Herbert Simon a pr√©dit que 'les machines seront capables, "
                "dans vingt ans, de faire tout travail qu'un homme peut faire'. Marvin Minsky √©tait d'accord, √©crivant, "
                "'dans une g√©n√©ration ... le probl√®me de la cr√©ation de 'l'intelligence artificielle' sera "
                "substantiellement r√©solu'. Ils avaient, cependant, sous-estim√© la difficult√© du probl√®me.  "
                "Les gouvernements am√©ricain et britannique ont interrompu la recherche exploratoire en r√©ponse "
                "aux critiques de Sir James Lighthill et √† la pression continue du Congr√®s am√©ricain "
                "pour financer des projets plus productifs. Le livre de Minsky et Papert, Perceptrons, a √©t√© compris "
                "comme prouvant que l'approche des r√©seaux de neurones artificiels ne serait jamais utile pour r√©soudre "
                "des t√¢ches du monde r√©el, disqualifiant ainsi compl√®tement l'approche.  L' 'hiver de l'IA', une p√©riode "
                "o√π obtenir un financement pour des projets d'IA √©tait difficile, a suivi.  Au d√©but des ann√©es 1980, "
                "la recherche en IA a √©t√© relanc√©e par le succ√®s commercial des syst√®mes experts, une forme d'IA "
                "qui simule les connaissances et les comp√©tences analytiques d'experts humains. D'ici 1985, "
                "le march√© de l'IA avait atteint plus d'un milliard de dollars. En m√™me temps, le cinqui√®me projet "
                "d'ordinateur de g√©n√©ration du Japon a inspir√© les gouvernements am√©ricain et britannique √† r√©tablir le financement "
                "de la recherche acad√©mique. Cependant, √† partir de l'effondrement du march√© des machines Lisp "
                "en 1987, l'IA est tomb√©e √† nouveau en disgr√¢ce, et un deuxi√®me hiver, plus long, a commenc√©."},

    {"query": "Quel est le taux actuel des obligations du Tr√©sor √† 10 ans?",
     "answer": "4,58%",
     "context": "Les actions ont grimp√© vendredi m√™me apr√®s la publication de donn√©es sur l'emploi am√©ricain plus fortes que pr√©vu "
                "et d'une augmentation majeure des rendements des obligations du Tr√©sor.  L'indice Dow Jones Industrial a gagn√© 195,12 points, "
                "soit 0,76 %, pour cl√¥turer √† 31 419,58. Le S&P 500 a ajout√© 1,59 % √† 4 008,50. Le Nasdaq Composite, riche en technologie, "
                "a augment√© de 1,35 %, cl√¥turant √† 12 299,68. L'√©conomie am√©ricaine a ajout√© 438 000 emplois en "
                "ao√ªt, a d√©clar√© le D√©partement du Travail. Les √©conomistes interrog√©s par Dow Jones s'attendaient √† 273 000 "
                "emplois. Cependant, les salaires ont augment√© moins que pr√©vu le mois dernier.  Les actions ont enregistr√© un retournement "
                "stup√©fiant vendredi, apr√®s avoir initialement chut√© suite au rapport sur l'emploi plus fort que pr√©vu. "
                "√Ä son point bas, le Dow avait chut√© de 198 points; il a grimp√© de plus de 500 points au plus fort du rallye. "
                "Le Nasdaq et le S&P 500 ont chut√© de 0,8 % lors de leurs points les plus bas de la journ√©e.  Les traders √©taient incertains "
                "des raisons de l'inversion intrajournali√®re. Certains ont not√© que cela pourrait √™tre d√ª au chiffre des salaires plus faibles "
                "dans le rapport sur l'emploi qui a amen√© les investisseurs √† reconsid√©rer leur position baissi√®re pr√©c√©dente. "
                "D'autres ont not√© le recul des rendements par rapport aux sommets de la journ√©e. Une partie du rallye pourrait simplement "
                "√™tre li√©e √† un march√© qui avait √©t√© extr√™mement survendu, le S&P 500 √©tant √† un moment donn√© de la semaine en baisse de plus de 9 % "
                "par rapport √† son sommet plus t√¥t cette ann√©e.  Les rendements ont initialement augment√© apr√®s le rapport, le taux des "
                "obligations du Tr√©sor √† 10 ans se n√©gociant pr√®s de son niveau le plus √©lev√© en 14 ans. Le taux de r√©f√©rence a ensuite diminu√© "
                "de ces niveaux, mais √©tait toujours en hausse d'environ 6 points de base √† 4,58 %.  'Nous voyons un peu de retour "
                "des rendements par rapport √† ce que nous √©tions autour de 4,8 %. [Avec] eux qui se replient un peu, je pense que cela "
                "aide le march√© boursier,' a d√©clar√© Margaret Jones, responsable des investissements chez Vibrant Industries "
                "Capital Advisors. 'Nous avons eu beaucoup de faiblesse sur le march√© ces derni√®res semaines, et potentiellement "
                "certaines conditions de survente.'"},

    {"query": "La marge brute attendue est-elle sup√©rieure √† 70 %?",
     "answer": "Oui, entre 71,5 % et 72%.",
     "context": "Pr√©visions La pr√©vision de NVIDIA pour le troisi√®me trimestre de l'exercice 2024 est la suivante :"
                "Les revenus devraient √™tre de 16,00 milliards de dollars, plus ou moins 2 %. Les marges brutes GAAP et non-GAAP "
                "sont attendues respectivement √† 71,5 % et 72,5 %, plus ou moins "
                "50 points de base.  Les d√©penses d'exploitation GAAP et non-GAAP devraient √™tre "
                "respectivement d'environ 2,95 milliards de dollars et 2,00 milliards de dollars.  Les autres revenus et d√©penses GAAP et "
                "non-GAAP devraient √™tre d'un revenu d'environ 100 millions de dollars, hors gains et pertes provenant d'investissements non affili√©s. "
                "Les taux d'imposition GAAP et non-GAAP devraient √™tre de 14,5 %, plus ou moins 1 %, excluant tout √©l√©ment distinct. "
                "Faits saillants NVIDIA a r√©alis√© des progr√®s depuis sa pr√©c√©dente annonce

 de b√©n√©fices, son action ayant grimp√© "
                "de 42 % depuis lors.  Les revenus de NVIDIA pour le deuxi√®me trimestre ont atteint 13,51 milliards de dollars, "
                "ce qui repr√©sente une augmentation de 101 % par rapport √† l'ann√©e pr√©c√©dente. "
                "NVIDIA a enregistr√© des b√©n√©fices net de 6,19 milliards de dollars, ou 2,48 $ par action. "
                "Les pr√©visions de l'entreprise indiquent une forte demande pour ses unit√©s de traitement graphique (GPU) utilis√©es dans l'IA. "
                "Les analystes s'attendent √† ce que l'IA et les services de cloud computing propulsent la croissance de l'entreprise √† l'avenir."},

    {"query": "Quel est le taux de croissance de l'√©conomie japonaise?",
     "answer": "2,5%",
     "context": "Le produit int√©rieur brut (PIB) du Japon a augment√© √† un rythme annualis√© de 2,5 % au cours du "
                "troisi√®me trimestre, d√©passant les attentes du march√©. "
                "Cette croissance a √©t√© aliment√©e par la consommation des m√©nages, qui a augment√© de 1,2 %."
                " Le Japon fait face √† des d√©fis, notamment une inflation √©lev√©e et des tensions g√©opolitiques croissantes. "
                "Les experts estiment que le pays devra continuer √† investir dans ses infrastructures et ses technologies "
                "pour soutenir cette croissance."},

    {"query": "Combien d'employ√©s de la soci√©t√© ont quitt√© leurs postes?",
     "answer": "200 employ√©s.",
     "context": "Au total, 200 employ√©s de l'entreprise XYZ ont quitt√© leurs postes au cours du dernier trimestre, "
                "ce qui repr√©sente une augmentation de 15 % par rapport au trimestre pr√©c√©dent. "
                "Les dirigeants de l'entreprise attribuent ce d√©part √† des changements dans la culture d'entreprise et "
                "√† l'augmentation de la concurrence dans le secteur."},

    {"query": "Quelle entreprise a annonc√© une augmentation de sa production?",
     "answer": "L'entreprise ABC.",
     "context": "L'entreprise ABC a annonc√© qu'elle augmenterait sa production de 20 % au cours du prochain trimestre "
                "en raison d'une forte demande pour ses produits. "
                "Cette d√©cision a √©t√© salu√©e par les investisseurs, qui voient cela comme un signe de la robustesse "
                "des performances de l'entreprise sur le march√©."}
    
    ]

Voici la version fran√ßaise du script que tu as fourni :

```python
# Ceci est le script principal √† ex√©cuter

def bling_meets_llmware_hello_world(model_name):

    t0 = time.time()

    # Charger les questions
    test_list = hello_world_questions()

    print(f"\n > Chargement du mod√®le : {model_name}...")

    # Charger le mod√®le 
    prompter = Prompt().load_model(model_name)

    t1 = time.time()
    print(f"\n > Temps de chargement du mod√®le {model_name} : {t1-t0} secondes")
 
    for i, entries in enumerate(test_list):

        print(f"\n{i+1}. Question : {entries['query']}")
     
        # Ex√©cuter le prompt
        output = prompter.prompt_main(entries["query"], context=entries["context"],
                                      prompt_name="default_with_context", temperature=0.30)

        # Afficher les r√©sultats
        llm_response = output["llm_response"].strip("\n")
        print(f"R√©ponse LLM : {llm_response}")
        print(f"R√©ponse correcte : {entries['answer']}")
        print(f"Utilisation LLM : {output['usage']}")

    t2 = time.time()

    print(f"\nTemps total de traitement : {t2-t1} secondes")

    return 0


if __name__ == "__main__":

    # Liste des petits mod√®les bling pr√™ts pour ordinateur portable 'rag-instruct' sur HuggingFace

    pytorch_models = ["llmware/bling-1b-0.1",                    #  le plus populaire
                      "llmware/bling-tiny-llama-v0",             #  le plus rapide 
                      "llmware/bling-1.4b-0.1",
                      "llmware/bling-falcon-1b-0.1",
                      "llmware/bling-cerebras-1.3b-0.1",
                      "llmware/bling-sheared-llama-1.3b-0.1",    
                      "llmware/bling-sheared-llama-2.7b-0.1",
                      "llmware/bling-red-pajamas-3b-0.1",
                      "llmware/bling-stable-lm-3b-4e1t-v0",
                      "llmware/bling-phi-3"                      # le plus pr√©cis (et le plus r√©cent)  
                      ]

    # Les versions GGUF quantifi√©es se chargent g√©n√©ralement plus rapidement et fonctionnent bien sur un ordinateur portable avec au moins 16 Go de RAM
    gguf_models = ["bling-phi-3-gguf", "bling-stablelm-3b-tool", "dragon-llama-answer-tool", "dragon-yi-answer-tool", "dragon-mistral-answer-tool"]

    # Essayer un mod√®le de la liste des mod√®les pytorch ou gguf
    # Le plus r√©cent (et le plus pr√©cis) est 'bling-phi-3-gguf'  

    bling_meets_llmware_hello_world(gguf_models[0])  

    # Consulter la fiche du mod√®le sur Huggingface pour les r√©sultats de performance du test de r√©f√©rence RAG et d'autres informations utiles
```
</details>

Voici la version fran√ßaise de ton texte :

<details>
<summary><b>Options de Base de Donn√©es Simples √† √âvoluer</b> - magasins de donn√©es int√©gr√©s de l'ordinateur portable au cluster parall√©lis√©.</summary>

```python
from llmware.configs import LLMWareConfig

#   pour d√©finir la base de donn√©es de collection - mongo, sqlite, postgres  
LLMWareConfig().set_active_db("mongo")  

#   pour d√©finir la base de donn√©es vectorielle (ou d√©clarer lors de l'installation)  
#   --options : milvus, pg_vector (postgres), redis, qdrant, faiss, pinecone, mongo atlas  
LLMWareConfig().set_vector_db("milvus")  

#   pour un d√©marrage rapide - aucune installation requise  
LLMWareConfig().set_active_db("sqlite")  
LLMWareConfig().set_vector_db("chromadb")   # essayer √©galement faiss et lancedb  

#   pour un d√©ploiement unique de postgres  
LLMWareConfig().set_active_db("postgres")  
LLMWareConfig().set_vector_db("postgres")  

#   pour installer mongo, milvus, postgres - voir les scripts docker-compose ainsi que les exemples
```

</details>

<details>

<summary> üî• <b> Agents avec Appels de Fonction et Mod√®les SLIM </b> üî• </summary>  

```python
from llmware.agents import LLMfx

text = ("L'action de Tesla a chut√© de 8 % lors des √©changes avant le march√© apr√®s avoir annonc√© un chiffre d'affaires et un b√©n√©fice du quatri√®me trimestre qui "
        "n'ont pas atteint les estimations des analystes. L'entreprise de v√©hicules √©lectriques a √©galement averti que la croissance du volume des v√©hicules en "
        "2024 'pourrait √™tre sensiblement inf√©rieure' √† celle de l'ann√©e derni√®re. Les revenus automobiles, quant √† eux, n'ont augment√© "
        "que de 1 % par rapport √† l'ann√©e pr√©c√©dente, en partie parce que les VE se vendaient √† un prix inf√©rieur √† celui d'avant. "
        "Tesla a mis en ≈ìuvre d'importantes r√©ductions de prix au cours de la seconde moiti√© de l'ann√©e dans le monde entier. Lors d'une pr√©sentation mercredi, "
        "l'entreprise a averti les investisseurs qu'elle √©tait 'actuellement entre deux vagues majeures de croissance.'")

#   cr√©er un agent en utilisant la classe LLMfx
agent = LLMfx()

#   charger le texte √† traiter
agent.load_work(text)

#   charger 'models' comme 'tools' √† utiliser dans le processus d'analyse
agent.load_tool("sentiment")
agent.load_tool("extract")
agent.load_tool("topics")
agent.load_tool("boolean")

#   ex√©cuter des appels de fonction en utilisant diff√©rents outils
agent.sentiment()
agent.topics()
agent.extract(params=["company"])
agent.extract(params=["automotive revenue growth"])
agent.xsum()
agent.boolean(params=["est-ce que la croissance de 2024 est pr√©vue pour √™tre forte ? (expliquer)"])

#   √† la fin du traitement, afficher le rapport qui a √©t√© automatiquement agr√©g√© par cl√©
report = agent.show_report()

#   afficher un r√©sum√© de l'activit√© dans le processus
activity_summary = agent.activity_summary()

#   liste des r√©ponses collect√©es
for i, entries in enumerate(agent.response_list):
    print("mise √† jour : analyse de la r√©ponse : ", i, entries)

output = {"report": report, "activity_summary": activity_summary, "journal": agent.journal}  
```

</details>


<details>

<summary> üöÄ <b>Commencer √† coder - D√©marrage rapide pour RAG</b> üöÄ </summary>

```python
# Cet exemple illustre une analyse simple de contrat
# utilisant un LLM optimis√© pour RAG fonctionnant localement

import os
import re
from llmware.prompts import Prompt, HumanInTheLoop
from llmware.setup import Setup
from llmware.configs import LLMWareConfig

def contract_analysis_on_laptop(model_name):

    #  Dans ce sc√©nario, nous allons :
    #  -- t√©l√©charger un ensemble d'√©chantillons de fichiers de contrat
    #  -- cr√©er un Prompt et charger un mod√®le LLM BLING
    #  -- analyser chaque contrat, extraire les passages pertinents et poser des questions √† un LLM local

    #  Boucle principale - It√©rer √† travers chaque contrat :
    #
    #      1.  analyser le document en m√©moire (convertir un fichier PDF en morceaux de texte avec des m√©tadonn√©es)
    #      2.  filtrer les morceaux de texte analys√©s avec un "sujet" (par exemple, "droit applicable") pour extraire les passages pertinents
    #      3.  emballer et assembler les morceaux de texte dans un contexte pr√™t pour le mod√®le
    #      4.  poser trois questions cl√©s pour chaque contrat au LLM
    #      5.  imprimer √† l'√©cran
    #      6.  sauvegarder les r√©sultats en json et csv pour un traitement et une r√©vision ult√©rieurs.

    #  Charger les fichiers d'exemple de llmware

    print(f"\n > Chargement des fichiers d'exemple de llmware...")

    sample_files_path = Setup().load_sample_files()
    contracts_path = os.path.join(sample_files_path, "Agreements")
 
    #  Liste de requ√™tes - voici les 3 sujets principaux et questions que nous aimerions que le LLM analyse pour chaque contrat

    query_list = {"contrat de travail ex√©cutif": "Quels sont les noms des deux parties ?",
                  "salaire de base": "Quel est le salaire de base de l'ex√©cutif ?",
                  "vacances": "Combien de jours de vacances l'ex√©cutif recevra-t-il ?"}

    #  Charger le mod√®le s√©lectionn√© par nom qui a √©t√© pass√© √† la fonction

    print(f"\n > Chargement du mod√®le {model_name}...")

    prompter = Prompt().load_model(model_name, temperature=0.0, sample=False)

    #  Boucle principale

    for i, contract in enumerate(os.listdir(contracts_path)):

        #   exclure l'artefact de fichier Mac (√©nervant, mais fait de la vie dans les d√©mos)
        if contract != ".DS_Store":

            print("\nAnalyse du contrat : ", str(i + 1), contract)

            print("R√©ponses du LLM :")

            for key, value in query_list.items():

                # √©tape 1 + 2 + 3 ci-dessus - le contrat est analys√©, d√©coup√© en morceaux de texte, filtr√© par cl√© de sujet,
                # ... puis emball√© dans le prompt

                source = prompter.add_source_document(contracts_path, contract, query=key)

                # √©tape 4 ci-dessus - appel au LLM avec les informations 'source' d√©j√† emball√©es dans le prompt

                responses = prompter.prompt_with_source(value, prompt_name="default_with_context")  

                # √©tape 5 ci-dessus - imprimer √† l'√©cran

                for r, response in enumerate(responses):
                    print(key, ":", re.sub("[\n]", " ", response["llm_response"]).strip())

                # Nous avons termin√© avec ce contrat, vider la source du prompt
                prompter.clear_source_materials()

    # √©tape 6 ci-dessus - sauvegarde de l'analyse en jsonl et csv

    # Sauvegarder le rapport jsonl dans jsonl dans le dossier /prompt_history
    print("\n√âtat du prompt sauvegard√© √† : ", os.path.join(LLMWareConfig.get_prompt_path(), prompter.prompt_id))
    prompter.save_state()

    # Sauvegarder le rapport csv qui inclut le mod√®le, la r√©ponse, le prompt et les preuves pour r√©vision humaine
    csv_output = HumanInTheLoop(prompter).export_current_interaction_to_csv()
    print("sortie csv sauvegard√©e √† :  ", csv_output)


if __name__ == "__main__":

    # utiliser un mod√®le CPU local - essayer le plus r√©cent - RAG finetune de Phi-3 quantifi√© et emball√© en GGUF  
    model = "bling-phi-3-gguf"

    contract_analysis_on_laptop(model)
```
</details>


## üî• Quoi de neuf ? üî•  

- **√âvaluation des capacit√©s des petits mod√®les** - voir [r√©sultats de benchmark](https://medium.com/@darrenoberst/best-small-language-models-for-accuracy-and-enterprise-use-cases-benchmark-results-cf71964759c8) et [exemple_de_classement_de_mod√®le](fast_start/agents/agents-15-get_model_benchmarks.py)  

- **Utilisation des mod√®les Qwen2 pour RAG, appel de fonction et chat** - commencez en quelques minutes - voir [exemple](https://github.com/llmware-ai/llmware/tree/main/examples/Models/using-qwen2-models.py)  

- **Nouveaux mod√®les d'appel de fonction Phi-3** - commencez en quelques minutes - voir [exemple](https://github.com/llmware-ai/llmware/tree/main/examples/Models/using-phi-3-function-calls.py)  

- **BizBot - RAG + Chatbot SQL Local** - voir [exemple](https://github.com/llmware-ai/llmware/tree/main/examples/Use_Cases/biz_bot.py) et [vid√©o](https://youtu.be/4nBYDEjxxTE?si=o6PDPbu0PVcT-tYd)  

- **Cas d'utilisation de l'outil de conf√©rence - posez des questions √† un enregistrement audio** - voir [outil_de_conf√©rence](https://github.com/llmware-ai/llmware/blob/main/examples/Use_Cases/lecture_tool/)   

- **Services Web avec appels d'agents pour la recherche financi√®re** - sc√©nario de bout en bout - [vid√©o](https://youtu.be/l0jzsg1_Ik0?si=hmLhpT1iv_rxpkHo) et [exemple](examples/Use_Cases/web_services_slim_fx.py)  

- **Transcription vocale avec WhisperCPP** - [premiers_pas](examples/Models/using-whisper-cpp-getting-started.py), [utilisation_des_fichiers_exemples](examples/Models/using-whisper-cpp-sample-files.py), et [cas_d'utilisation_d'analyse](examples/Use_Cases/parsing_great_speeches.py) avec [vid√©o_de_grandes_discours](https://youtu.be/5y0ez5ZBpPE?si=KVxsXXtX5TzvlEws)    

- **Chatbot local Phi-3 GGUF en streaming avec interface utilisateur** - configurez votre propre chatbot Phi-3-gguf sur votre ordinateur en quelques minutes - [exemple](examples/UI/gguf_streaming_chatbot.py) avec [vid√©o](https://youtu.be/gzzEVK8p3VM?si=8cNn_do0oxSzCEnM)  

- **Exemple de requ√™te en langage naturel vers CSV de bout en bout** - utilisant le mod√®le slim-sql - [vid√©o](https://youtu.be/z48z5XOXJJg?si=V-CX1w-7KRioI4Bi) et [exemple](examples/SLIM-Agents/text2sql-end-to-end-2.py) et maintenant en utilisant des tables personnalis√©es sur Postgres [exemple](https://github.com/llmware-ai/llmware/tree/main/examples/Use_Cases/agent_with_custom_tables.py)  

- **Agents multi-mod√®les avec mod√®les SLIM** - agents √† √©tapes multiples avec SLIM sur CPU - [vid√©o](https://www.youtube.com/watch?v=cQfdaTcmBpY) - [exemple](examples/SLIM-Agents)  

- **Exemple d'images de documents int√©gr√©s OCR** - extraire syst√©matiquement du texte √† partir d'images int√©gr√©es dans des documents [exemple](examples/Parsing/ocr_embedded_doc_images.py)   

- **Fonctions d'analyseur am√©lior√©es pour PDF, Word, Powerpoint et Excel** - nouveaux contr√¥les et strat√©gies de d√©coupage de texte, extraction de tableaux, images, texte d'en-t√™te - [exemple](examples/Parsing/pdf_parser_new_configs.py)   

- **Serveur d'inf√©rence d'agent** - configurer des agents multi-mod√®les sur le serveur d'inf√©rence [exemple](https://github.com/llmware-ai/llmware/tree/main/examples/SLIM-Agents/agent_api_endpoint.py)  

- **Optimisation de la pr√©cision des prompts RAG** - consultez [exemple](examples/Models/adjusting_sampling_settings.py) et vid√©os - [partie I](https://youtu.be/7oMTGhSKuNY?si=14mS2pftk7NoKQbC) et [partie II](https://youtu.be/a7ErHxdRo8E?si=UJzdMoRXnHdkaA84)  

- **Nouveau sc√©nario de conversation sur la durabilit√©** - ajustements pour un d√©veloppement bas√© sur des prompts - [exemple](https://github.com/llmware-ai/llmware/tree/main/examples/Use_Cases/sustainability_conversation.py)  

- **Nouveau cadre et exemple de chatbot local int√©gr√© pour la recherche sur les produits** - consultez [exemple](examples/Use_Cases/llm_research_chatbot.py) 

- **Pour une d√©monstration visuelle des mises √† jour r√©centes, regardez notre [vid√©o](https://www.youtube.com/watch?v=yCR0z2x5p9g)**.



## üå± Commencer

**√âtape 1 - Installer llmware** -  `pip3 install llmware` ou `pip3 install 'llmware[full]'`  

- Remarque : √† partir de la version v0.3.0, nous proposons des options pour une [installation de base](https://github.com/llmware-ai/llmware/blob/main/llmware/requirements.txt) (ensemble minimal de d√©pendances) ou une [installation compl√®te](https://github.com/llmware-ai/llmware/blob/main/llmware/requirements_extras.txt) (ajoute au c≈ìur un ensemble plus large de biblioth√®ques Python connexes).

<details>
<summary><b>√âtape 2 - Acc√©der aux Exemples</b> - Commencez rapidement avec plus de 100 recettes 'Copier-Coller'</summary>

## üî• Nouveaux Exemples Principaux üî•  

Sc√©nario de bout en bout - [**Appels de Fonction avec SLIM Extract et Services Web pour la Recherche Financi√®re**](https://github.com/llmware-ai/llmware/tree/main/examples/Use_Cases/web_services_slim_fx.py)  
Analyse des Fichiers Vocaux - [**Grandes Discours avec Requ√™te et Extraction LLM**](https://github.com/llmware-ai/llmware/tree/main/examples/Use_Cases/parsing_great_speeches.py)  
Nouveau sur LLMWare - [**S√©rie de tutoriels Fast Start**](https://github.com/llmware-ai/llmware/tree/main/fast_start)  
Configuration - [**Premiers Pas**](https://github.com/llmware-ai/llmware/tree/main/examples/Getting_Started)  
Exemples SLIM -  [**Mod√®les SLIM**](examples/SLIM-Agents/)  

| Exemple     |  D√©tails      |
|-------------|--------------|
| 1.   Mod√®les BLING fast start ([code](examples/Models/bling_fast_start.py) / [vid√©o](https://www.youtube.com/watch?v=JjgqOZ2v5oU)) | Commencez avec des mod√®les rapides, pr√©cis, bas√©s sur CPU - questions-r√©ponses, extraction cl√©-valeur et r√©sum√© de base.  |
| 2.   Analyser et Int√©grer 500 Documents PDF ([code](examples/Embedding/docs2vecs_with_milvus-un_resolutions.py))  | Exemple de bout en bout pour l'Analyse, l'Int√©gration et la Consultation des documents de R√©solution de l'ONU avec Milvus  |
| 3.  R√©cup√©ration Hybride - S√©mantique + Texte ([code](examples/Retrieval/dual_pass_with_custom_filter.py)) | Utiliser la r√©cup√©ration 'dual pass' pour combiner le meilleur de la recherche s√©mantique et textuelle |  
| 4.   Multiples Int√©grations avec PG Vector ([code](examples/Embedding/using_multiple_embeddings.py) / [vid√©o](https://www.youtube.com/watch?v=Bncvggy6m5Q)) | Comparaison de plusieurs Mod√®les d'Int√©gration utilisant Postgres / PG Vector |
| 5.   Mod√®les DRAGON GGUF ([code](examples/Models/dragon_gguf_fast_start.py) / [vid√©o](https://www.youtube.com/watch?v=BI1RlaIJcsc&t=130s)) | Mod√®les GGUF RAG de pointe 7B.  | 
| 6.   RAG avec BLING ([code](examples/Use_Cases/contract_analysis_on_laptop_with_bling_models.py) / [vid√©o](https://www.youtube.com/watch?v=8aV5p3tErP0)) | Utilisant l'analyse de contrats comme exemple, exp√©rimentez avec RAG pour une analyse complexe de documents et extraction de texte en utilisant le mod√®le GPT BLING d'environ 1B param√®tres fonctionnant sur votre ordinateur portable. |  
| 7.   Analyse de l'Accord de Service Principal avec DRAGON ([code](examples/Use_Cases/msa_processing.py) / [vid√©o](https://www.youtube.com/watch?v=Cf-07GBZT68&t=2s)) | Analyse des MSAs en utilisant le Mod√®le YI 6B de DRAGON.   |                                                                                                                          |
| 8.   Exemple Streamlit ([code](examples/UI/simple_rag_ui_with_streamlit.py))  | Posez des questions sur les factures avec l'UI ex√©cutant l'inf√©rence.  |  
| 9.   Int√©gration de LM Studio ([code](examples/Models/using-open-chat-models.py) / [vid√©o](https://www.youtube.com/watch?v=h2FDjUyvsKE&t=101s)) | Int√©gration des Mod√®les LM Studio avec LLMWare  |                                                                                                                                        |
| 10.  Prompts Avec Sources ([code](examples/Prompts/prompt_with_sources.py))  | Attachez une large gamme de sources de connaissances directement dans les Prompts.   |   
| 11.  V√©rification des Faits ([code](examples/Prompts/fact_checking.py))  | Explorez l'ensemble complet des m√©thodes de preuve dans ce script exemple qui analyse un ensemble de contrats.   |
| 12.  Utiliser des Mod√®les de Chat GGUF 7B ([code](examples/Models/chat_models_gguf_fast_start.py)) | Utiliser 4 mod√®les de chat de pointe 7B en quelques minutes fonctionnant localement |  

Consultez :  [exemples llmware](https://github.com/llmware-ai/llmware/blob/main/examples/README.md)  

</details>  

<details>
<summary><b>√âtape 3 - Vid√©os Tutoriels</b> - consultez notre cha√Æne Youtube pour des tutoriels percutants de 5 √† 10 minutes sur les derniers exemples.   </summary>

üé¨ Consultez ces vid√©os pour commencer rapidement :  
- [R√©sum√© de Document](https://youtu.be/Ps3W-P9A1m8?si=Rxvst3RJv8ZaOk0L)  
- [Bling-3-GGUF Chatbot Local](https://youtu.be/gzzEVK8p3VM?si=8cNn_do0oxSzCEnM)  
- [Analyse de Recherche Complexe Bas√©e sur des Agents](https://youtu.be/y4WvwHqRR60?si=jX3KCrKcYkM95boe)  
- [Commencer avec les SLIMs (avec code)](https://youtu.be/aWZFrTDmMPc?si=lmo98_quo_2Hrq0C)  
- [Prompting Incorrect pour RAG - √âchantillonnage Stochastique - Partie I](https://youtu.be/7oMTGhSKuNY?si=_KSjuBnqArvWzYbx)  
- [Prompting Incorrect pour RAG - √âchantillonnage Stochastique - Partie II - Exp√©riences de Code](https://youtu.be/iXp1tj-pPjM?si=3ZeMgipY0vJDHIMY)  
- [Introduction aux Mod√®les SLIM](https://www.youtube.com/watch?v=cQfdaTcmBpY)  
- [Introduction √† Text2SQL](https://youtu.be/BKZ6kO2XxNo?si=tXGt63pvrp_rOlIP)  
- [RAG avec BLING sur votre ordinateur portable](https://www.youtube.com/watch?v=JjgqOZ2v5oU)    
- [Mod√®les DRAGON-7B](https://www.youtube.com/watch?v=d_u7VaKu6Qk&t=37s)  
- [Installer et Comparer Plusieurs Int√©grations avec Postgres et PGVector](https://www.youtube.com/watch?v=Bncvggy6m5Q)  
- [Contexte sur la Quantification GGUF & Exemple de Mod√®le DRAGON](https://www.youtube.com/watch?v=ZJyQIZNJ45E)  
- [Utiliser les Mod√®les de LM Studio](https://www.youtube.com/watch?v=h2FDjUyvsKE)  
- [Utiliser les Mod√®les Ollama](https://www.youtube.com/watch?v=qITahpVDuV0)  
- [Utiliser n'importe quel Mod√®le GGUF](https://www.youtube.com/watch?v=9wXJgld7Yow)  
- [Utiliser de petits LLM pour RAG pour l'Analyse de Contrat (avec LLMWare)](https://www.youtube.com/watch?v=8aV5p3tErP0)
- [Traitement des Factures avec LLMware](https://www.youtube.com/watch?v=VHZSaBBG-Bo&t=10s)
- [Ingestion de PDF √† Grande √âchelle](https://www.youtube.com/watch?v=O0adUfrrxi8&t=10s)
- [√âvaluer les LLM pour RAG avec LLMWare](https://www.youtube.com/watch?v=s0KWqYg5Buk&t=105s)
- [D√©marrage Rapide avec la Biblioth√®que Open Source LLMWare](https://www.youtube.com/watch?v=0naqpH93eEU)
- [Utiliser la G√©n√©ration Augment√©e par R√©cup√©ration (RAG) sans Base de Donn√©es](https://www.youtube.com/watch?v

=y4WvwHqRR60)
- [R√©cup√©ration Multi-Modalit√© avec LLMware](https://www.youtube.com/watch?v=2zw8ByzEo1I)
- [Construire un Agent RAG en utilisant des Mod√®les SLIM](https://www.youtube.com/watch?v=6wIg7F5-kFg)

</details>



## Options de Stockage de Donn√©es

<details>
<summary><b>D√©but Rapide</b> : utilisez SQLite3 et ChromaDB (bas√© sur des fichiers) pr√™t √† l'emploi - aucune installation requise </summary>  

```python
from llmware.configs import LLMWareConfig 
LLMWareConfig().set_active_db("sqlite")   
LLMWareConfig().set_vector_db("chromadb")  
```
</details>  

<details>
<summary><b>Vitesse + √âchelle</b> : utilisez MongoDB (collection de texte) et Milvus (base de donn√©es vectorielle) - installez avec Docker Compose </summary> 

```bash
curl -o docker-compose.yaml https://raw.githubusercontent.com/llmware-ai/llmware/main/docker-compose.yaml
docker compose up -d
```

```python
from llmware.configs import LLMWareConfig
LLMWareConfig().set_active_db("mongo")
LLMWareConfig().set_vector_db("milvus")
```

</details>  

<details>
<summary><b>Postgres</b> : utilisez Postgres pour la collection de texte et la base de donn√©es vectorielle - installez avec Docker Compose </summary> 

```bash
curl -o docker-compose.yaml https://raw.githubusercontent.com/llmware-ai/llmware/main/docker-compose-pgvector.yaml
docker compose up -d
```

```python
from llmware.configs import LLMWareConfig
LLMWareConfig().set_active_db("postgres")
LLMWareConfig().set_vector_db("postgres")
```

</details>  

<details>
<summary><b>M√©lange et Association</b> : LLMWare prend en charge 3 bases de donn√©es de collection de texte (Mongo, Postgres, SQLite) et 
10 bases de donn√©es vectorielles (Milvus, PGVector-Postgres, Neo4j, Redis, Mongo-Atlas, Qdrant, Faiss, LanceDB, ChromaDB et Pinecone)  </summary>

```bash
# scripts pour d√©ployer d'autres options
curl -o docker-compose.yaml https://raw.githubusercontent.com/llmware-ai/llmware/main/docker-compose-redis-stack.yaml
```

</details>  

## Rencontrez nos Mod√®les    

- **S√©rie de mod√®les SLIM :** petits mod√®les sp√©cialis√©s ajust√©s pour les appels de fonction et les workflows d'agents multi-√©tapes et multi-mod√®les.  
- **S√©rie de mod√®les DRAGON :** mod√®les optimis√©s RAG de qualit√© production de 6 √† 9 milliards de param√®tres - "Delivering RAG on ..." les mod√®les de base leaders.  
- **S√©rie de mod√®les BLING :** mod√®les optimis√©s RAG, bas√©s sur CPU, suivant les instructions de 1 √† 5 milliards de param√®tres.  
- **Mod√®les Industry BERT :** mod√®les d'embedding personnalis√©s form√©s par d√©faut, affin√©s pour les industries suivantes : assurance, contrats, gestion d'actifs, SEC.  
- **Quantification GGUF :** nous fournissons des versions 'gguf' et 'tool' de nombreux mod√®les SLIM, DRAGON et BLING, optimis√©s pour le d√©ploiement sur CPU.  

## Utilisation des LLM et Configuration des Cl√©s API et Secrets

LLMWare est une plateforme ouverte et prend en charge une large gamme de mod√®les open source et propri√©taires. Pour utiliser LLMWare, vous n'avez pas besoin d'utiliser un LLM propri√©taire - nous vous encourageons √† exp√©rimenter avec [SLIM](https://www.huggingface.co/llmware/), [BLING](https://huggingface.co/llmware), [DRAGON](https://huggingface.co/llmware), [Industry-BERT](https://huggingface.co/llmware), les exemples GGUF, tout en int√©grant vos mod√®les pr√©f√©r√©s de HuggingFace et Sentence Transformers. 

Si vous souhaitez utiliser un mod√®le propri√©taire, vous devrez fournir vos propres cl√©s API. Les cl√©s API et les secrets pour les mod√®les, AWS et Pinecone peuvent √™tre configur√©s pour une utilisation dans des variables d'environnement ou pass√©s directement aux appels de m√©thode.  

<details>  
<summary> ‚ú®  <b>Feuille de Route - O√π allons-nous ... </b>  </summary>

- üí° Faciliter le d√©ploiement de mod√®les open source finement ajust√©s pour construire des workflows RAG √† la pointe de la technologie  
- üí° Cloud priv√© - garder les documents, les pipelines de donn√©es, les magasins de donn√©es et les mod√®les s√ªrs et s√©curis√©s  
- üí° Quantification des mod√®les, en particulier GGUF, et d√©mocratisation de l'utilisation r√©volutionnaire des LLM bas√©s sur CPU de 1 √† 9 milliards  
- üí° D√©velopper de petits LLM sp√©cialis√©s optimis√©s pour RAG entre 1 et 9 milliards de param√®tres  
- üí° LLM sp√©cifiques √† l'industrie, mod√®les d'embedding et processus pour soutenir des cas d'utilisation bas√©s sur la connaissance cl√©s  
- üí° √âvolutivit√© pour les entreprises - conteneurisation, d√©ploiements de travailleurs et Kubernetes  
- üí° Int√©gration de SQL et d'autres sources de donn√©es d'entreprise √† grande √©chelle  
- üí° Workflows bas√©s sur des agents multi-√©tapes et multi-mod√®les avec de petits mod√®les sp√©cialis√©s appelant des fonctions  

Comme nos mod√®les, nous aspirons √† ce que llmware soit "petit, mais puissant" - facile √† utiliser et √† d√©marrer, mais avec un impact puissant !  

</details>

Int√©ress√© √† contribuer √† llmware ? Les informations sur les fa√ßons de participer se trouvent dans notre [Guide des Contributeurs](https://github.com/llmware-ai/llmware/blob/main/repo_docs/CONTRIBUTING.md#contributing-to-llmware). Comme pour tous les aspects de ce projet, les contributions sont r√©gies par notre [Code de Conduite](https://github.com/llmware-ai/llmware/blob/main/repo_docs/CODE_OF_CONDUCT.md).

Questions et discussions sont les bienvenues dans nos [discussions GitHub](https://github.com/llmware-ai/llmware/discussions). 

Sure! Here‚Äôs the translation of the release notes and change log into French:

## üì£ Notes de version et journal des modifications

Voir aussi [notes de d√©ploiement/install suppl√©mentaires dans wheel_archives](https://github.com/llmware-ai/llmware/tree/main/wheel_archives)

**Dimanche 6 octobre - v0.3.7**  
- Ajout d'une nouvelle classe de mod√®le - OVGenerativeModel - pour prendre en charge l'utilisation de mod√®les empaquet√©s au format OpenVino  
- Ajout d'une nouvelle classe de mod√®le - ONNXGenerativeModel - pour prendre en charge l'utilisation de mod√®les empaquet√©s au format ONNX  
- Premiers pas avec [l'exemple OpenVino](https://github.com/llmware-ai/llmware/blob/main/examples/Models/using_openvino_models.py)  
- Premiers pas avec [l'exemple ONNX](https://github.com/llmware-ai/llmware/blob/main/examples/Models/using_onnx_models.py)  

**Mardi 1er octobre - v0.3.6**  
- Ajout de nouveaux mod√®les de chat pour les invites  
- Am√©lioration et mise √† jour des configurations de mod√®le  
- Nouvelles fonctions utilitaires pour localiser et mettre en surbrillance les correspondances de texte dans les r√©sultats de recherche  
- Am√©lioration des fonctions utilitaires de v√©rification de hachage  

**Lundi 26 ao√ªt - v0.3.5**  
- Ajout de 10 nouveaux mod√®les BLING+SLIM au catalogue de mod√®les - avec Qwen2, Phi-3 et Phi-3.5  
- Lancement de nouveaux mod√®les DRAGON sur Qwen-7B, Yi-9B, Mistral-v0.3 et Llama-3.1  
- Nouveaux mod√®les Qwen2 (et RAG + r√©glages d'appel de fonction) - [using-qwen2-models](https://github.com/llmware-ai/llmware/blob/main/examples/Models/using-qwen2-models.py)  
- Nouveaux mod√®les d'appel de fonction Phi-3 - [using-phi-3-function-calls](https://github.com/llmware-ai/llmware/blob/main/examples/Models/using-phi-3-function-calls.py)  
- Nouvel exemple de cas d'utilisation - [lecture_tool](https://github.com/llmware-ai/llmware/blob/main/examples/Use_Cases/lecture_tool/)  
- Am√©lioration des configurations GGUF pour √©tendre la fen√™tre de contexte  
- Ajout de donn√©es de performance de r√©f√©rence de mod√®le aux configurations de mod√®le  
- Am√©lioration des fonctions de hachage des utilitaires  

Pour l'historique complet des notes de version, veuillez ouvrir l'onglet journal des modifications.

**Syst√®mes d'exploitation pris en charge** : MacOS (Metal - M1/M2/M3), Linux (x86) et Windows  
- Linux - support d'Ubuntu 20+ (glibc 2.31+)  
- si vous avez besoin d'une autre version de Linux, veuillez signaler un probl√®me - nous donnerons la priorit√© aux tests et assurerons le support.  

**Bases de donn√©es vectorielles prises en charge** : Milvus, Postgres (PGVector), Neo4j, Redis, LanceDB, ChromaDB, Qdrant, FAISS, Pinecone, Mongo Atlas Vector Search

**Bases de donn√©es d'index textuel prises en charge** : MongoDB, Postgres, SQLite

<details>
<summary><b>Optionnel</b></summary>

- [Docker](https://docs.docker.com/get-docker/)
  
- Pour activer les capacit√©s de parsing OCR, installez les packages natifs [Tesseract v5.3.3](https://tesseract-ocr.github.io/tessdoc/Installation.html) et [Poppler v23.10.0](https://poppler.freedesktop.org/).

</details>

<details>
<summary><b>üöß Journal des modifications</b></summary>

**Lundi 29 juillet - v03.4**  
- Am√©lioration des protections de s√©curit√© pour les lectures de texte2sql db pour les agents LLMfx  
- Nouveaux exemples - voir [exemple](https://github.com/llmware-ai/llmware/blob/main/examples/UI/dueling_chatbot.py)  
- Plus d'exemples de Notebook - voir [exemples de notebook](https://github.com/llmware-ai/llmware/blob/main/examples/Notebooks)  

**Lundi 8 juillet - v03.3**  
- Am√©liorations des options de configuration des mod√®les, de journalisation et divers petits correctifs  
- Am√©lioration des configurations Azure OpenAI - voir [exemple](https://github.com/llmware-ai/llmware/blob/main/examples/Models/using-azure-openai.py)  

**Samedi 29 juin - v0.3.2**  
- Mise √† jour des parseurs PDF et Office - am√©liorations des configurations dans les options de journalisation et de d√©coupage de texte  

**Samedi 22 juin - v0.3.1**  
- Ajout du module 3 √† la s√©rie d'exemples Fast Start [exemples 7-9 sur Agents & Appels de fonction](https://github.com/llmware-ai/llmware/tree/main/fast_start)  
- Ajout du mod√®le de rerank Jina pour la similarit√© s√©mantique en m√©moire RAG - voir [exemple](https://github.com/llmware-ai/llmware/tree/main/examples/Embedding/using_semantic_reranker_with_rag.py)  
- Am√©lioration de la param√©trisation de r√©cup√©ration des mod√®les dans le processus de chargement des mod√®les  
- Ajout de nouvelles versions 'tiny' de slim-extract et slim-summary dans les versions Pytorch et GGUF - consultez 'slim-extract-tiny-tool' et 'slim-summary-tiny-tool'  
- Cas d'utilisation [Biz Bot] - voir [exemple](https://github.com/llmware-ai/llmware/tree/main/examples/Use_Cases/biz_bot.py) et [vid√©o](https://youtu.be/4nBYDEjxxTE?si=o6PDPbu0PVcT-tYd)  
- Mise √† jour des exigences numpy <2 et mise √† jour de la version minimum de yfinance (>=0.2.38)  



**Mardi 4 juin - v0.3.0**  
- Ajout de la prise en charge de la nouvelle base de donn√©es int√©gr√©e 'no-install' Milvus Lite - voir [exemple](https://github.com/llmware-ai/llmware/tree/main/examples/Embedding/using_milvus_lite.py).  
- Ajout de deux nouveaux mod√®les SLIM au catalogue et aux processus d'agent - ['q-gen'](https://github.com/llmware-ai/llmware/tree/main/examples/SLIM-Agents/using-slim-q-gen.py) et ['qa-gen'](https://github.com/llmware-ai/llmware/tree/main/examples/SLIM-Agents/using-slim-qa-gen.py)  
- Mise √† jour de l'instanciation de la classe de mod√®le pour fournir plus d'extensibilit√© afin d'ajouter de nouvelles classes dans diff√©rents modules  
- Nouveaux scripts d'installation rapide welcome_to_llmware.sh et welcome_to_llmware_windows.sh  
- Am√©lioration de la classe de mod√®le de base avec de nouvelles m√©thodes post_init et register configurables  
- Cr√©ation de InferenceHistory pour suivre l'√©tat global de toutes les inf√©rences compl√©t√©es  
- Am√©liorations et mises √† jour multiples des journaux au niveau du module  
- Remarque : √† partir de v0.3.0, l'installation avec pip fournit deux options - une installation minimale de base `pip3 install llmware` qui prendra en charge la plupart des cas d'utilisation, et une installation plus large `pip3 install 'llmware[full]'` avec d'autres biblioth√®ques couramment utilis√©es.  

**Mercredi 22 mai - v0.2.15**  
- Am√©liorations dans la gestion des d√©pendances Pytorch et Transformers par la classe de mod√®le (chargement juste √† temps, si n√©cessaire)  
- Expansion des options de point de terminaison de l'API et de la fonctionnalit√© du serveur d'inf√©rence - voir les nouvelles [options d'acc√®s client](https://github.com/llmware-ai/llmware/tree/main/examples/Use_Cases/llmware_inference_api_client.py) et [server_launch](https://github.com/llmware-ai/llmware/tree/main/examples/Use_Cases/llmware_inference_server.py)  

**Samedi 18 mai - v0.2.14**  
- Nouvelles m√©thodes de parsing d'images OCR avec [exemple](https://github.com/llmware-ai/llmware/tree/main/examples/Use_Cases/slicing_and_dicing_office_docs.py)  
- Ajout de la premi√®re partie des am√©liorations de journalisation (WIP) dans les Configs et les Mod√®les.  
- Nouveau mod√®le d'embedding ajout√© au catalogue - industry-bert-loans.  
- Mises √† jour des m√©thodes d'importation de mod√®le et des configurations.  

**Dimanche 12 mai - v0.2.13**  
- Nouvelle m√©thode de streaming GGUF avec [exemple de base](https://github.com/llmware-ai/llmware/tree/main/examples/Models/gguf_streaming.py) et [chatbot local phi3](https://github.com/llmware-ai/llmware/tree/main/examples/UI/gguf_streaming_chatbot.py)  
- Nettoyages significatifs dans les imports auxiliaires et les d√©pendances pour r√©duire la complexit√© de l'installation - note : les fichiers requirements.txt et setup.py mis √† jour.  
- Code d√©fensif pour fournir un avertissement informatif sur toute d√©pendance manquante dans les parties sp√©cialis√©es du code, par exemple, OCR, Web Parser.  
- Mises √† jour des tests, des avis et de la documentation.  
- OpenAIConfigs cr√©√©s pour prendre en charge Azure OpenAI.  



**Dimanche 5 mai - Mise √† jour v0.2.12**  
- Lancement de ["bling-phi-3"](https://huggingface.co/llmware/bling-phi-3) et ["bling-phi-3-gguf"](https://huggingface.co/llmware/bling-phi-3-gguf) dans le ModelCatalog - le mod√®le BLING/DRAGON le plus r√©cent et le plus pr√©cis  
- Nouvelle m√©thode de r√©sum√© de documents longs utilisant slim-summary-tool [exemple](https://github.com/llmware-ai/llmware/tree/main/examples/Prompts/document_summarizer.py)  
- Nouveaux fichiers d'exemple Office (PowerPoint, Word, Excel) [exemple](https://github.com/llmware-ai/llmware/tree/main/examples/Parsing/parsing_microsoft_ir_docs.py)  
- Ajout de la prise en charge de Python 3.12  
- D√©pr√©ciation de faiss et remplacement par 'no-install' chromadb dans les exemples Fast Start  
- R√©organisation des classes Datasets, Graph et Web Services  
- Mise √† jour du parsing de la voix avec WhisperCPP dans la biblioth√®que  

**Lundi 29 avril - Mise √† jour v0.2.11**  
- Mises √† jour des biblioth√®ques gguf pour Phi-3 et Llama-3  
- Ajout de Phi-3 [exemple](https://github.com/llmware-ai/llmware/tree/main/examples/Models/using-microsoft-phi-3.py) et Llama-3 [exemple](https://github.com/llmware-ai/llmware/tree/main/examples/Models/using-llama-3.py) et versions quantifi√©es au Model Catalog  
- Int√©gration de la classe de mod√®le WhisperCPP et des biblioth√®ques partag√©es pr√©construites - [exemple de d√©marrage](https://github.com/llmware-ai/llmware/tree/main/examples/Models/using-whisper-cpp-getting-started.py)  
- Nouveaux fichiers d'exemple de voix pour les tests - [exemple](https://github.com/llmware-ai/llmware/tree/main/examples/Models/using-whisper-cpp-sample-files.py)  
- Am√©lioration de la d√©tection CUDA sur Windows et des v√©rifications de s√©curit√© pour les anciennes versions de Mac OS  

**Lundi 22 avril - Mise √† jour v0.2.10**  
- Mises √† jour de la classe Agent pour prendre en charge les requ√™tes en langage naturel sur des tables personnalis√©es dans Postgres [exemple](https://github.com/llmware-ai/llmware/tree/main/examples/Use_Cases/agent_with_custom_tables.py)  
- Nouveau point de terminaison API Agent mis en ≈ìuvre avec le serveur d'inf√©rence LLMWare et nouvelles capacit√©s de l'agent [exemple](https://github.com/llmware-ai/llmware/tree/main/examples/SLIM-Agents/agent_api_endpoint.py)  

**Mardi 16 avril - Mise √† jour v0.2.9**  
- Nouvelle classe CustomTable pour cr√©er rapidement des tables de base de donn√©es personnalis√©es en conjonction avec des flux de travail bas√©s sur LLM.  
- M√©thodes am√©lior√©es pour convertir des fichiers CSV et JSON/JSONL en tables de base de donn√©es.  
- Voir les nouveaux exemples [Exemple de cr√©ation de table personnalis√©e](https://github.com/llmware-ai/llmware/tree/main/examples/Structured_Tables/create_custom_table-1.py)  

**Mardi 9 avril - Mise √† jour v0.2.8**  
- Analyseur Office (Word Docx, PowerPoint PPTX et Excel XLSX) - plusieurs am√©liorations - nouvelles biblioth√®ques + m√©thode Python.  
- Comprend : plusieurs corrections, contr√¥le am√©lior√© du d√©coupage de texte, extraction de texte d'en-t√™te et options de configuration.  
- En g√©n√©ral, les nouvelles options d'analyseur Office sont conformes aux nouvelles options d'analyseur PDF.  
- Veuillez consulter [Exemple de configurations d'analyse de bureau](https://github.com/llmware-ai/llmware/tree/main/examples/Parsing/office_parser_new_configs.py)  

**Mercredi 3 avril - Mise √† jour v0.2.7**  
- Analyseur PDF - plusieurs am√©liorations - nouvelles biblioth√®ques + m√©thodes Python.  
- Comprend : encodage UTF-8 pour les langues europ√©ennes.  
- Comprend : meilleur contr√¥le du d√©coupage de texte, extraction de texte d'en-t√™te et options de configuration.  
- Veuillez consulter [Exemple de configurations d'analyse PDF](https://github.com/llmware-ai/llmware/tree/main/examples/Parsing/pdf_parser_new_configs.py) pour plus de d√©tails.  
- Remarque : d√©pr√©ciation de la prise en charge de aarch64-linux (utilisera les analyseurs 0.2.6). Prise en charge compl√®te √† l'avenir pour Linux Ubuntu20+ sur x86_64 + avec CUDA.  


**Vendredi 22 mars - Mise √† jour v0.2.6**  
- Nouveaux mod√®les SLIM : r√©sum√©, extraction, xsum, bool√©en, tags-3b et combo sentiment-ner.  
- Nouvelles analyses de logit et d'√©chantillonnage.  
- Nouveaux exemples SLIM montrant comment utiliser les nouveaux mod√®les.  

**Jeudi 14 mars - Mise √† jour v0.2.5**  
- Am√©lioration de la prise en charge de GGUF sur CUDA (Windows et Linux), avec de nouveaux binaires pr√©compil√©s et gestion des exceptions.  
- Options de configuration de mod√®le am√©lior√©es (√©chantillonnage, temp√©rature, capture du logit sup√©rieur).  
- Ajout d'un support complet pour Ubuntu 20+ avec les analyseurs et le moteur GGUF.  
- Prise en charge des nouveaux mod√®les Anthropic Claude 3.  
- Nouvelles m√©thodes de r√©cup√©ration : document_lookup et aggregate_text.  
- Nouveau mod√®le : bling-stablelm-3b-tool - mod√®le de questions-r√©ponses quantifi√© 3b rapide et pr√©cis - l'un de nos nouveaux favoris.  

**Mercredi 28 f√©vrier - Mise √† jour v0.2.4**  
- Mise √† jour majeure de la classe de mod√®le g√©n√©ratif GGUF - prise en charge de Stable-LM-3B, options de construction CUDA et meilleur contr√¥le sur les strat√©gies d'√©chantillonnage.  
- Remarque : nouvelles biblioth√®ques construites avec llama.cpp emball√©es avec la construction √† partir de v0.2.4.  
- Am√©lioration de la prise en charge GPU pour les mod√®les d'incorporation HF.  

**Vendredi 16 f√©vrier - Mise √† jour v0.2.3**  
- Ajout de 10+ mod√®les d'incorporation au ModelCatalog - nomic, jina, bge, gte, ember et uae-large.  
- Mise √† jour de la prise en charge d'OpenAI >=1.0 et nouveaux mod√®les d'incorporation text-3.  
- Cl√©s de mod√®le SLIM et output_values d√©sormais accessibles dans le ModelCatalog.  
- Mise √† jour des encodages √† 'utf-8-sig' pour mieux g√©rer les fichiers txt/csv avec BOM.  

**Derni√®res mises √† jour - 19 janv. 2024 - llmware v0.2.0**  
- Ajout de nouvelles options d'int√©gration de bases de donn√©es - Postgres et SQLite.  
- Am√©lioration des options de mise √† jour d'√©tat et de journalisation des √©v√©nements d'analyseur pour l'analyse parall√©lis√©e.  
- Am√©liorations significatives des interactions entre les bases de donn√©es d'incorporation et de collection de texte.  
- Am√©lioration de la gestion des exceptions d'erreur lors du chargement de modules dynamiques.  

**Derni√®res mises √† jour - 15 janv. 2024 : llmware v0.1.15**  
- Am√©liorations des requ√™tes de r√©cup√©ration √† double passage.  
- Objets de configuration √©largis et options pour les ressources de point de terminaison.  

**Derni√®res mises √† jour - 30 d√©c. 2023 : llmware v0.1.14**  
- Ajout de la prise en charge des serveurs d'inf√©rence Open Chat (compatible avec l'API OpenAI).  
- Am√©lioration des capacit√©s pour plusieurs mod√®les d'incorporation et configurations de base de donn√©es vectorielles.  
- Ajout de scripts d'installation docker-compose pour les bases de donn√©es vectorielles PGVector et Redis.  
- Ajout de 'bling-tiny-llama' au catalogue de mod√®les.  

**Derni√®res mises √† jour - 22 d√©c. 2023 : llmware v0.1.13**  
- Ajout de 3 nouvelles bases de donn√©es vectorielles - Postgres (PG Vector), Redis et Qdrant.  
- Am√©lioration de la prise en charge de l'int√©gration des transformateurs de phrases directement dans le catalogue de mod√®les.  
- Am√©liorations des attributs du catalogue de mod√®les.  
- Plusieurs nouveaux exemples dans Models & Embeddings, y compris GGUF, base de donn√©es vectorielle et catalogue de mod√®les.  

- **17 d√©c. 2023 : llmware v0.1.12**  
  - dragon-deci-7b ajout√© au catalogue - mod√®le RAG affin√© sur la nouvelle base de mod√®le haute performance 7B de Deci.  
  - Nouvelle classe GGUFGenerativeModel pour une int√©gration facile des mod√®les GGUF.  
  - Ajout de biblioth√®ques partag√©es pr√©construites llama_cpp / ctransformer pour Mac M1, Mac x86, Linux x86 et Windows.  
  - 3 mod√®les DRAGON empaquet√©s en tant que mod√®les GGUF Q4_K_M pour une utilisation sur ordinateur portable CPU (dragon-mistral-7b, dragon-llama-7b, dragon-yi-6b).  
  - 4 mod√®les de chat open source de premier plan ajout√©s au catalogue par d√©faut avec Q4_K_M.  

- **8 d√©c. 2023 : llmware v0.1.11**  
  - Nouveaux exemples de d√©marrage rapide pour l'ingestion et les embeddings de documents en haute volume avec Milvus.  
  - Nouvelle classe de mod√®le de serveur d'inf√©rence 'Pop up' LLMWare et script d'exemple.  
  - Nouvel exemple de traitement de factures pour RAG.  
  - Am√©lioration de la gestion de la pile Windows pour prendre en charge le parsing de documents plus volumineux.  
  - Am√©lioration des options de mode de sortie de journalisation de d√©bogage pour les analyseurs PDF et Office.  

- **30 nov. 2023 : llmware v0.1.10**  
  - Windows ajout√© en tant que syst√®me d'exploitation pris en charge.  
  - Am√©liorations suppl√©mentaires du code natif pour la gestion de la pile.  
  - Corrections mineures de d√©fauts.  

- **24 nov. 2023 : llmware v0.1.9**  
  - Les fichiers Markdown (.md) sont d√©sormais analys√©s et trait√©s comme des fichiers texte.  
  - Optimisations de la pile des analyseurs PDF et Office qui devraient √©viter la n√©cessit√© de d√©finir ulimit -s.  
  - Nouvel exemple llmware_models_fast_start.py qui permet de d√©couvrir et de s√©lectionner tous les mod√®les llmware HuggingFace.  
  - D√©pendances natives (biblioth√®ques partag√©es et d√©pendances) d√©sormais incluses dans le d√©p√¥t pour faciliter le d√©veloppement local.  
  - Mises √† jour de la classe Status pour prendre en charge les mises √† jour de statut d'analyse de documents PDF et Office.  
  - Corrections mineures de d√©fauts, y compris la gestion des blocs d'images dans les exportations de biblioth√®que.  



- **17 nov. 2023 : llmware v0.1.8**  
  - Performance de g√©n√©ration am√©lior√©e en permettant √† chaque mod√®le de sp√©cifier le param√®tre d'espace de fin.  
  - Am√©lioration de la gestion pour eos_token_id pour llama2 et mistral.  
  - Meilleure prise en charge du chargement dynamique de Hugging Face.  
  - Nouveaux exemples avec les nouveaux mod√®les DRAGON de llmware.  

- **14 nov. 2023 : llmware v0.1.7**  
  - Pass√© au format de paquet Python Wheel pour la distribution PyPi afin de fournir une installation transparente des d√©pendances natives sur toutes les plateformes prises en charge.  
  - Am√©liorations du ModelCatalog :  
    - Mise √† jour d'OpenAI pour inclure les nouveaux mod√®les ¬´ turbo ¬ª 4 et 3.5 r√©cemment annonc√©s.  
    - Mise √† jour de l'incorporation Cohere v3 pour inclure les nouveaux mod√®les d'incorporation Cohere.  
    - Mod√®les BLING en tant qu'options enregistr√©es pr√™tes √† l'emploi dans le catalogue. Ils peuvent √™tre instanci√©s comme tout autre mod√®le, m√™me sans le drapeau ¬´ hf=True ¬ª.  
    - Capacit√© √† enregistrer de nouveaux noms de mod√®les, au sein des classes de mod√®les existantes, avec la m√©thode register dans le ModelCatalog.  
  - Am√©liorations des prompts :  
    - ‚Äúevidence_metadata‚Äù ajout√© aux dictionnaires de sortie prompt_main permettant aux r√©ponses de prompt_main d'√™tre int√©gr√©es dans les √©tapes de preuve et de v√©rification des faits sans modification.  
    - La cl√© API peut maintenant √™tre pass√©e directement dans une prompt.load_model(model_name, api_key = ‚Äú[ma-cl√©-api]‚Äù).  
  - Serveur LLMWareInference - Livraison initiale :  
    - Nouvelle classe pour LLMWareModel qui est un wrapper sur un mod√®le bas√© sur une API de style HF personnalis√©.  
    - LLMWareInferenceServer est une nouvelle classe qui peut √™tre instanci√©e sur un serveur distant (GPU) pour cr√©er un serveur API de test qui peut √™tre int√©gr√© dans n'importe quel flux de travail de prompt.  

- **03 nov. 2023 : llmware v0.1.6**  
  - Mise √† jour de l'emballage pour n√©cessiter mongo-c-driver 1.24.4 afin de contourner temporairement le plantage de segmentation avec mongo-c-driver 1.25.  
  - Mises √† jour dans le code Python n√©cessaires en pr√©vision d'un support futur de Windows.  

- **27 oct. 2023 : llmware v0.1.5**  
  - Quatre nouveaux scripts d'exemple ax√©s sur les flux de travail RAG avec de petits mod√®les d'instruction ajust√©s finement qui fonctionnent sur un ordinateur portable (mod√®les BLING de `llmware` [BLING](https://huggingface.co/llmware)).  
  - Options √©largies pour d√©finir la temp√©rature √† l'int√©rieur d'une classe de prompt.  
  - Am√©lioration du post-traitement de la g√©n√©ration de mod√®les Hugging Face.  
  - Chargement simplifi√© des mod√®les g√©n√©ratifs Hugging Face dans les prompts.  
  - Livraison initiale d'une classe de statut central : lecture/√©criture de l'√©tat d'incorporation avec une interface coh√©rente pour les appelants.  
  - Am√©lioration du support de recherche de dictionnaire en m√©moire pour des requ√™tes multi-cl√©s.  
  - Suppression de l'espace final dans l'encadrement humain-bot pour am√©liorer la qualit√© de g√©n√©ration dans certains mod√®les ajust√©s finement.  
  - Corrections mineures de d√©fauts, scripts de test mis √† jour et mise √† jour de version pour Werkzeug afin de r√©soudre une [alerte de s√©curit√© de d√©pendance](https://github.com/llmware-ai/llmware/security/dependabot/2).  

- **20 oct. 2023 : llmware v0.1.4**  
  - Prise en charge GPU pour les mod√®les Hugging Face.  
  - Corrections de d√©fauts et scripts de test suppl√©mentaires.  


- **13 oct. 2023 : llmware v0.1.3**  
  - Prise en charge de la recherche vectorielle MongoDB Atlas.  
  - Prise en charge de l'authentification √† l'aide d'une cha√Æne de connexion MongoDB.  
  - M√©thodes de r√©sum√© de documents.  
  - Am√©liorations dans la capture automatique de la fen√™tre de contexte du mod√®le et le passage des modifications dans la longueur de sortie attendue.  
  - Carte de jeu de donn√©es et description avec recherche par nom.  
  - Temps de traitement ajout√© au dictionnaire d'utilisation de l'inf√©rence du mod√®le.  
  - Scripts de test suppl√©mentaires, exemples et corrections de d√©fauts.  

- **06 oct. 2023 : llmware v0.1.1**  
  - Ajout de scripts de test au d√©p√¥t GitHub pour les tests de r√©gression.  
  - Corrections mineures de d√©fauts et mise √† jour de version de Pillow pour r√©soudre une [alerte de s√©curit√© de d√©pendance](https://github.com/llmware-ai/llmware/security/dependabot/1).  

- **02 oct. 2023 : llmware v0.1.0** üî• Version initiale de llmware en open source !! üî•  

</details>